\documentclass[a4paper,12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[italian]{babel}
\usepackage{afterpage}
\usepackage[pdftex,
pdfauthor={Claudio Mastronardo},
pdftitle={Algoritmi di ricerca locale per l'apprendimento di Cutset Network},
pdfsubject={Tesi di laurea triennale},
pdfkeywords={stochastic local search machine learning probabilistic graphical models cutset network},
pdfproducer={Latex with hyperref + TexStudio},
pdfcreator={pdflatex}]{hyperref}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes, arrows}
\usetikzlibrary{fit}
\usepackage{subcaption}
\tikzstyle{X} = [circle,fill=white,draw=black]
\tikzstyle{Y} = [X,fill=gray!25]
\tikzset{>={triangle 45}}
\usepackage{amsmath}
\usepackage{pdfpages}


%\graphicspath{{images/}}
\usepackage{algorithm}
\usepackage{longtable}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\usepackage{fixltx2e}
\usepackage{amsbsy}
\usepackage[linewidth=1pt]{mdframed}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{booktabs}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage{epigraph}
\renewcommand{\sourceflush}{flushleft}
\setlength{\epigraphwidth}{.5\textwidth}
\author{Claudio Mastronardo}
\date{}
\begin{document}
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
		
		\center % Center everything on the page
		
		%----------------------------------------------------------------------------------------
		%	HEADING SECTIONS
		%----------------------------------------------------------------------------------------
		
		\textsc{\LARGE Università degli studi di Bari}\\[1cm] % Name of your university/college
		\includegraphics[scale=0.25]{fig/uniba.png}\\[1cm]
		\textsc{\Large Dipartimento di Informatica}\\[0.5cm] % Major heading such as course name
		\textsc{\large Corso di Laurea Triennale in Informatica}\\[0.5cm] % Minor heading such as course title
		\textsc{\large TESI DI LAUREA}\\[0.5cm] % Minor heading such as course title
		\text{\large in}\\[0.5cm] % Minor heading such as course title
		\textsc{\large Algoritmi e strutture dati}\\[0.5cm] % Minor heading such as course title
		%----------------------------------------------------------------------------------------
		%	TITLE SECTION
		%----------------------------------------------------------------------------------------
		
		\HRule \\[0.5cm]
		{ \huge \bfseries Algoritmi di ricerca locale per l'apprendimento di\\[0.3cm]Cutset~Network}\\[0.5cm] 
		\HRule \\[1.0cm]
		
		%----------------------------------------------------------------------------------------
		%	AUTHOR SECTION
		%----------------------------------------------------------------------------------------
		
		\begin{minipage}{0.5\textwidth}
			\begin{flushleft} \large
				\emph{Relatore:}\\
				Dr. Nicola Di Mauro
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\emph{Laureando:} \\
				Claudio Mastronardo 
			\end{flushright}
		\end{minipage}
		\vfill % Fill the rest of the page with whitespace
		{\large Anno Accademico 2015/2016}
		\afterpage{
\epigraph{\textit{''Considerate la vostra semenza: fatti non foste a viver come bruti ma per seguir virtute e canoscenza''}}{Dante Alighieri, Inferno XXVI, 116-120}
			\newpage}
	\end{titlepage}
	
	
	\newpage
	\tableofcontents
	\clearpage%\null\newpage
	\section{Introduzione}
	\subsection{Contesto}
	Le tematiche di Intelligenza Artificiale (Artificial Intelligence, AI) e Apprendimento Automatico (Machine Learning, ML) sono delle aree dell'informatica studiate da diversi decenni. Nella speranza di creare macchine sempre più intelligenti, i ricercatori di tutto il mondo hanno creato e continuano a creare sempre nuovi e più sofisticati algoritmi. Molto spesso questi algoritmi sono destinati alla creazione di modelli che rappresentino, in maniera unitaria, come un programma debba effettuare un compito all'interno di una certa area di interesse. La creazione di questi modelli è basata spesso sulla somministrazione di un'insieme di dati d'esempio all'algoritmo.
	L'obiettivo centrale è quello di risolvere nuovi problemi addestrando le macchine secondo vari paradigmi e tecniche.
	
	In Machine Learning si cerca di costruire programmi che, sfruttando una certa quantità di dati, siano in grado di apprendere autonomamente dei modelli in grado di descrivere e spiegare in maniera chiara suddetti dati e di predirne di nuovi.
	
	Benché la quantità di informazioni, modelli ed algoritmi sia di notevole ampiezza, questa tesi ne analizza molto limitatamente una tipologia: quella dei \textit{Modelli Grafici Probabilistici \cite{koller} (Probabilistic Graphical Model - PGM)}.\\
	Essi rappresentano un ricco framework in grado di codificare e descrivere distribuzioni di probabilità su domini complessi, in maniera grafica, sfruttando i concetti della teoria delle probabilità e della teoria dei grafi. Un punto di forza è sicuramente quello di descrivere il loro comportamento in maniera chiara (White Box) a differenza di altri modelli (es. Reti neurali \cite{bishop}) che non permettono di capire i meccanismi e le relazioni interne alla struttura che permettano ad essi di fare inferenza (Black Box).
	
	La famiglia dei Probabilistic Graphical Model è formata da diverse tipologie di modelli. Il lavoro svolto in questa tesi ha avuto a che fare con una nuova tipologia chiamata ''Cutset Network''\cite{rahman}, che sfrutta modelli ed algoritmi già presentati in letteratura come gli alberi di Chow-Liu\cite{chowliu} e l'algoritmo di Kruskal \cite{krusk}.
	Per la creazione di questi modelli è stato utilizzato un approccio presentato con successo in \cite{dimauro}, che sfrutta la definizione ricorsiva delle Cutset Networks.
	 
	Questa tesi investiga l'utilizzo di algoritmi stocastici di ottimizzazione per migliorare le strutture grafiche apprese. In particolare si analizzano alcuni semplici algoritmi stocastici di ricerca locale \cite{slsbook} come Iterative Improvement, Randomised Iterative Improvement e GRASP (Greedy Randomised Adaptive Search Procedures). Essi sono stati applicati ed integrati al lavoro svolto in~\cite{dimauro}.\\
	In maniera congiunta all'applicazione degli algoritmi di ottimizzazione, è stato analizzato l'impatto di alcune modifiche all'algoritmo presentato in \cite{dimauro}, come l'uso di foreste e l'uso di rumore nel processo di apprendimento. 
	
	L'implementazione ed analisi di codesti algoritmi hanno dato luogo a sperimentazioni numeriche riportate e descritte alla fine del manoscritto. Le sperimentazioni sono state fatte utilizando dei datasets comunemente accettati dalla comunità scentifica come standard per il benchmarking di algoritmi di apprendimento di PGM. La parte finale del lavoro svolto è costituita dalla discussione dei risultati ottenuti.
	
	\subsection{Strutturazione del documento}
	
	I primi capitoli del documento contengono i concetti base e necessari per poter comprendere il lavoro svolto. Successivamente sono presentati gli algoritmi di ricerca utilizzati. In seguito è analizzato come essi sono stati applicati al problema specifico dell'apprendimento di Cutset Networks, ed infine sono analizzati, riportati e discussi i risultati derivati dalle sperimentazioni.\\
	Il capitolo \ref{mlpgm} contiene le nozioni legate al tema di Machine Learning, Density Estimation e modelli grafici probabilistici. Il capitolo fornisce definizioni ed esempi, presentando le Cutset Networks e analizzando l'algoritmo originale di apprendimento\cite{rahman} e quello nuovo basato sulla decomponibilità\cite{dimauro}.\\
	Il capitolo \ref{sls} tratta delle nozioni legate alla ricerca stocastica locale descrivendo ad alto livello gli algoritmi applicati in questo lavoro.\\
	Il capitolo \ref{appsls} è dedicato alla descrizione di come gli algoritmi presentati nel capitolo \ref{sls} sono stati adattati al problema centrale. Inoltre sono presentate alcune modifiche all'algoritmo originale \cite{dimauro}.\\
	Il capitolo \ref{sperimentazione} riporta le sperimentazioni compiute sui datasets e la loro discussione.\\
	Il capitolo \ref{conc} è dedicato alle conclusioni derivate dalle sperimentazioni.
	\subsection{Concetti preliminari}
	Nel contesto in cui è stata svolta questa tesi si è fatto uso di un insieme di dati detto ''training set'' o ''learning set'' che consiste in un insieme di osservazioni. Le osservazioni sono derivate dal problema che si è scelto di affrontare. Solitamente si assume che le osservazioni siano indipendenti tra di loro e che siano generate dalla stessa densità che non cambia nel tempo.
	
	Formalmente, l'osservazione di un fenomeno può essere descritta tramite un insieme di \textit{k} variabili aleatorie trattate in un certo ordine:\\
	\centerline{$\pmb{\mathcal{X}} = \{\mathcal{X}\textsubscript{1} , ... ,\mathcal{X}\textsubscript{k}\}$}\\
	Ogni variabile può essere categorica o numerica, continua o discreta.
	\\ La probabilità congiunta di $\pmb{\mathcal{X}}$ è denotata come $\mathcal{P}(\pmb{\mathcal{X}})$.
	\\Quindi una osservazione corrisponde ad una tupla di $k$ valori rappresentanti i valori assunti da altrettante variabili aleatorie nel momento in cui sono osservate. Formalmente :\\[0.2cm]
	\centerline{
		$Val(\pmb{\mathcal{X}})	\ni$ x\textsubscript{D} = x = ($\mathcal{X}_1$ = $x_1$, ..., $\mathcal{X}_p$ = $x_p$) $\sim$ $\mathcal{P}(\mathcal{\pmb{X}})$.
	}
	\\[0.2cm]
	con $Val(\pmb{\mathcal{X}})$ l'insieme dei valori assumibili da $\pmb{\mathcal{X}}$, ovvero il prodotto cartesiano di tutti gli insiemi di valori assumibili di tutte le variabili.
	
	Il training set è utilizzato per la fase di apprendimento e generazione del modello con cui si decide di affrontare il problema scelto.
	Un algoritmo di apprendimento è un algoritmo il cui compito è quello di generare un \textit{modello} che sia in grado di rappresentare e risolvere nuovi problemi all'interno di un determinato ambito di interesse.
	\\Esso ha come input il training set, che utilizza per $apprendere$ automaticamente particolari pattern e regole che permettano di trovare il modello ottimale capace di ''capire'' tutte le informazioni rilevanti e quindi
	\textit{generalizzare}.
	\begin{figure}[h]
		\begin{tikzpicture}
		\node[draw,rounded corners=0.1cm] (ArgumentA) {Training set};
		\node[draw,rounded corners=0.1cm] (ArgumentB) at (5.7,0) {Algoritmo di apprendimento};
		\node[draw,rounded corners=0.1cm] (ArgumentC) at (11.4,0) {Modello};
		
		\draw[->,draw=black] (ArgumentA) to node[above]{input} (ArgumentB);
		\draw[->,draw=black] (ArgumentB) to node[above]{output} (ArgumentC);
		\end{tikzpicture}
		\caption{Un algoritmo di apprendimento prende in input un learning set e produce in output un modello.}
	\end{figure}
	\\La strategia intrapresa dall'algoritmo per scegliere il modello da fornire in output può essere di due tipi: deterministica o stocastica.\\
	Nel primo caso l'algoritmo, dato un training set, darà in output sempre lo stesso modello.
	Al contrario nel secondo caso l'algoritmo darà in output modelli diversi per ogni esecuzione sullo stesso training set.\\
	Alcune strategie stocastiche saranno discusse nel capitolo \ref{sls}.
	
	Il modello può essere informativo, predittivo o entrambi. Un modello informativo fornisce informazioni a proposito del problema, ad esempio alcune correlazioni tra variabili. Un modello predittivo fornisce informazioni su nuove realizzazioni del problema.
	
	Una componente spesso fondamentale è rappresentata da un set chiamato ''validation set''.
	Il validation set è anch'esso una collezione di osservazioni rappresentate tramite l'avvalorazione delle \textit{k} variabili aleatorie prese in considerazione. A differenza del training set, il validation set non viene utilizzato dall'algoritmo per ''apprendere'', ma viene utilizato durante la fase di apprendimento per fare la messa a punto dei parametri e della struttura del modello finale. Alla fine della fase di apprendimento, quindi, ci si è costruiti un modello appreso grazie al training set e ''validato'' sul validation set.
	
	Una volta appreso il modello si utilizza il cosiddetto ''test set''.
	Il test set è anch'esso una collezione di osservazioni rappresentate tramite l'avvalorazione delle \textit{k} variabili aleatorie prese in considerazione. Ciò che lo distingue dal training set/validation set è il fatto che esso non viene somministrato all'algoritmo in fase di apprendimento. Esso è utilizzato a posteriori per valutare la performance del modello appreso.
	\newpage
	\section{Machine Learning e Probabilistic Graphical Models}\label{mlpgm}
	\subsection{Machine Learning}\label{ml}
	L'apprendimento automatico (in inglese \textit{Machine Learning}) \cite{bishop2} è una branca dell'intelligenza artificiale che si occupa di studiare modelli ed algoritmi che permettono al software di apprendere automaticamente come risolvere nuovi problemi. Ciò è ottenuto spesso fornendo, agli algoritmi, dei dati rappresentanti osservazioni del fenomeno da studiare.
	\\Questo insieme di dati è rappresentato dal training set e dal validation set. 
	
	
	\subsubsection{Tipologie di apprendimento}
	Gli algoritmi di apprendimento automatico sono divisi in diverse tipologie. Di seguito si fa accenno solamente a due macro tipologie: apprendimento supervisionato (Supervised Learning) e apprendimento non supervisionato (Unsupervised Learning) \cite{bishop2}.
	
	L'apprendimento supervisionato consiste nel somministrare, all'algoritmo, un training set contenente le caratteristiche delle osservazioni e uno o più valori di output desiderato. L'output desiderato viene utilizzato per quantificare l'errore commesso dall'algoritmo e, in base ad esso, correggere e migliorare l'inferenza per le successive osservazioni.\\
	Nell'apprendimento non supervisionato l'obiettivo dell'algoritmo è trovare pattern nascosti all'interno di un data set.
	
	In questa tesi verranno trattati algoritmi appartenenti alla prima tipologia presentata.
	\subsubsection{Valutazione}\label{valutazione}
	Grazie ad un algoritmo di apprendimento ed un training set si è in grado, quindi, di generare un modello. Ma come capire se il modello generato è \textit{effettivamente} adeguato? Risposta: effettuare una fase di valutazione atta a misurare il livelli di performance del modello appreso.
	
	Una parte fondamentale dell'apprendimento automatico è rappresentata dalle metriche e tecniche di valutazione dei modelli appresi. La valutazione di un modello è quella fase in cui si misura il comportamento, in termini di accuratezza, su un set di dati non precedentemente ''visti'' . Per far ciò si utilizza il test set.
	
	Una delle metriche più semplici e comuni è l'accuratezza, che misura la percentuale di osservazioni correttamente  classificate da un modello di classificazione. Altre misure in classificazione sono : precision, recall e specificity.
	
	La misura dell'errore, generalmente, migliora rispetto alle osservazioni presenti nel training set. Una misura d'errore troppo ''buona'' sul training set potrebbe portare il modello ad avere comportamenti negativi su un set di osservazioni non ancora ''viste''. Questo problema è chiamato \textit{overfitting}. Quindi l'obiettivo dell'apprendimento è quello di \textit{generalizzare}, in modo tale da comportarsi in maniera ottima anche su osservazioni non ancora viste.\\
	La figura \ref{fig:over} mostra, in un esempio, l'evoluzione delle misure di errore sul training e sul test set all'aumentare della complessità del modello. Si può notare come all'aumentare della complessità del modello l'errore sul training set diminuisca sempre più ma l'errore sul test set decresca fino ad un certo punto per poi riprendere ad aumentare. Dal punto in cui si peggiora l'errore sul test set ma si continua a migliorare quello sul training set inizia la fase di overfitting. L'obiettivo di un buon addestramento, quindi, è quello di trovare il giusto grado di complessità tale da minimizzare l'errore sul test set e non fare overfitting.\\
	Nel caso di density estimation (presentato in \ref{de}) la complessità del modello aumenta con l'aumentare del numero di parametri indipendenti utilizzati per rappresentare il modello.
	
	Altre tecniche atte all'evitare l'overfitting si occupano di utilizzare anche il validation set (introdotto prima). In particolare in situazioni computazionalmente abbastanza favorevoli è molto efficace l'uso del \textit{K-Fold cross-validation}. Questa procedura prevede il partizionamento del set originale di osservazioni in K slices (con K deciso a priori). Per ogni slice si avvia una procedura di apprendimento utilizzando i restanti K-1 slices come training set, usando il corrente slice come validation set. La procedura viene fatta per tutti i K slices e la valutazione finale del modello è ricavata mediando tutte le K valutazioni.
	\begin{figure}[h]
		\centerline{
			\includegraphics{grafici/cut_60.pdf}}
		\caption{Cercando di minimizzare l'errore sul learning set si può superare un punto per cui l'errore sul training set continua a migliorare ma quello sul test set inizia a peggiorare, portando così all'overfitting.}
		\label{fig:over}
	\end{figure}
	\subsubsection{Density Estimation}\label{de}
	In statistica per stima della densità s'intende la costruzione di una stima di una funzione di densità di probabilità inosservabile. Questo viene fatto tramite l'utilizzo di un insieme di dati osservati.
	
	Sfruttando le definizioni date nel capitolo \ref{ml}, possiamo ricondurre l'insieme dei dati osservati utilizzati per fare density estimation come il training set $\mathcal{\pmb{X}}$ da cui costruire un modello che sia in grado di codificare questa stima.
	
	Definendo con $\mathcal{P(\pmb{X})} $ la distribuzione congiunta di $\mathcal{X}$ possiamo rappresentare la probabilità di un set \pmb{\textit{T}}  di \textit{N} osservazioni come:\\
	\centerline{$\mathcal{P}(\textit{T} ) = {\displaystyle \prod_{i=1}^{N} \mathcal{P} (\mathcal{X}=xd_\textsubscript{i})  }$}\\
	con $xd_\textsubscript{i}$ la i-esima tupla di valori del training set.
	
	Per quanto riguarda la valutazione, nel caso di density estimation, una misura di errore adeguata è rappresentata dal \textit{loglikelihood}, che si occupa di comparare la bontà dell'adattamento del modello appreso rispetto alla densità obiettivo da codificare. Una misura nota è quella della divergenza di Kullback-Leibler (\textit{D\textsubscript{KL}}) \cite{kullback}, anche chiamata information divergence o relative entropy:\\[0.2cm]
	\centerline{
		Err(\textit{M}\textsubscript{D})= \textit{D\textsubscript{KL}}
		($\mathcal{P}\vert\vert$\textit{M}\textsubscript{D}) =
		$\sum_{x}\mathcal{P}$(x)
		log$\frac{\mathcal{P}(x)}{\textit{M}\textsubscript{D}(x)}$
	}\\[0.2cm]
	con \textit{M}\textsubscript{D} il modello $M$ appreso sul training set D.\\
	La divergenza può essere interpretata come il numero di bit extra richiesti per codificare esempi dalla densità originale utilizzando codice basato su \textit{M}\textsubscript{D}, al posto di $\mathcal{P}$. 
	Questa misura si occupa di misurare la differenza tra due distribuzioni di probabilità. In questo caso una distribuzione è quella pura da stimare e l'altra è quella codificata dal modello appreso.
	Se la misura è 0 allora le due densità sono uguali.\\
	Un'altra misura utilizzata è il negative log-likelihood:\\[0.3cm]
	\centerline{
		nlogll(\textit{M}\textsubscript{D},x) = -log\textit{M}\textsubscript{D}(x)
	}\\[0.3cm]
	con nlogll(\textit{M}\textsubscript{D},x) = 0 se le due distribuzioni coincidono.\\
	Queste misure non sono calcolabili in maniera esatta dato che la densità vera che si sta cercando di approssimare è ignota e anche perché di solito il numero di variabili in gioco è molto alto causando il calcolo delle misure improponibile.
	
	Quindi le misure sono calcolate in maniera stimata basandosi su un test set \textit{D} di N osservazioni tramite:\\[0.2cm]
	\centerline{
		$\hat{Err}$\textsubscript{D'}(\textit{M}\textsubscript{D}) = $\frac{1}{N}\sum_{i=1}^{N}Err(M\textsubscript{D},x\textsubscript{D'\textsubscript{i}})$
	}\\[0.2cm]
	In questa tesi saranno utilizzate alcune tecniche di machine learning per apprendere modelli che cercano di risolvere problemi di stima delle densità, sfruttando le formule e le definizioni ivi fornite.
	\newpage
	
	\subsection{Probabilistic Graphical Models}\label{pgm}
	I Probabilistic Graphical Model (PGM)\cite{koller} sono una classe di modelli che sfruttano la teoria dei grafi e la teoria delle probabilità per descrivere in forma compatta distribuzioni di probabilità su cui è possibile fare inferenza. Attraverso una struttura grafica ed un insieme di parametri permettono di codificare una distribuzione di probabilità multivariata identificando le relazioni tra le variabili aleatorie prese in considerazione. Identificare e esplicitare i possibili valori da esse assumibili corrisponde a determinare i \textit{parametri} del modello.
	
	La potenza espressiva dei PGM permette loro di rappresentare facilmente le relazioni presenti tra le variabili trattando esse come nodi del grafo e gli archi tra i nodi come interazioni fra di esse. Quindi il modello grafico rappresenta strutturalmente la distribuzione esplicitandone le variabili e i parametri associati mostrando eventuali relazioni di indipendenza condizionata tra di esse.
	
	Formalmente un PGM è formato da un set di parametri $\theta$ ed una struttura grafica $\mathcal{G}=(V,E)$ con $V$ il set di vertici del grafo e $E \subset V$ x $V$ il set di archi del grafo. Catturare ed interpretare il significato codificato dalla struttura di un PGM dipenderà dal particolare tipo di modello impiegato. Ad un primo livello gerarchico si distinguono i modelli $diretti$ (directed) e i modelli $indiretti$ (undirected), direttamente collegati alla tipologia di archi utilizzati.
	
	\subsubsection{Reti Bayesiane}
	Un esempio di PGM è rappresentato dalle reti bayesiane.
	Le reti bayesiane rappresentano un insieme di variabili aleatorie e le loro dipendenze attraverso un \textit{grafo aciclico $\pmb{diretto}$}.
	In questo tipo di rappresentazioni le variabili aleatorie vengono rappresentate tramite dei nodi e forniscono informazioni utili alla strutturazione del problema. Gli archi tra i nodi invece indicano delle condizioni di dipendenza. I parametri quantificano la distribuzione di probabilità di ogni variabile  $\mathcal{X} \textsubscript{i}$ condizionata dai propri padri nel grafo.\\
	Un esempio è riportato nella figura \ref{fig:bn} .
	%\begin{figure}[h]
	
	%	\includegraphics{bn}
	%	\centering
	%	\caption{Una semplice rete bayesiana}
	%	\label{fig:bn}
	%	\end{figure}
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\node[draw,rounded corners=0.1cm] (ArgumentA) at(0,5) {Età};
		\node[draw,rounded corners=0.1cm] (ArgumentB) at(2,5) {Sesso};
		\node[draw,rounded corners=0.1cm] (ArgumentC) at(4,5){Peso};
		\node[draw,rounded corners=0.1cm] (ArgumentD) at(6,5){Alcohol};
		\node[draw,rounded corners=0.1cm] (ArgumentE) at(8,4){Livello di ferro};
		\node[draw,rounded corners=0.1cm] (ArgumentF) at(4,3){Malattia del fegato};
		\node[draw,rounded corners=0.1cm] (ArgumentG)at(0,0) {Stanchezza};
		\node[draw,rounded corners=0.1cm] (ArgumentH) at(2.5,0.4){Colesterolo};
		\node[draw,rounded corners=0.1cm] (ArgumentI) at(5,0){Trigliceridi};
		\node[draw,rounded corners=0.1cm] (ArgumentL) at(10,2){Itterizia};
		\node[draw,rounded corners=0.1cm] (ArgumentM) at(8.5,0.4){Livello di Bilirubina};
		
		
		\draw[->,draw=black,line width=0.3mm] (ArgumentA) to (ArgumentF);
		\draw[->,draw=black,line width=0.3mm] (ArgumentB) to (ArgumentF);
		\draw[->,draw=black,line width=0.3mm] (ArgumentC) to (ArgumentF);
		\draw[->,draw=black,line width=0.3mm] (ArgumentD) to (ArgumentF);
		\draw[->,draw=black,line width=0.3mm] (ArgumentD) to (ArgumentE);
		\draw[->,draw=black,line width=0.3mm] (ArgumentE) to (ArgumentF);
		\draw[->,draw=black,line width=0.3mm] (ArgumentF) to (ArgumentG);
		\draw[->,draw=black,line width=0.3mm] (ArgumentF) to (ArgumentH);
		\draw[->,draw=black,line width=0.3mm] (ArgumentF) to (ArgumentI);
		\draw[->,draw=black,line width=0.3mm] (ArgumentF) to (ArgumentM);
		\draw[->,draw=black,line width=0.3mm] (ArgumentM) to (ArgumentL);
		
		
		\end{tikzpicture}
		
		\caption{Rete bayesiana che modella malattie al fegato(in particolare la \textit{cirrosi biliare primitva} e \textit{steatosi epatica}). Esempio ispirato da \cite{onisko} }
		\label{fig:bn}
	\end{figure}
	Nella figura d'esempio possiamo notare la presenza di 11 variabili aleatorie identificate attraverso nodi del grafo. Ogni arco rappresenta una relazione indicante la dipendenza condizionale di una variabile verso un'altra.\\
	Questa rete, per esempio, potrebbe essere usata sfruttando un insieme di osservazioni per derivare nuove informazioni riguardo la malattia.\\
	Nella rete bayesiana possiamo notare come l'età, il sesso, peso, alcohol e il livello di ferro influiscano sulla malattia al fegato. Si può anche notare come non esistano dipendenze dirette tra età ed itterizia, ma bensì una dipendenza transitiva tramite malattia del fegato e livello di bilirubina.
	Grazie alle dipendenze condizionali espresse dal modello grafico, si può quindi codificare una fattorizzazione della probabilità congiunta tramite il prodotto delle distribuzioni condizionali:\newline
	
	\centerline{$\mathcal{P} (\mathcal{X}) = 
		{\displaystyle \prod_{i=1}^{P} \mathcal{P} (\mathcal{X}\textsubscript{i} | \mathcal{P}a(\mathcal{X}\textsubscript{i}) ) }$ }~\\[0.3cm]
	Con $ \mathcal{X}$ un array di possibili valori delle variabili aleatorie prese in considerazione, $\mathcal{X}\textsubscript{i}$ la i-esima variabile aleatoria e $\mathcal{P} (\mathcal{X}\textsubscript{i} | \mathcal{P}a(\mathcal{X}\textsubscript{i}) )$ la probabilità della variabile $\mathcal{X}\textsubscript{i}$ condizionata alle variabili padre $\mathcal{P}a(\mathcal{X}\textsubscript{i}) $. 
	L'utilizzo di una rete bayesiana, oltre a essere visualmente vantaggiosa,
	permette di visualizzare le probabilità e le relazioni in maniera sintetica rispetto ad una ipotetica tabella di contingenza (contenente la probabilità di ogni possibile configurazione di variabili).
	
	La struttura di una rete bayesiana può essere costruita grazie alla conoscenza di un esperto nell'area di appartenenza del problema che si sta cercando di risolvere oppure grazie ad un algoritmo di apprendimento automatico.\\
	Nel caso in cui si utilizzi un algoritmo di apprendimento automatico, la costruzione della rete può essere utile ad evidenziare particolari relazioni di dipendenza e indipendenza che magari a priori non fossero note.
	
	\subsubsection{Alberi di Chow-Liu}\label{cltree}
	La struttura ''albero di Chow-Liu'' è un modello \textit{trattabile} ad albero che permette di approssimare una rete bayesiana. \`{E} un modello grafico presentato da Chow e Liu in \cite{chowliu}. Una assunzione principale in questa struttura è, quindi, che ogni nodo ha un solo padre e la radice è il padre diretto/indiretto di tutti i nodi e non ha padre a sua volta.\\
	La struttura dell'albero è appresa sfruttando la cosiddetta \textit{matrice delle mutue informazioni}. Questa matrice rappresenta sulle righe e sulle colonne tutte le variabili aleatorie. In corrispondenza della cella sulla riga \textit{i} e colonna \textit{j} si ha un valore che rappresenta la mutua informazione della coppia formata dalla i-esima e j-esima variabile aleatoria. La mutua informazione rappresenta una misura della mutua dipendenza tra le due variabili aleatorie.  Date due variabili \textit{u,v} la mutua informazione è così calcolata:\\\\
	\centerline{\textit{I\textsubscript{uv}} = $\sum_{\textit{x\textsubscript{u}x\textsubscript{v}}} \mathcal{P}\textsubscript{\textit{uv}}(\textit{x\textsubscript{u},v\textsubscript{v}})$  log$ \frac{\mathcal{P}\textsubscript{\textit{uv}}(\textit{x\textsubscript{\textit{u}},v\textsubscript{v}})}{\mathcal{P}\textsubscript{\textit{u}}(\textit{x\textsubscript{u}})\mathcal{P}\textsubscript{\textit{v}}(\textit{x\textsubscript{v}})}$ ,}\\[0.15cm]\centerline{ \textit{u,v $\in$V, u $\neq$ v}, $x_v \in Val(v)$, $x_u \in Val(u)$ }\\\\
	Con \textit{V} l'insieme delle variabili aleatorie e $Val(u)$ l'insieme dei valori assunti dalla variabile $u$.\\ Piu sarà alto il valore della mutua informazione tra due variabili, più sarà utile includere la loro relazione causale nell'albero finale. I valori di mutua informazione per ogni coppia di variabili è direttamente calcolato dal training set.
	Questo procedimento dà vita ad una matrice simmetrica come quella presente nella parte destra della figura \ref{fig:kruskal}.\\
	Una volta calcolata la matrice delle mutue informazioni per tutte le coppie di variabili si procede con la costruzione dell'albero tramite un algoritmo di \textit{Maximum Weight Spanning Tree} (MWST) usando le mutue informazioni come pesi per gli archi tra le variabili. L'obiettivo è quello di costruire un albero $G_T = (V, E_T)$ utilizzando tutte le variabili come nodi, tale che ogni arco $(u,v) \in (E_T)$ sia pesato da $I(u,v)$ e la sommatoria dei pesi degli archi finale sia la massima raggiungibile.
	\\L'algoritmo utilizzato da Chow e Liu è l'algoritmo di Kruskal (esempio nella \hyperref[fig:kruskal]{figura sottostante \ref{fig:kruskal}}). L'algoritmo di Kruskal è di tipo greedy, tutti gli archi candidati sono ordinati in ordine decrescente di peso. Ad uno ad uno, partendo dall'arco con peso maggiore, vengono aggiunti all'albero (inizialmente avente l'insieme degli archi vuoto). Ad ogni passo si verifica che l'arco aggiunto non crei un ciclo. Nel caso in cui si venga  a creare un ciclo l'arco viene scartato. Il processo va avanti fino a quando non sono state collegate tutte le \textit{n} variabili aleatore all'albero avendo, quindi, \textit{n - 1} archi.
	%%%kruskal ex
	
	\begin{figure}
		
		\begin{subfigure}{\linewidth}
			\centerline{
				\includegraphics{kruskal/117.pdf}}
			\caption{Situazione iniziale}
		\end{subfigure}\par\medskip
		\begin{subfigure}{\linewidth}
			\centerline{
				\includegraphics{kruskal/119.pdf}}
			\caption{Si sceglie l'arco con peso maggiore e lo si aggiunge all'albero}
		\end{subfigure}\par\medskip
		\begin{subfigure}{\linewidth}
			\centerline{
				\includegraphics{kruskal/121.pdf}}
			\caption{Si sceglie l'arco con peso maggiore scartando quelli già aggiunti e lo si aggiunge all'albero controllando che non si vengano a creare cicli}
		\end{subfigure}\par\medskip
		\begin{subfigure}{\linewidth}
			\centerline{
				\includegraphics{kruskal/123.pdf}}
			\caption{In questa situazione l'arco con peso maggiore se aggiunto all'albero creerebbe un ciclo, quindi viene scartato}
		\end{subfigure}\par\medskip
	\caption{}
	\label{fig:kruskal}
	\end{figure}
\begin{figure}
		\centerline{
			\includegraphics{kruskal/125.pdf}}
		\caption{Il processo va avanti fino a quando non si è collegati tutti i nodi mantenendo le proprietà di un albero.}
\end{figure}
	\begin{algorithm}
		\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
		\algtext*{Indent}
		\algtext*{EndIndent}
		\caption{Learn Chow-Liu tree($\mathcal{D}$,X)}
		\label{alg:alg1}
		\begin{algorithmic}[1]
			\Require : il learning set $\mathcal{D}$,il set di variabili X
			\Ensure : ($\mathcal{T}$,$\theta$), un albero $\mathcal{T}$ avente $\theta$ come parametri
			\State MI = 0\textsubscript{$\vert\mathcal{X}\vert$ x $\vert\mathcal{X}\vert$ } \Comment{Inizializzazione matrice}
			\State Per ogni coppia \textit{X\textsubscript{u},X\textsubscript{v}} $\in \mathcal{X}$ :
			\Indent
			\State\textit{MI\textsubscript{u,v}} = calcolaMutuaInformazione(\textit{X\textsubscript{u},X\textsubscript{v}},$\mathcal{D}$)
			\EndIndent
			\State T = MWST(MI)
			\State $\mathcal{T}$ = attraversaAlbero(T) //Costruito scegliendo un nodo random come radice
			\State 	$\theta$ = calcolaParametri($\mathcal{D,T}$)
			\State return ($\mathcal{T,\theta}$)
		\end{algorithmic}
	\end{algorithm}
	~\\
	\begin{minipage}{\textwidth}
		Una volta appresa la struttura dell'albero si procede scegliendo un nodo random come radice. Successivamente si attraversa l'albero definendo i padri e i figli. Una volta calcolati i relativi parametri è possibile sfruttare le relazioni, in essa codificate, per poter calcolare la distribuzione rappresentata dal nuovo albero tramite:\\
		\centerline{$\mathcal{P} (\mathcal{X}) = 
			{\displaystyle \prod_{i=1}^{P} \mathcal{P} (\mathcal{X}\textsubscript{i} | \mathcal{P}a(\mathcal{X}\textsubscript{i}) ) }$ }
		utilizzando la semplice probabilità non condizionata : $\mathcal{P(X\textsubscript{i})}$, per la variabile posta come radice dall'algoritmo di apprendimento.
	\end{minipage}\\
	\subsubsection{Misture di PGM}
	Il compito di questo capitolo è quello di dare un'infarinatura sulle misture di probabilistic graphical model.
	
	Invece di utilizzare un'unica potenziale struttura, si sceglie di trattare il problema tramite una combinazione di semplici PGM, ognuno modellante una distribuzione di probabilità congiunta su tutte le stesse variabili.\\
	Quindi è definita \textit{Mistura (Mixture)}\cite{manuscript} un insieme di $m$ PGM. Ad ogni PGM viene dato un peso \textit{w}.
	
	La densità di probabilità definita da una mistura è calcolata mediando quelle delle differenti densità codificate da ogni PGM:\\
	\centerline{
		$\mathcal{P(X)} = \sum\limits_{i=1}^m w\textsubscript{i}\mathcal{P}\textsubscript{i}(\mathcal{X})$	
	}
	con\\[0.3cm]
	\centerline{
		$\sum\limits_{i=1}^m w\textsubscript{i} = 1 \wedge \forall i: w\textsubscript{i} \geq 0$
	}\\[0.3cm]
	Questo approccio permette l'utilizzo di algoritmi di apprendimento semplici e sfrutta la media le predizioni di ogni PGM per rappresentare delle classi di densità più ricche.
	
	Ogni modello può essere visto come una distribuzione alternativa e la media pesata può essere visto come un modo per trattare il problema in modo differente dove la mancanza di esempi non permette di discriminare.
	
	Un modo di creare queste misture è quello di creare gli $m$ PGM addestrati su altrettanti $m$ versioni del training set. Ogni versione del training set è il risultato di una leggera alterazione applicata al training set originale. Questo permette di far avere alla mistura una visione più generale del problema. La creazione dei training sets alterativi può essere fatta attraverso il bagging\cite{bre96}.\\
	Questo meta-algoritmo consiste nel creare un nuovo training set $D'$ della stessa dimensione $N$ del training set originale $D$ scegliendo $N$ numeri naturali, in maniera uniforme ed indipendente, $r\textsubscript{i} \in [1,N]$ e popolando $D'$ tramite:
	\centerline{
		x\textsubscript{$D'$\textsubscript{i}} = x\textsubscript{$D$\textsubscript{r\textsubscript{i}}} $\forall i \in [1,N']$ 
	}
	con x\textsubscript{$D$\textsubscript{r\textsubscript{i}}} la $j$-esima osservazione del dataset $D$.\\
	Il processo è illustrato nella figura \ref{fig:bagging} presa da \cite{manuscript}.
	\begin{figure}[h]
		\centerline{
			\includegraphics{grafici/bagging.pdf}}
		\caption{Il processo di bagging sceglie in maniera randomica delle osservazioni dal training set originale e le copia nel nuovo training set. Si può notare come nel nuovo training set c'è la possibilità di trovare una stessa osservazione più volte.}
		\label{fig:bagging}
	\end{figure}
\subsubsection{Alberi OR}
 Un albero OR è un albero utilizzato per rappresentare lo spazio di ricerca esplorato durante inferenza probabilistica condizionata \cite{dechter}. Ogni nodo dell'albero rappresenta una variabile aleatoria e ne è etichettato di conseguenza.\\
  Ogni arco uscente da un nodo $v$ rappresenta il condizionamento della variabile nodo $v$ per un certo valore $x_v$ ed è etichettata con la probabilità che la variabile $v$ assuma valore $x_v$ dato il cammino che parte dalla radice ed arriva al nodo. Ogni nodo quindi avrà un numero di archi uscenti pari alla dimensione del dominio dei valori assumibili dalla variabile da esso rappresentata.
 Quindi per ogni inferenza, per ogni nodo si sceglie l'arco uscente etichettato con il valore assunto dalla variabile e si procede per i successivi nodi. Arrivati alla foglia si calcola $\mathcal{P}(x)$ sfruttando il cammino percorso dalla radice.
 La distribuzione rappresentata da un albero OR $T$ equivale a:\\[0.2cm]
 \centerline{
$\mathcal{P}(x) = \prod_{(v\textsubscript{i},v\textsubscript{j})\in path T(x)} w(v_i,v_j)$ 
}\\[0.2cm]
con $path T(x)$ il cammino dalla radice alla foglia $l(x)$ corrispondente all'assegnazione $x$ e $w(v_i,v_j)$ la probabilità con cui è etichettato l'arco tra il nodo $v_i$ e il nodo $v_j$. La produttoria in congiunzione alla struttura dell'albero, rappresenta quindi la probabilità di osservare $x$ calcolata tenendo conto delle dipendenze funzionali rappresentate dalla relazione padre $\rightarrow$ figlio.
\newpage
	\subsubsection{Cutset Networks}
	Le \textit{Cutset Networks} (CNets) sono delle strutture ibride contenenti alberi di Chow-Liu le cui radici sono rappresentate da nodi OR, con nodi OR utilizzati come nodi interni e alberi di Chow-Liu come foglie.
	Sono state presentate da Rahman et al. in \cite{rahman}.
	
	Ogni nodo OR rappresenta una variabile aleatoria \textit{X\textsubscript{i}} ed ogni arco partente da esso rappresenta il condizionamento della variabile \textit{X\textsubscript{i}} per un certo valore \textit{x\textsuperscript{j}\textsubscript{i}} $\in$ \textit{Val (X\textsubscript{i})} pesato con la probabilità \textit{w\textsubscript{i,j}} che la variabile \textit{X\textsubscript{i}} assuma il valore \textit{x\textsuperscript{j}\textsubscript{i}} .
	
	Una cutset network è rappresentata da una coppia $\mathcal{<G, \xi>}$.\\
	Dove $\mathcal{G = O \cup \{T\textsubscript{1},...,T\textsubscript{H}\}}$, con $\mathcal{O}$ un albero OR e $\mathcal{\{T\textsubscript{1},...,T\textsubscript{H}\}}$ le sue foglie di alberi Chow-Liu.\\
	Con $\mathcal{\xi = \pmb{w} \cup \{\theta \textsubscript{1},...,\theta\textsubscript{H}\}}$; dove $\mathcal{\pmb{w}}$ corrisponde ai parametri dell'albero OR e $\mathcal{\{\theta\textsubscript{1},...,\theta\textsubscript{H}\}}$ corrisponde ai parametri degli alberi figli.\\
	Di seguito ne viene data una definizione ricorsiva che ne descrive l'essenza in maniera elegante e sintetica.
	\begin{figure}[h]
		\includegraphics[scale=0.5]{fig/csn.pdf}
		\caption{Una cutset network avente 6 features.\textit{Figura presa da \cite{dimauro}}}
		\label{fig:cutsetn1}
	\end{figure}
	\begin{mdframed}
		\paragraph{Definizione: Cutset Network.}\widowpenalties 1 10000
		\raggedbottom
		~\\Sia $\mathcal{X}$ un insieme di variabili aleatorie discrete, una Cutset Network è:\\
		\pmb{1.} un albero di Chow-Liu avente come scope $\mathcal{X}$\\
		\centerline{oppure}
		\pmb{2.} data $\mathcal{X\textsubscript{i} \in X}$ una variabile con $\vert$Dominio($\mathcal{X\textsubscript{i}}$)$\vert$= k, condizionata graficamente da un nodo OR, una disgiunzione pesata di k cutset networks $\mathcal{G\textsubscript{i}}$ aventi tutte lo scope ($\mathcal{X\textsubscript{$\setminus$i}}$) , dove tutti i pesi w\textsubscript{i,j} con j=1,...,k , si sommano ad 1  e $\mathcal{X\textsubscript{$\setminus$i}}$ denota l'insieme delle $\mathcal{X}$ variabili meno la variabile $\mathcal{X\textsubscript{i}}$ presa come nodo OR.\end{mdframed}
	~\\
	Una cutset network d'esempio è riportata nella figura \ref{fig:cutsetn1}.
	
	Nell'esempio riportato possiamo notare un $\mathcal{X = \{X\textsubscript{1},...,X\textsubscript{6}\} }$ . Ogni variabile ha uno scope la cui dimensione è 2. Sono presenti tre nodi OR, aventi due figli ciascuno, che rappresentano le variabili $\mathcal{X\textsubscript{3},X\textsubscript{5},X\textsubscript{2}}$ , e quattro alberi di Chow-Liu come foglie. Si può notare come al primo livello di profondità della cutset network da un lato sia presente il nodo OR $\mathcal{X\textsubscript{5}}$, dall'altro sia presente già un albero di Chow-Liu. Questo indica che nel caso in cui la variabile $\mathcal{X\textsubscript{3}}$ assuma il valore per cui si scenda verso il figlio sinistro, sia adeguato trattare l'osservazione discriminando subito sulla variabile $\mathcal{X\textsubscript{5}}$; mentre nel caso in cui si scenda verso il figlio destro di $\mathcal{X\textsubscript{3}}$ sia adeguato continuare trattare l'osservazione tramite l'albero di Chow-Liu \textit{$\mathcal{T}$}\textsubscript{1} .
	
	La distribuzione rappresentata da una cutset network è data da:\\\\
	\centerline{$\mathcal{P} (\mathcal{X}) = 
		\Bigg[{\displaystyle  \prod_{(v\textsubscript{i},v\textsubscript{j})\in path O(x)} w(v\textsubscript{i},v\textsubscript{j}) }\Bigg]\cdot
		\Big(T\textsubscript{l(x)}\Big)$ }
	con $pathO(x)$ il cammino dalla radice all'unica foglia $l(x)$ e
	$T\textsubscript{l(x)}$ l'albero di Chow-Liu foglia associato a $l(x)$.
	Nel lavoro originale \cite{rahman} Rahman et al. utilizzano anche il concetto di mistura applicato alle Cutset networks che generalizza misture di alberi di Chow-Liu \cite{meila}, cercando così di migliorare l'accuratezza.\\ %Ad ogni Cutset Network viene dato un peso $\lambda$ e l'inferenza finale è data da:\\[0.2cm]
	%\centerline{
	%$\mathcal{P}(x) = \sum_{i=1}^{k} \lambda_i C_i(x)$	
%}\\[0.2cm]
%con $\lambda_i \geq 0$ per $i=1,...,k$ e $\sum_{i=1}^{k} \lambda_i =1$.
	In questa tesi non è stato fatto utilizzo di misture di CNets benchè disponibili nel progetto di Di Mauro et al. \cite{dimauro}.
	
	\subsubsection{Apprendimento delle Cutset Networks}
	Rahman et al. in \cite{rahman} hanno presentato il primo assoluto algoritmo per l'apprendimento di una cutset network.\\
	L'algoritmo utilizzato sfrutta un approccio \textit{divide et impera} per apprendere in maniera ricorsiva la rete da un dataset $\mathcal{D}$. Si basa sul selezionare una variabile $\mathcal{X'\in X}$, alla volta, usando una determinata \textit{euristica di splitting} ed utilizzare la variabile $\mathcal{X'}$ come radice della cutset network. Da qui generare un arco per ogni valore del dominio dei valori della feature. Quindi ripetere l'approccio, in maniera ricorsiva, su ogni arco utilizzando per ogni arco le variabili aleatorie $\mathcal{X\setminus X'}$ e le sole istanze del dataset il cui valore della feature padre (in questo caso $\mathcal{X'}$) sia quello dell'arco corrispondente.
	
	Ogni volta viene verificato un certo \textit{criterio di terminazione} e se soddisfatto si interrompe l'algoritmo di apprendimento della cutset network e si esegue l'algoritmo classico per l'apprendimento di un albero di Chow-Liu sulle features e istanze rimanenti in quell'arco, così generando una foglia.
	
	L'\textit{euristica di splitting} proposta da Rahman et al. consiste nel calcolare la riduzione attesa in entropia causata dal conoscere il valore di una data feature.\\
	In particolare l'entropia di un dataset $\mathcal{D}$ definito su un set \textit{V} di variabili è dato da:\\
	\centerline{$\hat{\mathcal{H}}(D) = \frac{1}{\vert V\vert}\sum\limits_{v\in V}\mathcal{H\textsubscript{D}}(v) $}\\[0.4cm]
	con l'entropia di un dataset $\mathcal{D}$ relativa ad una variabile \textit{v} :\\[0.4cm]
	\centerline{$\mathcal{H}$\textsubscript{$\mathcal{D}$}($v$) = $- 
		\sum\limits_{x \textsubscript{v}\in \varDelta\textsubscript{v}} \mathcal{P}(x\textsubscript{v})log\mathcal{(P}(x\textsubscript{v}))$}\\[0.4cm]
	Calcolando, quindi, l'\textit{information gain} condizionato ad una variabile \textit{v} usando la seguente espressione:\\[0.2cm]
	\centerline{
		$ Gain$\textsubscript{$\mathcal{D}$}$(v) =\hat{\mathcal{H}}(D) - 
		\sum\limits_{x \textsubscript{v}\in \varDelta\textsubscript{v}}
		\frac{\vert \mathcal{D\textsubscript{\textit{x\textsubscript{v}}}} \vert}
		{\mathcal{D}}\hat{\mathcal{H}}(\mathcal{D}\textit{\textsubscript{x\textsubscript{v}}})$
	}\\[0.4cm]
	con $\mathcal{D}\textit{\textsubscript{x\textsubscript{v}}} = \{x\textsuperscript{i}\in \mathcal{D}\vert x\textsuperscript{i}\textsubscript{v} = v\textsubscript{v}\}$ .\\
	Quindi viene scelta la variabile che ha il più alto information gain.
	
	Come \textit{criterio di terminazione} si può pensare di stoppare lo splitting se il numero di istanze rimaste è inferiore ad un certo threshold oppure se l'entropia è troppo bassa.
	\subsubsection{Decomposability based Cutset Network learning}\label{dcsn}
	Un altro algoritmo di apprendimento per le cutset networks è stato presentato dal Di Mauro et al. in \cite{dimauro}. L'approccio utilizzato da questo algoritmo sfrutta la decomponibilità del likelihood di una cutset network. L'algoritmo riformula la ricerca nello spazio delle strutture come un task di ottimizzazione che massimizza il likelihood direttamente sui dati costruendo la cutset network in maniera ricorsiva.
	
	Il primo passo dell'algoritmo è quello di costruire un singolo albero di Chow-Liu partendo dall'intero training set. Il passo successivo è quello di determinare se sia possibile scegliere una variabile, da cui costruire un nodo OR e appendere ad esso degli alberi di Chow-Liu costruiti sulle restanti variabili, che permetta di ottenere una likelihood migliore rispetto a quella iniziale. Se viene trovata tale variabile allora si procede alla decomposizione, sostituzione della nuova struttura alla vecchia e applicazione dell'algoritmo in maniera ricorsiva sui figli del nodo OR appena creato.\\
	La scelta di uno splitting viene fatta attraverso l'utilizzo del Bayesian Information Criterion (BIC) come in \cite{friedman}. In particolare si misura la differenza di BIC fra la vecchia e la nuova struttura. Il BIC è un criterio per la selezione di un modello tra una classe di modelli e rappresenta una forma di regolarizzazione. Il valore BIC per una cutset network è:\\[0.2cm]
	\centerline{score\textsubscript{BIC}($\langle\mathcal{G},\gamma \rangle$) = 
		log$\mathcal{P\textsubscript{D}(\langle G,\gamma \rangle)}$ - 
		$\frac{\text{log} M}{2}Dim(G)$
	}\\[0.2cm]
	con Dim($\mathcal{G}$) il numero di nodi OR presenti in $\mathcal{G}$. L'utilizzo di un altro criterio come il BDe\cite{heckerman} è anche possibile
	
	Quindi date due cutset networks $\mathcal{G}$ e $\mathcal{G'}$, con $\mathcal{G'}$ ottenuta da $\mathcal{G}$ sostituendo un albero foglia con una nuova cutset network avente come radice un nodo OR, :\\
	\centerline{
		score\textsubscript{BIC}($\langle\mathcal{G'},\gamma' \rangle$) -
		score\textsubscript{BIC}($\langle\mathcal{G},\gamma \rangle$) =}\\[0.2cm] 
	\centerline{
		\textit{l}\textsubscript{$\mathcal{D}$}
		$(\langle \mathcal{G'},\gamma' \rangle)$ -
		\textit{l}\textsubscript{$\mathcal{D}$}$(\langle \mathcal{G},\gamma \rangle)$ -
		$\frac{\text{log} M}{2}(Dim(\mathcal{G'}) - Dim(\mathcal{G}))$ =
	}\\[0.2cm]
	\centerline{
		\textit{l}\textsubscript{$\mathcal{D}$}
		$(\langle \mathcal{G'},\gamma' \rangle)$ -
		\textit{l}\textsubscript{$\mathcal{D}$}$(\langle \mathcal{G},\gamma \rangle)$ -
		$\frac{\text{log} M}{2}(1)$ =
	}\\[0.2cm]
	\centerline{
		\textit{l}\textsubscript{$\mathcal{D}$}
		$(\langle \mathcal{G'},\gamma' \rangle)$ -
		\textit{l}\textsubscript{$\mathcal{D}$}$(\langle \mathcal{G},\gamma \rangle)$ -
		$\frac{\text{log} M}{2}$
	}~\\[0.3cm]
	$\mathcal{G'}$ è accettata se :\\
	\begin{mdframed}
		\centering \textit{l}\textsubscript{$\mathcal{D}$}
		$(\langle \mathcal{G'},\gamma' \rangle)$ -
		\textit{l}\textsubscript{$\mathcal{D}$}$(\langle \mathcal{G},\gamma \rangle) >
		\frac{\text{log} M}{2}$ 
	\end{mdframed}
	Per tenere aggiornato il likelihood della cutset network sarà necessario solamente rivalutare il likelihood locale e sommarlo a quello della rete restante, sfruttando la decomponibilità del likelihood. La decomposizione di un albero $\mathcal{T\textsubscript{l}}$ è indipendente da quella di un altro albero $\mathcal{T\textsubscript{k}}, k\neq l$ dato che le loro contribuzioni al likelihood globale sono indipendenti; quindi non è importante l'ordine con cui decomporre le foglie.
	
	Come criterio di terminazione si utilizza il numero minimo di istanze del training set rimaste su cui splittare e/o il numero minimo di features rimaste.
	\begin{figure}[p]
		\includegraphics[height=\dimexpr \textheight - 4\baselineskip\relax]{cnetlearning/document.pdf}
		\caption{Alcuni passi di dcsn. I nodi rossi rappresentano la variabili per cui la decomposizione produce un aumento nel likelihood. I nodi tratteggiati rappresentano nodi OR. La cutset network finale, in questo caso, contiene 2 nodi OR (X\textsubscript{2} e X\textsubscript{4}) e 3 alberi di Chow-Liu come foglie. Ogni variabile può assumere solo 2 valori.}
	\end{figure}
	\begin{algorithm}
		
		\algtext*{Indent}
		\algtext*{EndIndent}
		\caption{dCSN($\mathcal{D}$,X,$\alpha$\textsubscript{\textit{f}},$\delta$,
			$\sigma$)}
		\begin{algorithmic}[1]
			\label{alg:alg2}
			\Require : il learning set $\mathcal{D}$,il set di variabili X,
			$\alpha$\textsubscript{\textit{f}}$\in [0,1],\delta$ numero minimo di istanze su cui decomporre, $\sigma$ minimo numero di features su cui decomporre
			\Ensure : una cutset network $\langle \mathcal{G, \gamma}\rangle$ appresa su X attraverso $\mathcal{D}$ 
			\State $\alpha$ =  $\alpha$\textsubscript{\textit{f}}$\vert\mathcal{D}\vert$
			\State $\langle \mathcal{T,\theta} \rangle$ = Learn Chow-Liu tree($\mathcal{D}$,X,$\alpha$)
			\State $w$ = 0
			\State $\langle \mathcal{G,\gamma}\rangle$ = decompose($\mathcal{D}$,X,$\alpha,\mathcal{T},\theta,w,\delta,\sigma$)
		\end{algorithmic}
	\end{algorithm}
	
	L'algoritmo in pseudocodice è riportato nell'algoritmo \hyperref[alg:alg2]{2}. La prima operazione è quindi l'apprendimento di un albero di Chow-Liu(riga 2). Successivamente si chiama l'algoritmo di decomposizione (algoritmo \hyperref[alg:alg3]{3}), passando in input l'albero appena creato.
	
	Alla riga 1 vengono verificate le condizioni per poter tentare di decomporre la struttura; ovvero la presenza di almeno $\delta$ istanze e di $\sigma$ features. In caso positivo si inizia a testare la decomposizione usando ogni variabile. Generato un nodo OR con la variabile X\textsubscript{i}, per ogni valore del dominio di X\textsubscript{i} si genera un arco e si apprende un albero di Chow-Liu con il rispettivo slice del training set. Dopo aver testato tutte le features si cerca di ottenere una decomposizione utilizzando la miglior variabile trovata che generi una nuova struttura avente log-likelihood migliore di quello corrente. In caso positivo (riga 14) si tenta di decomporre, in maniera ricosriva, i nuovi alberi di Chow-Liu della nuova cutset network (riga 19).\\ L'algoritmo termina quando almeno uno dei due criteri di terminazione è soddisfatto.
	
	
	
	\begin{algorithm}
		
		\algtext*{Indent}
		\algtext*{EndIndent}
		\caption{decompose($\mathcal{D}$,X,$\alpha,\mathcal{T},\theta,w,\delta,\sigma$)}
		\begin{algorithmic}[1]
			\label{alg:alg3}
			\Require : il learning set $\mathcal{D}$,il set di variabili X,
			$\alpha$\textsubscript{\textit{f}}$\in [0,1],\delta$ numero minimo di istanze su cui decomporre, $\sigma$ minimo numero di features su cui decomporre, $\mathcal{T}$ l'albero da decomporre e i suoi parametri $\theta$
			\Ensure : una cutset network $\langle \mathcal{G,\gamma}\rangle$ appresa su X attraverso $\mathcal{D}$
			\If{$\vert\mathcal{D}\vert > \delta$ and $\vert X\vert > \sigma$}
			\State \textit{l}\textsubscript{best} = -$\infty$
			\For{X\textsubscript{i}$\in$X}
			\State $\mathcal{G}$\textsubscript{i}=0,$w$\textsubscript{i}=0,
			$\theta$\textsubscript{i}=0,C\textsubscript{i} nodo OR associato a X\textsubscript{i}
			\For{$x$\textsuperscript{j}\textsubscript{i}$\in$ Val(X\textsubscript{i})}
			\State $\mathcal{D}$\textsubscript{j}=\{ $\xi\in\mathcal{D} : \xi[X\textsubscript{s}] = x\textsubscript{s}\textsuperscript{j}$ \}
			\State $w$\textsubscript{i}\textsubscript{j}=$\vert\mathcal{D\textsubscript{j}}/\vert\mathcal{D}\vert$
			\State $\langle \mathcal{T\textsubscript{j}},\theta\textsubscript{ij} \rangle$ = Learn Chow-Liu Tree($\mathcal{D\textsubscript{j}},$X\textsubscript{$\setminus s$},$\alpha w$\textsubscript{$ij$})
			\State $\mathcal{G\textsubscript{i}}$ = aggiungiSottoAlberi(C\textsubscript{i},$\mathcal{T\textsubscript{j}}$)
			\State $w\textsubscript{i} = w\textsubscript{i} \cup \{ w\textsubscript{ij}\},\theta\textsubscript{i} = \theta\textsubscript{i} \cup \{ \theta\textsubscript{ij}\}  $
			
			\EndFor
			\State \textit{l\textsubscript{i}} = \textit{l}\textsubscript{D\textsubscript{i}}($\langle\mathcal{G\textsubscript{i}}$,$w\textsubscript{i} \cup \theta\textsubscript{i}\rangle$)
			\If{\textit{l\textsubscript{i}}$>$\textit{l\textsubscript{best}} and \textit{l\textsubscript{i}} $>$ \textit{l}\textsubscript{D\textsubscript{i}}($\langle\mathcal{T,\theta}\rangle$)}
			\State \textit{l\textsubscript{best}} = \textit{l\textsubscript{i}}, X\textsubscript{best}=X\textsubscript{i},$\mathcal{G\textsubscript{best}= G\textsubscript{i},\theta\textsubscript{best}=\theta\textsubscript{i}}$,$w$\textsubscript{best}=$w$\textsubscript{i}
			\EndIf
			\EndFor
			\If{\textit{l}\textsubscript{best} - \textit{l}\textsubscript{$\mathcal{D}$}($\langle\mathcal{T,\theta}\rangle$) $>$ ($log\vert\mathcal{D}\vert$)/2}
			\State sostituisci $\mathcal{T}$ con $\mathcal{G\textsubscript{i}}$
			\State $w = w \cup w\textsubscript{best}$
			\For{$x$\textsuperscript{j}\textsubscript{b}$\in$Val(x\textsubscript{best})}
			\State $\mathcal{D\textsubscript{j}}$ = \{$\xi\in\mathcal{D} : \xi[X\textsubscript{best}]$\}
			\State decompose($\mathcal{D\textsubscript{j}}$,X\textsubscript{$\setminus best$},$\alpha w\textsubscript{ij},\mathcal{T\textsubscript{j}},\theta\textsubscript{j},w,\delta,\sigma$)
			\EndFor
			\EndIf
			\EndIf
		\end{algorithmic}
	\end{algorithm}
	
	
	
	\newpage
	\section{Tecniche di Stochastic Local Search}\label{sls}
	Gli argomenti cuore di questa tesi saranno descritti in questa sezione. L'obiettivo di questa sezione è quello di presentare algoritmi stocastici di ricerca locale (sez. \ref{ricloc} e \ref{stoc}). In particolare saranno presentati nel dettaglio:
	Iterative improvement (sez. \ref{iip}), Randomised iterative improvement (sez. \ref{riip}) e Greedy randomised 'adaptive' search procedure (GRASP) (sez. \ref{grasppp}).
	\subsection{Ricerca locale e problemi combinatoriali}\label{ricloc}
	Prima di descrivere ed affrontare la tipologia di problemi discussi in questo capitolo è utile descrivere un problema in termini di spazio di input, spazio delle soluzioni e spazio di output.\\ Si definisce lo \textit{spazio di input} l'insieme dei valori assumibili dalle variabili di input del problema. Inoltre si definisce lo \textit{spazio delle soluzioni} come l'insieme dei valori, per alcune variabili del problema, che costituiscano una soluzione del problema. Lo \textit{spazio di output} costituisce l'insieme delle informazioni in uscita si vuole che il processo risolutivo produca.
	
	In natura esistono diverse tipologie di problemi. Una tipologia di problemi per cui trovare la soluzione ottima non è un compito immediato è rappresentato dai $problemi$ $combinatoriali$.\\
	I problemi combinatoriali si presentano in molte aree come l'intelligenza artificiale, ricerca operativa, bioinformatica e commercio elettronico. La loro caratteristica comune è la crescita esponenziale del numero di soluzioni possibili, per una certa istanza di problema, all'aumentare della dimensione della specifica istanza. Questo fa sì che un approccio esaustivo nella ricerca di una soluzione accettabile non sia fattibile.
	
	Un esempio di problema combinatoriale è quello di trovare il percorso più breve o meno costoso in un grafo (problema del commesso viaggiatore). Un altro è quello di trovare il minimo albero ricoprente di un grafo (Minimum Spanning Tree). Altri problemi combinatoriali si incontrano in pianificazione, scheduling, allocazione di risorse, code design, hardware design e nel sequenzionamento del genoma.\\ Questo tipo di problemi di solito richiede il raggruppamento, l'ordinamento o l'assegnazione di insiemi finiti di oggetti che soddisfano certe condizioni. Combinazioni di queste componenti formano potenziali soluzioni finali.
	
	Spesso ci interessa trovare la miglior soluzione dello spazio delle soluzioni, spesso determinarla in maniera diretta è quasi impossibile. In questo tipo di problemi ci si limita a trovare una soluzione ragionevolmente buona cercando di ottimizzare soluzioni iniziali non ottime. Questo tipo di problema appartiene alla categoria dei problemi di \textit{ottimizzazione}.\\ Nei problemi di ottimizzazione si utilizza una funzione detta \pmb{funzione obiettivo}, il cui compito è di associare ad un elemento dello spazio delle soluzioni un valore che è direttamente collegato alla qualità della soluzione. Quindi per ottimizzare la soluzione di un problema ci si pone l'obiettivo di utilizzare algoritmi che minimizzino o massimizzino il più possibile la funzione obiettivo partendo da una soluzione iniziale.
	
	La crescente complessità dei problemi combinatoriali ci spinge ad assumere approcci risolutivi che possono essere caratterizzati come \textit{algoritmi di ricerca}. L'idea fondamentale dietro l'approccio di ricerca è di generare e valutare iterativamente delle soluzioni candidate; dove con ''valutare'' s'intende stabilire il corrispettivo valore della funzione obiettivo. Questo è possibile grazie al fatto che, pur essendo i problemi combinatoriali di complessità elevata, la valutazione delle loro soluzioni candidate è spesso possibile più efficientemente in tempo polinomiale.\\
	Quindi per ''ricerca'' s'intende la ricerca di una soluzione, secondo un certo criterio, nello \textit{spazio delle possibili soluzioni}.
	
	Si può immaginare lo spazio delle possibili soluzioni come un grafo in cui ogni nodo rappresenta una possibile soluzione ed ogni arco, collegante due soluzioni, rappresenti un ''passo'' nello spazio di ricerca.
	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{fig/local_search.pdf}
		\caption{Presa da \cite{slsbook}}
		\label{fig:localsearch}
	\end{figure}
	Si può immaginare una soluzione candidata come il nodo etichettato con ''C'' nella figura \ref{fig:localsearch}. Il compito di un algoritmo di ricerca è quello di cercar di raggiungere o comunque avvicinarsi il più possibile alla soluzione ottimalmente globale, rappresentata dal nodo ''S''.
Una volta posizionati in una soluzione non ancora ottima, gli algoritmi di ricerca adottano diverse tecniche per poter decidere in che direzione andare. In base a queste tecniche essi sono suddivisi in diverse categorie: a seconda di come rappresentano le soluzioni, a seconda di come le generano e a seconda dell'uso che fanno delle informazioni della soluzione corrente.
	\subsubsection{Paradigmi di ricerca}~\\
	Gli approcci di ricerca ricadono in diversi paradigmi, ovvero diversi sotto-approcci con cui gli algoritmi di ricerca generano soluzioni candidate.
	
	Gli algoritmi di ricerca presenti in questa sezione appartengono al paradigma \textit{perturbativo}. Questo paradigma sfrutta la proprietà delle soluzioni per cui, esse sono formate a loro volta da un insieme di \textit{componenti}. Ad esempio una soluzione per il problema del commesso viaggiatore è composta da componenti che rappresentano i singoli archi che compongono il percorso finale.\\
	Trattando le soluzioni come insiemi di componenti, gli algoritmi perturbativi generano facilmente nuove soluzioni modificando una o più componenti di una soluzione avente a disposizione.
	
	Un altro paradigma è quello \textit{generativo}, dove si sceglie di generare una soluzione componente per componente. Questo tipo di ricerca ha luogo nello spazio di ricerca includendo anche le soluzioni \textit{parziali}, ovvero, soluzioni per cui alcune componenti sono mancanti.\\
	La generazione di nuove soluzioni è quindi ottenuta estendendo iterativamente le soluzioni candidate migliorando la funzione obiettivo.
	
	Altri due tipi di paradigmi sono quello \textit{sistematico} e quello \textit{locale}.\\
	Gli algoritmi di ricerca sistematica sono algoritmi che attraversano lo spazio di ricerca in maniera completa, garantendo il ritrovamento della soluzione ottimale (se presente).
	Gli algoritmi di ricerca locale, d'altra parte, partono ad una determinata posizione dello spazio di ricerca e successivamente si muovono dalla posizione corrente verso posizioni vicine basandosi esclusivamente su conoscenza \textit{locale}. Codesti algoritmi sono quindi incompleti, ovvero, non garantiscono il ritrovamento di una soluzione ottima (se presente) dato che possono non visitare tutto lo spazio di ricerca o addirittura visitare la stessa posizione più di una volta .
	
	Quindi, perché utilizzare la ricerca locale? Un motivo è rappresentato dalla natura costruttiva di molti problemi. In queste situazioni è chiaro che l'obiettivo di un algoritmo risolutivo è quello di generare una soluzione e non cercarne una nello spazio di tutte le possibili soluzioni. Un altro motivo è rappresentato dal tempo di risoluzione. Quasi tutti i problemi del mondo reale sono legati indissolubilmente al tempo. La ricerca esaustiva attuata dagli algoritmi sistematici è quasi sempre impraticabile dato che visitare l'intero spazio delle soluzioni per trovare una soluzione ottima impiegherebbe un tempo di gran lunga superiore a quello richiesto dalla tipologia di applicazione. Gli algoritmi di ricerca locale, invece, sono capaci di trovare comunque una soluzione in tempi brevi. La loro natura iterativa permette il ritrovamento di soluzioni man mano migliori, garantendo una risposta in tempi brevi al contrario degli algoritmi sistematici che potrebbero abortire la ricerca dopo il quanto di tempo avuto a disposizione. Idealmente algoritmi per problemi real-time dovrebbero essere in grado di dare una buona soluzione a qualsiasi punto della loro esecuzione.
	\subsection{Ricerca stocastica}\label{stoc}
	Molti algoritmi di ricerca locale fanno uso di scelte randomiche nella generazione e selezione di soluzioni candidate. Questi algoritmi sono chiamati \textit{algoritmi stocastici di ricerca locale (stochastic local search - SLS) \cite{slsbook}}.
	
	Come per gli algoritmi di ricerca locale normale, essi selezionano una soluzione iniziale e procedono iterativamente verso soluzioni ''vicine'' nello spazio delle soluzioni basandosi su conoscenza limitata e locale. La differenza chiave è che gli algoritmi stocastici fanno uso di randomicità, includendola nel processo di generazione iniziale della soluzione e nel processo di decisione della soluzione vicina verso cui muoversi.
	\begin{mdframed}
		\pmb{Definizione:} \underline{Algoritmo stocastico di ricerca locale}. Dato un problema (combinatoriale) $\varPi$, un algoritmo stocastico di ricerca locale per la risoluzione di un'arbitraria istanza  $\pi \in \varPi$  è definito dalle seguenti componenti:\\
		\begin{itemize}
			\item lo spazio di ricerca S($\pi$) dell'istanza $\pi$, ovvero l'insieme di soluzioni candidate.
			\item un insieme di soluzioni attuabili S'($\pi$)$\subseteq$ S($\pi$)
			\item una funzione neighbourhood su S($\pi$), N($\pi$)$\subseteq$ S($\pi$) x S($\pi$), atta ad identificare le soluzioni ''vicine'' nello spazio delle soluzioni
			\item un insieme finito di stati di memoria M($\pi$), per algoritmi facenti uso di uno storico degli stati
			\item una funzione di inizializzazione init($\pi$): 0 $\rightarrow$ $\mathcal{D}$(S($\pi$) x M($\pi$)), che specifica una distribuzione di probabilità sulle posizioni iniziali
			\item una funzione scalino step($\pi$): S($\pi$) x M($\pi$)$\rightarrow$ $\mathcal{D}$(S($\pi$) x M($\pi$)), che fa un mapping tra le posizioni e gli stati di memoria in una distribuzione di probabilità sulle posizioni vicine e sugli stati di memoria
			\item un predicato di terminazione\\ terminate($\pi$): S($\pi$) x M($\pi$) $\rightarrow$ $\mathcal{D}$(true,false), che fa il mapping da ogni soluzione e stato di memoria a una distribuzione di probabilità sui valori booleani indicando la probabilità di terminazione della ricerca
		\end{itemize}
	\end{mdframed}
	Ogni relazione di vicinanza N($\pi$) può essere trasformata equivalentemente in una funzione Neighbourhood: S($\pi$) $\rightarrow$ 2\textsuperscript{S($\pi$)} che si occupa di fare il mapping tra una soluzione candidata s$ \in $S e il suo insieme di soluzioni direttamente ''vicine'' N(s) := \{s'$ \in $S $\vert$ N(s,s')\} $\subseteq$S.\\L'insieme N(s) è chiamato \textit{vicinato (Neighbourhood)} . In generale la scelta della relazione di vicinanza tra soluzioni è cruciale per la performance dell'algoritmo SLS. Fra le diverse relazioni più popolari si trova il \textit{k-exchange neighbourhoods}, che afferma che due soluzioni candidate sono vicine se, e solo se, esse differiscono di almeno \textit{k} componenti.
	
	Avendo dato informazioni di base e sufficienti sugli algoritmi stocastici di ricerca locale, si procede con la descrizione degli algoritmi utilizzati nel lavoro di tesi.
	
	\subsection{Iterative improvement}\label{iip}
	Il più semplice algoritmo stocastico di ricerca locale di questa tesi è \textit{Iterative improvement}. Dato uno spazio di ricerca S, una relazione di vicinato N e una funzione obiettivo g, Iterative Improvement parte da una soluzione random dello spazio di ricerca e cerca di migliorare la soluzione candidata corrente rispetto alla funzione g. La versione in pseudocodice è rappresentata dall'algoritmo \ref{algii}.
	\begin{algorithm}
		\caption{Iterative improvement}
		\label{algii}
		\begin{algorithmic}[1]
			\State Determina una soluzione candidata iniziale $s$
			\While{$s$ non è un ottimo locale}
			\State Scegli una soluzione $s'$ nel vicinato di $s$ tale che g($s'$) sia migliore di g($s$)
			\State $s$:=$s'$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	
	L'approccio utilizzato da Iterative Improvement prevede una scansione dei vicini di una soluzione e successivamente una valutazione della qualità di ognuno di essi. Alla fase di valutazione segue la fase in cui si sceglie la soluzione vicina che abbia il più alto valore della funzione obiettivo. Infine ci si sposta su di essa se il suo valore della funzione obiettivo è migliore del valore della funzione obiettivo calcolata sulla soluzione corrente. Questo ciclo viene fatto fino a quando si hanno dei miglioramenti.\\
	Per implementare efficientemente iterative improvement, i valori della funzione obiettivo sono tipicamente trattati usando \textit{aggiornamenti incrementali} dopo ogni ciclo di ricerca. In molti problemi il valore della funzione obiettivo è formato dai contributi indipendenti delle componenti delle soluzioni. Quindi, per calcolare gli effetti della soluzione $s'$ sulla funzione obiettivo, ci si limita a considerare il contributo delle componenti non in comune tra $s$ e $s'$.
	
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
		\node [draw,rounded corners=0.1cm] (1) at(0,0) {Costo: 3};
		\node [draw,rounded corners=0.1cm,right=.1cm of 1](2) {Costo:5};
		\node [draw,rounded corners=0.1cm,right=.1cm of 2](3) {Costo:1};
		\node [draw=none,below=.1cm of 1](4) {1};
		\node [draw=none,below=.1cm of 2](5) {2};
		\node [draw=none,below=.1cm of 3](6) {3};
		
		\node [draw=gray,rectangle,rounded corners,fit=(1) (2) (3)(4)(5)(6)] (r1) {};
		\node[draw=none,below=.2cm of r1] (nn){$s$};
		\node [draw,rounded corners=0.1cm] (11) at(6,0) {Costo: 3};
		\node [draw,rounded corners=0.1cm,right=.1cm of 11](21) {Costo:1};
		\node [draw,rounded corners=0.1cm,right=.1cm of 21](31) {Costo:1};
		\node [draw=none,below=.1cm of 11](41) {1};
		\node [draw=none,below=.1cm of 21](51) {2};
		\node [draw=none,below=.1cm of 31](61) {3};
		
		\node [draw=gray,rectangle,rounded corners,fit=(11) (21) (31)(41)(51)(61)] (r2) {};
		\node[draw=none,below=.2cm of r2] (nna){$s'$};
		
		
		\end{tikzpicture}
	
		\caption{In questa figura le soluzioni $s$ e $s'$ differiscono solamente per la componente 2. Quindi per capire se conviene scegliere $s'$ al posto di $s$ è efficiente misurare solamente i costi delle componenti non in comune. In questo caso la componente 2 di $s'$ ha un costo minore rispetto alla componente 2 di $s$; essendo le componenti indipendenti il cambiamento di costo della componente 2 non influenza quello delle altre componenti 1 e 3.}
		\label{fig:ii}
	\end{figure}
	Quindi nella figura \ref{fig:ii} dopo aver constatato che il costo della soluzione $s'$ è minore rispetto a quello corrente, iterative improvement si sposta verso $s'$.
	
	Iterative improvement può sfruttare due tipi di approcci nella scelta della posizione verso cui dirigersi nello spazio di ricerca. Questi due tipi differiscono soprattutto nell'efficienza.
	
	Uno di questi è \textit{Iterative Best Improvement} (anche chiamato Greedy Hill Climbing), basato sull'idea di scegliere il vicino che causi il maggior miglioramento della funzione obiettivo. Questa tipologia prevede, quindi, una completa valutazione di tutte le soluzioni del vicinato in ogni ciclo di ricerca e la scelta del miglior vicino.
	
	L'altro è \textit{First Improvement}, dove la selezione di una soluzione vicina tenta di evitare la complessità di valutare tutti i vicini scegliendo la \textit{prima} soluzione che migliora il valore della funzione obiettivo. 
	
	\paragraph{Minimo locale e problemi annessi.}
	In Iterative Improvement la strategia di scelta della soluzione verso cui muoversi non è ben adatta nei casi in cui le soluzioni candidate non abbiano vicini che migliorino il valore della funzione obiettivo. Una soluzione candidata avente questa proprietà è chiamata \textit{minimo locale}, e la sua definizione formale segue:\\
	\begin{mdframed}
		\paragraph{Definizione: Minimo locale.} Sia S uno spazio di ricerca, S' $\subseteq$ S un insieme di soluzioni, N $\subseteq$ S x S una relazione di vicinato e g: S $\rightarrow$ $\mathbb{R}$ una funzione obiettivo, una soluzione candidata s $\in$ S è un minimo locale se $\forall s' \in N(s), g(s)\leq g(s')$ (con $a \leq b$ rappresentante $a$ soluzione migliore rispetto a $b$). Un minimo locale è detto \textit{strettamente minimo locale} se $\forall s' \in N(s), g(s) < g(s')$.
	\end{mdframed}
	
	Intuitivamente un minimo locale è una posizione nello spazio di ricerca dove nessun passo può portare al miglioramento della funzione obiettivo. In questi casi, quando un algoritmo incontra un minimo locale rispetto ad una certa funzione obiettivo, esso rimane ''intrappolato''.
	
	In letteratura esistono diversi approcci atti ad evadere da queste situazioni. Molti si servono dell'utilizzo di randomicità, includendola nelle fasi di decisione e valutazione. Un algoritmo che sfrutta questo approccio e che è direttamente collegato ad iterative improvement è Randomised Iterative Improvement.
	\subsection{Randomised iterative improvement}\label{riip}
	Una delle più semplici vie per estendere iterative improvement per evadere da minimi locali è quello di includere randomicità nella scelta della prossima soluzione. Quindi, invece di scegliere un vicino che migliori la qualità della soluzione corrente, si sceglie un vicino random.
	
	La frequenza con cui scegliere un vicino in maniera random può essere determinata dal programmatore. Quindi ad ogni ciclo di ricerca è preferibile determinare in maniera probabilistica se applicare un passo di iterative improvement oppure applicare un passo di scelta randomica.
	
	Questo viene fatto introducendo un parametro $p \in [0,1]$, chiamato \textit{walk probability o noise parameter}, che corrisponde alla probabilità di effettuare una scelta random rispetto ad una scelta non-random.
	
	L'introduzione di questa scelta dà vita a \textit{Randomised Iterative Improvement (RII)}.
	\begin{algorithm}
		\caption{Randomised Iterative improvement($\pi$,s,p)}
		\label{riipseudo}
		\begin{algorithmic}[1]
			\Require $\pi$: l'istanza del problema, s una soluzione iniziale, p il noise parameter
			\Ensure una soluzione candidata s'
			\State u:= random([0,1])
			\If{ u $\leq$ p}
			\State s':= scelta\_random($\pi$,s)
			\Else
			\State s':= scelta\_iterative\_improvement($\pi$,s)
			\EndIf
			\Return return s'
		\end{algorithmic}
	\end{algorithm}
	
	Quindi un effetto benefico della scelta random è che nel caso in cui si incorra in un ottimo locale, RII permette di evadere anche da ottimi locali aventi un grande ''bacino di attrazione'' nel senso che molti passi peggiorativi possano essere richiesti per assicurare che i successivi miglioramenti abbiano la chance di portare al raggiungimento di un differente ottimo locale.
	
	Inoltre è stato dimostrato che, quando la procedura di ricerca è eseguita tanto abbastanza, RII permette il raggiungimento di un'eventuale soluzione ottimale per ogni tipologia di problema e per arbitarie alte probabilità.
	
	L'algoritmo \ref{riipseudo} presenta una versione in pseudocodice di RII. Si può notare come l'algoritmo verifichi la possibilità di effettuare un passo random nello spazio di ricerca valutando il valore della variabile \textit{u} e confrontandolo con \textit{p} che rappresenta la probabilità di effettuare una scelta random. In caso contrario la soluzione parziale \textit{s'} è scelta attraverso Iterative improvement.
	\subsection{Greedy Randomised Adaptive Search Procedures}\label{grasppp}
	Una famiglia di algoritmi stocastici di ricerca locale è rappresentata da tutti quegli algoritmi i cui approcci come risultato di combinazioni di approcci più semplici. La combinazione di due o più approcci caratterizza la famiglia degli \textit{algoritmi stocastici di ricerca locale ibridi}.
	
	Il comportamento e le performance degli algoritmi stocastici ''semplici'' possono essere spesso migliorati significativamente combinando diverse strategie.
	
	Un approccio standard per trovare velocemente delle soluzioni di alta qualità è quello di applicare metodi greedy di ricerca alle fasi di costruzione degli algoritmi. Questi metodi, ad ogni step costruttivo, aggiungono una componente solutiva scelta come migliore in un insieme di altre componenti utilizzando una funzione euristica di selezione. Successivamente utilizzano ricerca locale perturbativa per migliorare la qualità della soluzione appena costruita.
	
	Un difetto comune di questi metodi è che la fase costruttiva greedy genera solamente un numero limitato di soluzioni candidate.
	
	Per evitare questo difetto le \textit{Greedy Randomised Adaptive Search Procedures (GRASP)} includono la randomizzazione nel metodo di costruzione in modo tale da generare molte più potenziali buone soluzioni da poter poi passare ad un metodo di ricerca locale.
	
	Un algoritmo di base di GRASP è riportato nell'algoritmo \ref{alg:grasp}.
	\begin{algorithm}
		\caption{GRASP($\pi$)}
		\begin{algorithmic}[1]
			\Require $\pi$: l'istanza del problema
			\Ensure una soluzione candidata s' oppure 0
			\State $s$ := 0
			\State $\hat{s}$ := $s$
			\State f($\hat{s}$) := $\infty$
			\While{not (terminate($\pi$,$s$))}
			\State $s$ := costruisci($\pi$)
			\State $s'$ := ricercaLocale($\pi$,$s$)
			\If{ f($s'$) $<$ f($s'$)}
			\State $\hat{s}$ := $s'$
			\EndIf
			\EndWhile
			\If{$\hat{s} \in $S' }
			\State \Return $\hat{s}$
			\Else \State\Return 0
			\EndIf
		\end{algorithmic}
		\label{alg:grasp}
	\end{algorithm}
	
	In GRASP in ogni iterata si genera una soluzione candidata tramite una procedura di ricerca costruttiva randomizzata (costruisci() - riga 5). Successivamente si applica un algoritmo di ricerca locale alla soluzione appena generata. Se alla fine della ricerca si è trovata una soluzione migliore rispetto alla soluzione corrente allora si aggiorna $s$ e si ripete il ciclo fino a quando un criterio di terminazione viene verificato.
	
	L'algoritmo di costruzione usato in GRASP, invece che scegliere man mano la miglior componente solutiva rispetto ad una determinata euristica, seleziona randomicamente una componente all'interno di un set di componenti aventi comunque alta qualità. Questo set di componenti è chiamato \textit{Restricted Candidate List (RCL)}. Quindi, per ogni passo costruttivo GRASP genera una RCL di componenti e ne sceglie una in maniera random secondo una distribuzione uniforme di probabilità. La RCL può essere generata secondo due meccanismi: \textit{restrizione di cardinalità o restrizione di valore}. Nel caso della restrizione di cardinalità la RCL contiene le migliori $k$ componenti. Nel caso della restrizione di valore la RCL contiene le componenti la cui qualità supera un certo valore threshold.
	
	Effettuate tutte le fasi costruttive, la procedura $costruisci$ fornisce una soluzione che rappresenta la soluzione iniziale per l'algoritmo di ricerca locale (riga 6). L'algoritmo di ricerca locale cerca di ottimizzare questa soluzione. Nella riga 7 si verifica se la soluzione appena trovata è migliore della miglior soluzione precedentemente trovata. In caso positivo si aggiorna $\hat{s}$.
	
	GRASP è 'adattivo' perché di solito, nel passo costruttivo, il valore euristico di ogni componente solutiva dipende da quello delle componenti già facenti parte della soluzione parziale in costruzione.
	
	In generale, comunque, non è garantito che le soluzioni candidate create dal passo costruttivo randomizzato siano degli ottimi locali rispetto a delle semplici soluzioni di un vicinato.
	\newpage
	\section{Stochastic Local Search per Cutset Network}\label{appsls}
	Gli algoritmi presentati nella precedente sezione sono stati applicati per migliorare ed ottimizzare le soluzioni del problema dell'apprendimento di cutset networks. Questa sezione ha
	il compito di descrivere, per ogni algoritmo, la sua applicazione al problema specifico. Prima di descrivere gli algoritmi viene data una motivazione ed alcune osservazioni riguardo
	le modifiche all'algoritmo originale dCSN (sez. \ref{for} e \ref{noise}).
	\subsection{Apprendimento di foreste}\label{for}
	Come descritto nella sezione \ref{dcsn},  l'algoritmo di apprendimento di una cutset network, basato sulla decomponibilità, apprende degli alberi di Chow-Liu alle foglie della cutset network.
	
	Una nuova modifica applicata all'algoritmo originale è stata quella di apprendere delle foreste invece che semplici alberi.\\
	Una foresta non è altro che un insieme di alberi i cui insiemi di vertici sono disgiunti. Formalmente:\\
	\begin{mdframed}
		\paragraph{Definizione: Foresta di Chow-Liu}~\\$f$ è una foresta se:\\
		- $f = \{\mathcal{T\textsubscript{1},..,T\textsubscript{k}}\}$ con $t$ albero di Chow-Liu $\forall t \in \{\mathcal{T\textsubscript{1},..,T\textsubscript{k}}\}$
		\\- $\forall t_i,t_j \in \{\mathcal{T\textsubscript{1},..,T\textsubscript{k}}\}$, $V(t_i) \cap V(t_j) = \emptyset $, con $V(x)$ insieme dei nodi dell'albero $x$
		\\- $\bigcup\limits_{i=1}^{k} V(t_i) = V$ con $V$ l'insieme originale delle variabili aleatorie
	\end{mdframed}
	
	Una foresta avente 1 solo albero corrisponde ad un albero di Chow-Liu.
	
	La decisione è nata dal problema che non sempre la distribuzione di probabilità che si sta cercando di approssimare è un albero come approssimata dell'algoritmo di Chow-Liu. Un esempio grafico di foresta è riportato nella figura \ref{fig:forest} . 
	
	\begin{figure}[h]
		%\includegraphics[]{forest/docs.pdf}
		
		\centerline{\scalebox{1}{\includegraphics[]{forest/docs.pdf}}}
		
		\caption{
			La figura rappresenta una foresta contenente 3 alberi. La struttura potrebbe evidenziare il fatto che la variabile $X_7$ non dipenda da nessun'altra variabile e quindi sia essa stessa un albero di Chow-Liu.
		}
		\label{fig:forest}
	\end{figure}
	La figura \ref{fig:forest} evidenzia il caso in cui la struttura migliore per approssimare una data distribuzione possa essere codificabile attraverso una foresta e non un albero. In particolare la scelta dell'albero obbliga la connessione di variabili che magari non siano veramente correlate. Se avessimo scelto di utilizzare un albero di Chow-Liu sicuramente la variabile $X_7$ sarebbe stata collegata ad un'altra variabile portando così la struttura ad approssimare male la distribuzione.
	
	Per poter applicare gli algoritmi presentati nella sezione \ref{sls} c'è bisogno di inquadrare la tipologia di problema da affrontare e la definizione di ''soluzione'' del problema.
	
	Il problema da affrontare nell'apprendimento di foreste di Chow-Liu è quello di approssimare il più possibile la distribuzione rappresentata dal training set, portando a classificare la tipologia di problema come problema di ottimizzazione.
	
	Per il problema in questione si è definita ''soluzione del problema'', una foresta la cui unione degli insiemi di vertici degli alberi in essa contenuti è uguale all'insieme delle features presenti nello slices del learning set ad essa correlato. In particolare si è trattata una soluzione come insiemi di componenti, dove per componenti s'intendono gli archi in essa contenuti.\\
	Come affermato nella sezione \ref{ricloc}, due soluzioni sono differenti se differiscono per almeno una componente. Da questa definizione ne segue che, nel problema corrente, due soluzioni sono differenti se differiscono per almeno un arco.
	Si può quindi definire il ''vicinato'' $N$ di una soluzione $s$ come l'insieme delle foreste che differiscono di \pmb{un} solo arco rispetto ad $s$ .
	
	Come funzione obiettivo è stata scelta la likelihood (sez. \ref{valutazione}), essendo direttamente collegata alla bontà di adattamento della foresta appresa rispetto alla densità obiettivo.
	Per il calcolo della likelihood è stato utilizzato un validation set, diverso quindi dal training set.
	
	Tutti gli algoritmi sono stati applicati utilizzando come soluzione iniziale l'albero generato dall'algoritmo originale di Chow-Liu (sez. \ref{cltree}).
	
	\subsection{Aggiunta di rumore}\label{noise}
	Un'altra modifica applicata all'algoritmo originale dCSN è costituita dall'aggiunta di rumore alla matrice delle mutue informazioni.
	
	In particolare si è fatto uso di rumore puramente additivo sulla matrice simmetrica dei valori delle informazioni mutue.
	Per evitare valori negativi nella matrice con aggiunta di rumore si è utilizzato il valore assoluto dei valori campionati dalla distribuzione normale standard.
	
	Sia $M$ la matrice delle mutue informazioni, $N$ una matrice simmetrica avente in ogni cella un valore $\vert v\vert \in N(\mu, \sigma\textsuperscript{2})$ , $MR$ è una matrice con aggiunta di rumore in valore assoluto se $MR$ = $M$ + $N$.
	
	Inoltre sono stati utilizzati diversi valori di varianza (0.1, 0.5, 1,2) per gestire il grado di amplificazione della aggiunta di rumore.
	
	
	\subsection{Stochastic Local Search per l'apprendimento di Cutset Network}\label{applicazione}
	Gli algoritmi stocastici di ricerca locale sono stati utilizzati per migliorare gli alberi di Chow-Liu inizialmente appresi da dCSN. Una rivisitazione di iterative improvement è riportata in \ref{alg:ii}.
	\begin{algorithm}
		\caption{Iterative improvement(cltree)}
		\begin{algorithmic}[1]
			\Require cltree: albero di Chow-Liu
			\Ensure una foresta o l'albero ricevuto in input
			\State improved = True
			\State best\_ll = -$\infty$
			\State best\_forest= null
			\State current\_best\_solution = cltree
			\State current\_best\_ll = score(cltree)
			\While{improved = True}
			\State improved = False
			\State N= generateNeighbourhood(cltree)
			\For{$s'$ in N}
			\State log\_likelihood= score($s'$)
			\If{log\_likelihood $>$ best\_ll}
			\State best\_ll = log\_likelihood
			\State best\_forest = $s'$
			\EndIf
			\EndFor
			\If{ best\_ll $>$ current\_best\_ll}
			\State current\_best\_solution = best\_forest
			\State current\_best\_ll = best\_ll
			
			\EndIf
			\EndWhile
			\State\Return current\_best\_solution
		\end{algorithmic}
		\label{alg:ii}
	\end{algorithm}
	
	La variante utilizzata nel progetto è quella dell'Iterative Best Improvement, scegliendo tra le soluzioni vicine quella che migliora più di tutte la likelihood.
	
	Parte del processo di miglioramento delle soluzioni di Iterative Improvement è quello di generazione del vicinato di soluzioni. Per il problema affrontato in questa tesi la funzione $generateNeighbourhood()$ ha avuto il compito di generare l'insieme dei vicini in termini di foreste. Data una soluzione $s$, $s'$ appartiene al vicinato di $s$ se $E(s) - E(s') = 1$ e $s'$ è ottenuta da $s$ ''tagliando'' un arco.
	\\Se $s$ è un albero di Chow-Liu allora $s'$ è una foresta avente due alberi di Chow-Liu.
	\\Se $s$ è una foresta avente $n$ alberi allora $s'$ è una nuova foresta avente $n+1$ alberi.\\
	La foresta è, quindi, ottenuta eliminando un arco dall'albero originale. Così facendo si punta ad eliminare l'arco che maggiormente influenzava in maniera negativa il comportamento dell'albero originale.
	
	Quindi in tutti gli algoritmi di ricerca locale applicati, la dimensione del vicinato di una soluzione è stata pari al numero di archi in essa contenuti. Ogni vicino di una soluzione $s$ corrisponde ad un arco tagliato da $s$.
	
	Successivamente iterative improvement misura, per ogni elemento del vicinato, il valore di likelihood raggiunto sul validation set. Seguendo il comportamento di base dell'algoritmo di ricerca, si sceglie la miglior soluzione del vicinato.\\
	Finito il ciclo si testa il miglior log-likelihood con il likelihood correntemente raggiunto dalla struttura. Se si sono avuti dei miglioramenti allora si sostituisce la nuova struttura con l'arco tagliato alla vecchia struttura. Il tutto viene ripetuto fino a quando si hanno dei miglioramenti.
	
	
	\begin{algorithm}
		\caption{Randomised Iterative improvement(cltree,probability,times)}
		\begin{algorithmic}[1]
			\Require cltree: albero di Chow-Liu, probability: probabilità di scegliere il miglior vicino, times: numero di iterate (criterio di terminazione)
			\Ensure una foresta o l'albero ricevuto in input
			\State t = 0
			\State best\_ll = -$\infty$
			\State best\_forest= null
			\State current\_best\_solution = cltree
			\State current\_best\_ll = score(cltree)
			\While{t $<$ times}
			\State N = generateNeighbourhood(cltree)
			\State r=random([0,1])
			\If{r $>$ probability}
			\State $s'$ = getRandom(N)
			\State best\_ll = score($s'$)
			\State best\_forest = $s'$
			\Else
			\For{$s'$ in N}
			\State log\_likelihood= score($s'$)
			\If{log\_likelihood $>$ best\_ll}
			\State best\_ll = log\_likelihood
			\State best\_forest = $s'$
			\EndIf
			\EndFor
			\EndIf
			
			\If{ best\_ll $>$ current\_best\_ll}
			\State current\_best\_solution = best\_forest
			\State current\_best\_ll = best\_ll
			\State improved = True
			\EndIf
			\State t = t + 1
			\EndWhile
			\State	\Return current\_best\_solution
		\end{algorithmic}
		\label{alg:rii}
	\end{algorithm}
	Anche Randomised Iterative Improvement è stato rivisitato per adattarlo al problema specifico affrontato. L'algoritmo è riporato in \ref{alg:rii}. Le adattazioni attuate in Iterative Improvement sono state le stesse di Randomised Iterative Improvement.
	\begin{algorithm}
		\caption{GRASP(times, k,cltree)}
		\begin{algorithmic}[1]
			\Require times: numero di iterate (criterio di terminazione), k: cardinalità della RCL, cltree: l'output dell'algoritmo di chow-liu
			\Ensure 
			
			\State current\_best\_solution = cltree
			\State current\_best\_ll = score(cltree)
			\State t := 0
			\While{t $<$ times}
			\State //\textit{Inizio costruzione}
			\State mst = minimum\_spanning\_tree(k) //\textit{versione modificata dell'algoritmo di kruskal}
			\State s = attraversaAlbero(mst)
			\State //\textit{Fine costruzione}
			\State //\textit{Inizio ricerca locale}
			
			\State improved = True
			\State best\_ll = -$\infty$
			\State best\_forest= null
			\State initial\_ll = score(s)
			\While{improved = True}
			\State improved = False
			\State N= generateNeighbourhood(s)
			\For{$s'$ in N}
			\State log\_likelihood= score($s'$)
			\If{log\_likelihood $>$ best\_ll}
			\State best\_ll = log\_likelihood
			\State best\_forest = $s'$
			\EndIf
			\EndFor
			\If{ best\_ll $>$ initial\_ll}
			\State s = best\_forest
			\State initial\_ll = best\_ll
			\EndIf
			\EndWhile
			
			\State //\textit{Fine ricerca locale}
			\If{ initial\_ll $>$ current\_best\_ll}
			\State current\_best\_ll = initial\_ll
			\State current\_best\_solution = s
			\EndIf
			\State t = t + 1 
			\EndWhile
		\end{algorithmic}
		\label{alg:graspa}
	\end{algorithm}
	\newpage
	L'algoritmo GRASP  è riportato in pseudocodice nell'algoritmo \ref{alg:graspa}. In GRASP si è utilizzata una versione modificata dell'algoritmo di kruskal. La differenza è che nell'algoritmo di kruskal in ogni ciclo si cerca di aggiungere l'arco con mutua informazione migliore all'albero in costruzione, mentre nella versione modificata si sceglie un arco $random$ tra i migliori $k$ archi (ordinati in base alla mutua informazione che rappresentano).
	
	Questa modifica è stata fatta per permettere la creazione della RCL (Restricted Candidate List), componente fondamentale dell'approccio GRASP. In questo adattamento la RCL è composta dai migliori $k$ archi, appartenendo quindi alla tipologia di RCL con restrizione di cardinalità.
	
	GRASP, quindi, costruisce l'albero di Chow-Liu iniziale per una foglia tramite l'algoritmo modificato di kruskal. Dopo la fase di costruzione randomizzata, si dà inizio alla fase di ricerca locale. Benchè teoricamente in GRASP si possa utilizzare qualsiasi algoritmo di ricerca locale, nel progetto è stato utilizzato Iterative Best Improvement. Successivamente testa il likelihood nuovo con quello vecchio e, come in Iterative Improvement e Randomised Iterative Improvement, sostituisce la nuova struttura con la vecchia.
	
	Come criterio di terminazione è stato scelto un parametro chiamato ''times'' rappresentante il numero di cicli costruzione-ricerca effettuati dall'algoritmo.
	
	\newpage
	\section{Sperimentazioni}\label{sperimentazione}
	Alla fase di adattamento ed implementazione degli algoritmi di ricerca al problema dell'apprendimento delle cutset networks è seguita una fase di sperimentazione. La fase di sperimentazione è stata utilizzata per valutare le eventuali migliorie prodotte dall'applicazione delle modifiche a dCSN e degli algoritmi presentati nella sezione \ref{sls}.
	
	In questa sezione saranno presentati i datasets utilizzati (sez. \ref{datasets}). Alcuni dettagli implementativi (sez. \ref{det}), i risultati (sez \ref{ris}) e la loro relativa discussione (sez \ref{disc}).
	\subsection{Datasets}\label{datasets}
	Nella fase di sperimentazione sono stati utilizzati 7 datasets. Questi datasets sono alcuni dei datsets standard per il benchmark degli algoritmi di apprendimento di PGM presentati in \cite{lowd} e \cite{haaren}. I datasets hanno un numero di istanze di training che va da 9000 a 291326. Il numero di features in esse contenute cade in un range di 16 - 111 unità.
	\begin{table}[h!]
		\centering
		
		\begin{tabular}{lcccc}
			\toprule
			& $\vert X\vert$ & $\vert T_{train}\vert$ &$\vert T_{val}\vert$ &$\vert T_{test}\vert$\\
			\midrule
			NLTCS & 16 & 16181 &2157 &3236\\
			MSNBC & 17 & 291326 &38843 &58265\\
			Plants & 69 & 17412 &2321&3482\\
			Audio & 100 & 15000 &2000&3000\\
			Jester & 100 & 9000 &1000&4116\\
			Netflix & 100 & 15000 &2000&3000\\
			Accidents & 111 & 12758 &1700&2551\\
			
			\bottomrule
		\end{tabular}
		\caption{Datasets utilizzati}
		\label{tab:table1}
	\end{table}
	
	Tutti i datasets contengono variabili binare, ovvero, variabili che possono assumere valori \{0,1\}.\\
	Si utilizzano gli stessi split per training, validation (10\%), e test set (15\%) individuati originariamente dagli autori.\\
	 Da qui segue che ogni nodo OR delle strutture apprese ha avuto 2 nodi figli: uno per le istanze aventi la feature avvalorata a 0 e un altro per le istanze aventi la feature avvalorata a 1.
	\subsection{Dettagli implementativi}\label{det}
	Il lavoro svolto da Di Mauro at al. in \cite{dimauro} è stato utilizzato come base per l'applicazione degli algoritmi e delle varianti precedentemente discusse. Il codice
	originale è disponibile su Github\footnote{\href{https://github.com/nicoladimauro/dcsn}{https://github.com/nicoladimauro/dcsn}}.
	
	Gli algoritmi di ricerca, così come l'intero progetto originale, sono stati sviluppati in Python 3.5 con l'ausilio delle librerie SciPy, NumPy, Numba e sklearn.
	Per poter implementare la variante dell'algoritmo di kruskal presentata nella sezione \ref{applicazione} è stata utilizzata, come base, l'implementazione presente nella libreria SciPy.
	
	L'intero progetto presentato in questa tesi è anch'esso disponibile su Github\footnote{\href{https://github.com/Rhuax/dcsn}{https://github.com/Rhuax/dcsn}}. Gli script python sono stati parametrizzati per facilitare i vari test e per automatizzare le grid searches. Gli script hanno prodotto risultati in forma testuale e tabellare così come nel progetto originale.
	
	\subsection{Risultati}\label{ris}
	Ogni tipologia di algoritmo di ricerca è stata applicata ad ogni dataset, e per ogni dataset è stata fatta una grid search sui seguenti parametri:
	
	\begin{itemize}
		\item $\alpha\textsubscript{f} \in \{0.5,1.0,5.0,10.0\}$
		\item $ \delta \in \{10,50,100,200,500\}$
		\item $ \sigma\textsuperscript{2} \in \{0.1,0.5,1,2\}$ solo per le varianti con aggiunta di rumore
		\item $\sigma\in \{3\}$
	\end{itemize}
	
	Per ogni tipologia di algoritmo è stata testata la versione normale e la versione con aggiunta di rumore alla matrice delle mutue informazioni.
	
	I risultati numerici delle esecuzioni degli algoritmi sono stati prodotti dagli script python e salvati in alcuni file di log. Questi file di log sono stati trasformati in tabelle. Dalle tabelle prodotte sono state estratte le colonne contenenti i valori dei parametri passati in input. Queste tabelle sono di seguito riportate raggruppate per dataset.
	\newpage
	\subsubsection{nltcs}
		\begin{longtable}{|l|l|l|}
			\caption{dataset:nltcs, nessun algoritmo di SLS applicato, senza rumore}
			\label{nltcs}\\
			\hline
			 alpha    & mnist & test\_ll \\ \hline
			0.50000  & 10    & -6.49890 \\ \hline
			0.50000  & 50    & -6.33067 \\ \hline
			0.50000  & 100   & -6.21828 \\ \hline
			0.50000  & 200   & -6.15773 \\ \hline
			0.50000  & 500   & -6.08355 \\ \hline
			1.00000  & 10    & -6.43014 \\ \hline
			 1.00000  & 50    & -6.30230 \\ \hline
			 1.00000  & 100   & -6.18094 \\ \hline
			 1.00000  & 200   & -6.13457 \\ \hline
			 1.00000  & 500   & -6.06773 \\ \hline
			 5.00000  & 10    & -6.30488 \\ \hline
			 5.00000  & 50    & -6.20013 \\ \hline
			 5.00000  & 100   & -6.14045 \\ \hline
			 5.00000  & 200   & -6.10698 \\ \hline
			5.00000  & 500   & -6.05411 \\ \hline
			 10.00000 & 10    & -6.25948 \\ \hline
			 10.00000 & 50    & -6.18628 \\ \hline
			 10.00000 & 100   & -6.15030 \\ \hline
			 10.00000 & 200   & -6.10845 \\ \hline
			 10.00000 & 500   & -6.07792 \\ \hline
		\end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs, nessun algoritmo di SLS applicato, varianza del rumore: 0.1}
		\label{nltcs2}\\
		\hline
	 alpha    & mnist & test\_ll \\ \hline
			 0.50000  & 10    & -6.45926 \\ \hline
		 0.50000  & 50    & -6.27106 \\ \hline
		 0.50000  & 100   & -6.20831 \\ \hline
			 0.50000  & 200   & -6.13424 \\ \hline
			 0.50000  & 500   & -6.09508 \\ \hline
			 1.00000  & 10    & -6.40590 \\ \hline
			 1.00000  & 50    & -6.23987 \\ \hline
			 1.00000  & 100   & -6.19141 \\ \hline
			 1.00000  & 200   & -6.12428 \\ \hline
			 1.00000  & 500   & -6.09071 \\ \hline
			 5.00000  & 10    & -6.25544 \\ \hline
			 5.00000  & 50    & -6.16408 \\ \hline
			5.00000  & 100   & -6.14874 \\ \hline
			 5.00000  & 200   & -6.06335 \\ \hline
			5.00000  & 500   & -6.07834 \\ \hline
			10.00000 & 10    & -6.23529 \\ \hline
			 10.00000 & 50    & -6.14924 \\ \hline
			 10.00000 & 100   & -6.11765 \\ \hline
			10.00000 & 200   & -6.10998 \\ \hline
			10.00000 & 500   & -6.08494 \\ \hline
		\end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs, nessun algoritmo di SLS applicato, varianza del rumore: 0.5}
		\label{nltcs3}\\
		\hline
	alpha    & mnist & test\_ll \\ \hline
	 0.50000  & 10    & -6.39648 \\ \hline
			0.50000  & 50    & -6.25413 \\ \hline
			 0.50000  & 100   & -6.16192 \\ \hline
			 0.50000  & 200   & -6.11764 \\ \hline
			 0.50000  & 500   & -6.10513 \\ \hline
			 1.00000  & 10    & -6.40243 \\ \hline
			 1.00000  & 50    & -6.20579 \\ \hline
			 1.00000  & 100   & -6.15182 \\ \hline
			 1.00000  & 200   & -6.12577 \\ \hline
			 1.00000  & 500   & -6.11664 \\ \hline
			 5.00000  & 10    & -6.23935 \\ \hline
			 5.00000  & 50    & -6.15090 \\ \hline
			 5.00000  & 100   & -6.12457 \\ \hline
			 5.00000  & 200   & -6.09997 \\ \hline
			 5.00000  & 500   & -6.11330 \\ \hline
			 10.00000 & 10    & -6.19467 \\ \hline
			10.00000 & 50    & -6.13907 \\ \hline
			10.00000 & 100   & -6.12610 \\ \hline
			10.00000 & 200   & -6.10557 \\ \hline
			 10.00000 & 500   & -6.11241 \\ \hline
		\end{longtable}
	\begin{longtable}{|l|l|l|l|}
		\caption{dataset: nltcs, nessun algoritmo di SLS applicato, varianza del rumore: 1}
		\label{nltcs4}\\
		\hline
		 alpha    & mnist & test\_ll \\ \hline
			0.50000  & 10    & -6.40386 \\ \hline
			 0.50000  & 50    & -6.22129 \\ \hline
			 0.50000  & 100   & -6.17629 \\ \hline
			 0.50000  & 200   & -6.11294 \\ \hline
			 0.50000  & 500   & -6.09374 \\ \hline
			1.00000  & 10    & -6.34833 \\ \hline
			 1.00000  & 50    & -6.20471 \\ \hline
			1.00000  & 100   & -6.13001 \\ \hline
			1.00000  & 200   & -6.12358 \\ \hline
			 1.00000  & 500   & -6.09309 \\ \hline
			 5.00000  & 10    & -6.22281 \\ \hline
			 5.00000  & 50    & -6.13921 \\ \hline
			 5.00000  & 100   & -6.11422 \\ \hline
			5.00000  & 200   & -6.10392 \\ \hline
			5.00000  & 500   & -6.09141 \\ \hline
			 10.00000 & 10    & -6.19747 \\ \hline
			 10.00000 & 50    & -6.14352 \\ \hline
			 10.00000 & 100   & -6.10899 \\ \hline
			 10.00000 & 200   & -6.10105 \\ \hline
			10.00000 & 500   & -6.13633 \\ \hline
		\end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs, nessun algoritmo di SLS applicato, varianza del rumore: 2}
		\label{nltcs5}\\
		\hline
	 alpha    & mnist & test\_ll \\ \hline
			 0.50000  & 10    & -6.36372 \\ \hline
			0.50000  & 50    & -6.22910 \\ \hline
		 0.50000  & 100   & -6.18135 \\ \hline
		0.50000  & 200   & -6.11208 \\ \hline
		 0.50000  & 500   & -6.08472 \\ \hline
		1.00000  & 10    & -6.34566 \\ \hline
			 1.00000  & 50    & -6.21412 \\ \hline
			1.00000  & 100   & -6.13242 \\ \hline
			1.00000  & 200   & -6.10306 \\ \hline
		1.00000  & 500   & -6.09442 \\ \hline
			5.00000  & 10    & -6.22394 \\ \hline
			 5.00000  & 50    & -6.11743 \\ \hline
			5.00000  & 100   & -6.11249 \\ \hline
			 5.00000  & 200   & -6.10344 \\ \hline
		 5.00000  & 500   & -6.09353 \\ \hline
		10.00000 & 10    & -6.18307 \\ \hline
			10.00000 & 50    & -6.13529 \\ \hline
			 10.00000 & 100   & -6.10228 \\ \hline
			 10.00000 & 200   & -6.09426 \\ \hline
			 10.00000 & 500   & -6.11280 \\ \hline
		\end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs, nessun algoritmo di SLS applicato, varianza del rumore: 3}
		\label{nltcs6}\\
		\hline
		 alpha    & mnist & test\_ll \\ \hline
			0.50000  & 10    & -6.35577 \\ \hline
			 0.50000  & 50    & -6.25925 \\ \hline
		0.50000  & 100   & -6.17176 \\ \hline
		 0.50000  & 200   & -6.12072 \\ \hline
		0.50000  & 500   & -6.09210 \\ \hline
			 1.00000  & 10    & -6.31135 \\ \hline
		1.00000  & 50    & -6.23468 \\ \hline
			 1.00000  & 100   & -6.13348 \\ \hline
			1.00000  & 200   & -6.08710 \\ \hline
			 1.00000  & 500   & -6.09475 \\ \hline
			5.00000  & 10    & -6.19364 \\ \hline
			 5.00000  & 50    & -6.16082 \\ \hline
		 5.00000  & 100   & -6.11079 \\ \hline
			 5.00000  & 200   & -6.09327 \\ \hline
			 5.00000  & 500   & -6.10482 \\ \hline
			 10.00000 & 10    & -6.19684 \\ \hline
			10.00000 & 50    & -6.12901 \\ \hline
			 10.00000 & 100   & -6.12471 \\ \hline
		 10.00000 & 200   & -6.09956 \\ \hline
	 10.00000 & 500   & -6.10248 \\ \hline
		\end{longtable}
		\begin{longtable}{|l|l|l|}
			\caption{dataset: nltcs, Iterative Improvement senza rumore}
			\label{nltcsii}\\ \hline
 minst & alpha    & test\_ll \\ \hline
10    & 0.50000  & -6.46330 \\ \hline
 50    & 0.50000  & -6.33055 \\ \hline
 100   & 0.50000  & -6.21818 \\ \hline
 200   & 0.50000  & -6.15824 \\ \hline
 500   & 0.50000  & -6.08381 \\ \hline
 10    & 1.00000  & -6.39304 \\ \hline
 50    & 1.00000  & -6.30148 \\ \hline
 100   & 1.00000  & -6.18007 \\ \hline
 200   & 1.00000  & -6.13408 \\ \hline
 500   & 1.00000  & -6.06773 \\ \hline
 10    & 5.00000  & -6.29269 \\ \hline
 50    & 5.00000  & -6.20016 \\ \hline
 100   & 5.00000  & -6.14048 \\ \hline
 200   & 5.00000  & -6.10736 \\ \hline
 500   & 5.00000  & -6.05411 \\ \hline
 10    & 10.00000 & -6.23944 \\ \hline
 50    & 10.00000 & -6.18542 \\ \hline
 100   & 10.00000 & -6.15031 \\ \hline
 200   & 10.00000 & -6.10843 \\ \hline
 500   & 10.00000 & -6.07792 \\ \hline
			\end{longtable}
		\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Iterative Improvement senza rumore}
	\label{nltcsii2}\\ \hline
	minst & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.36980 \\ \hline
 50    & 0.50000  & -6.22196 \\ \hline
 100   & 0.50000  & -6.17539 \\ \hline
 200   & 0.50000  & -6.11439 \\ \hline
 500   & 0.50000  & -6.09293 \\ \hline
 10    & 1.00000  & -6.32592 \\ \hline
 50    & 1.00000  & -6.20494 \\ \hline
 100   & 1.00000  & -6.12984 \\ \hline
 200   & 1.00000  & -6.12455 \\ \hline
500   & 1.00000  & -6.09284 \\ \hline
 10    & 5.00000  & -6.21486 \\ \hline
 50    & 5.00000  & -6.13556 \\ \hline
 100   & 5.00000  & -6.11506 \\ \hline
200   & 5.00000  & -6.10635 \\ \hline
 500   & 5.00000  & -6.09497 \\ \hline
 10    & 10.00000 & -6.18216 \\ \hline
 50    & 10.00000 & -6.14665 \\ \hline
 100   & 10.00000 & -6.10864 \\ \hline
 200   & 10.00000 & -6.10083 \\ \hline
 500   & 10.00000 & -6.13594 \\ \hline
						\end{longtable}
							\begin{longtable}{|l|l|l|}
						\caption{dataset: nltcs, Iterative Improvement con rumore avente varianza 2}
						\label{nltcsii3}\\ \hline
						minst & alpha    & test\_ll \\ \hline
10    & 0.50000  & -6.35208 \\ \hline
50    & 0.50000  & -6.21991 \\ \hline
100   & 0.50000  & -6.14243 \\ \hline
 200   & 0.50000  & -6.13501 \\ \hline
 500   & 0.50000  & -6.11457 \\ \hline
 10    & 1.00000  & -6.34224 \\ \hline
 50    & 1.00000  & -6.19734 \\ \hline
 100   & 1.00000  & -6.14516 \\ \hline
200   & 1.00000  & -6.10100 \\ \hline
 500   & 1.00000  & -6.09123 \\ \hline
 10    & 5.00000  & -6.19478 \\ \hline
 50    & 5.00000  & -6.11170 \\ \hline
 100   & 5.00000  & -6.10484 \\ \hline
 200   & 5.00000  & -6.07638 \\ \hline
 500   & 5.00000  & -6.09773 \\ \hline
 10    & 10.00000 & -6.19689 \\ \hline
 50    & 10.00000 & -6.14672 \\ \hline
 100   & 10.00000 & -6.12733 \\ \hline
 200   & 10.00000 & -6.10693 \\ \hline
 500   & 10.00000 & -6.13927 \\ \hline
						\end{longtable}
												\begin{longtable}{|l|l|l|}
						\caption{dataset: nltcs, Iterative Improvement con rumore avente varianza 0.1}
						\label{nltcsii4}\\ \hline
						minst & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.42910 \\ \hline
 50    & 0.50000  & -6.27202 \\ \hline
 100   & 0.50000  & -6.20841 \\ \hline
 200   & 0.50000  & -6.13534 \\ \hline
 500   & 0.50000  & -6.09618 \\ \hline
 10    & 1.00000  & -6.36876 \\ \hline
 50    & 1.00000  & -6.23962 \\ \hline
 100   & 1.00000  & -6.18934 \\ \hline
 200   & 1.00000  & -6.12620 \\ \hline
 500   & 1.00000  & -6.09036 \\ \hline
 10    & 5.00000  & -6.24290 \\ \hline
 50    & 5.00000  & -6.16578 \\ \hline
100   & 5.00000  & -6.14800 \\ \hline
 200   & 5.00000  & -6.06638 \\ \hline
 500   & 5.00000  & -6.07826 \\ \hline
 10    & 10.00000 & -6.22805 \\ \hline
 50    & 10.00000 & -6.15302 \\ \hline
 100   & 10.00000 & -6.11755 \\ \hline
 200   & 10.00000 & -6.10926 \\ \hline
 500   & 10.00000 & -6.08803 \\ \hline
						\end{longtable}
												\begin{longtable}{|l|l|l|}
						\caption{dataset: nltcs, Iterative Improvement con rumore avente varianza 0.5}
						\label{nltcsii5}\\ \hline
						minst & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.37725 \\ \hline
 50    & 0.50000  & -6.25252 \\ \hline
 100   & 0.50000  & -6.16200 \\ \hline
 200   & 0.50000  & -6.11771 \\ \hline
 500   & 0.50000  & -6.10755 \\ \hline
 10    & 1.00000  & -6.37504 \\ \hline
 50    & 1.00000  & -6.20715 \\ \hline
 100   & 1.00000  & -6.15266 \\ \hline
 200   & 1.00000  & -6.12888 \\ \hline
 500   & 1.00000  & -6.11613 \\ \hline
 10    & 5.00000  & -6.23254 \\ \hline
 50    & 5.00000  & -6.15083 \\ \hline
 100   & 5.00000  & -6.12424 \\ \hline
 200   & 5.00000  & -6.09984 \\ \hline
 500   & 5.00000  & -6.11749 \\ \hline
 10    & 10.00000 & -6.18815 \\ \hline
 50    & 10.00000 & -6.14216 \\ \hline
 100   & 10.00000 & -6.12766 \\ \hline
 200   & 10.00000 & -6.10517 \\ \hline
 500   & 10.00000 & -6.11693 \\ \hline
		\end{longtable}
		\begin{longtable}{|l|l|l|}
			\caption{dataset: nltcs, Randomised Iterative Improvement senza rumore, con probabilità 0.6 e times = 10}
			\label{nltcsrii}\\
			\hline
 alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.4633  \\ \hline
 0.5   & 50    & -6.33055 \\ \hline
 0.5   & 100   & -6.21818 \\ \hline
 0.5   & 200   & -6.15824 \\ \hline
 0.5   & 500   & -6.08381 \\ \hline
 1     & 10    & -6.39304 \\ \hline
 1     & 50    & -6.30148 \\ \hline
 1     & 100   & -6.18007 \\ \hline
 1     & 200   & -6.13408 \\ \hline
 1     & 500   & -6.06773 \\ \hline
 5     & 10    & -6.29269 \\ \hline
 5     & 50    & -6.20016 \\ \hline
 5     & 100   & -6.14048 \\ \hline
 5     & 200   & -6.10736 \\ \hline
 5     & 500   & -6.05411 \\ \hline
 10    & 10    & -6.23938 \\ \hline
 10    & 50    & -6.18542 \\ \hline
 10    & 100   & -6.15031 \\ \hline
 10    & 200   & -6.10843 \\ \hline
 10    & 500   & -6.07792 \\ \hline
 \end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza 0.1, con probabilità 0.6 e times = 10}
		\label{nltcsrii2}\\
		\hline
		alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.42902 \\ \hline
0.5   & 50    & -6.27202 \\ \hline
0.5   & 100   & -6.20841 \\ \hline
 0.5   & 200   & -6.13535 \\ \hline
 0.5   & 500   & -6.09618 \\ \hline
 1     & 10    & -6.36847 \\ \hline
 1     & 50    & -6.23962 \\ \hline
 1     & 100   & -6.18934 \\ \hline
 1     & 200   & -6.1262  \\ \hline
 1     & 500   & -6.09036 \\ \hline
 5     & 10    & -6.2429  \\ \hline
 5     & 50    & -6.16578 \\ \hline
 5     & 100   & -6.148   \\ \hline
 5     & 200   & -6.06697 \\ \hline
 5     & 500   & -6.07826 \\ \hline
 10    & 10    & -6.22805 \\ \hline
10    & 50    & -6.15302 \\ \hline
 10    & 100   & -6.11755 \\ \hline
10    & 200   & -6.10926 \\ \hline
10    & 500   & -6.08803 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 0.5, con probabilità 0.6 e times = 10}
	\label{nltcsrii3}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.37725 \\ \hline
0.5   & 50    & -6.25252 \\ \hline
 0.5   & 100   & -6.162   \\ \hline
 0.5   & 200   & -6.11771 \\ \hline
 0.5   & 500   & -6.10755 \\ \hline
 1     & 10    & -6.37506 \\ \hline
 1     & 50    & -6.20715 \\ \hline
 1     & 100   & -6.15266 \\ \hline
 1     & 200   & -6.12856 \\ \hline
 1     & 500   & -6.11613 \\ \hline
 5     & 10    & -6.23252 \\ \hline
 5     & 50    & -6.15083 \\ \hline
 5     & 100   & -6.12424 \\ \hline
 5     & 200   & -6.09984 \\ \hline
 5     & 500   & -6.11749 \\ \hline
 10    & 10    & -6.18815 \\ \hline
 10    & 50    & -6.14216 \\ \hline
 10    & 100   & -6.12766 \\ \hline
 10    & 200   & -6.10543 \\ \hline
 10    & 500   & -6.11693 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 1, con probabilità 0.6 e times = 10}
	\label{nltcsrii4}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.36976 \\ \hline
 0.5   & 50    & -6.22196 \\ \hline
 0.5   & 100   & -6.17539 \\ \hline
 0.5   & 200   & -6.11439 \\ \hline
 0.5   & 500   & -6.09293 \\ \hline
 1     & 10    & -6.32593 \\ \hline
 1     & 50    & -6.20494 \\ \hline
 1     & 100   & -6.12984 \\ \hline
 1     & 200   & -6.12455 \\ \hline
 1     & 500   & -6.09284 \\ \hline
 5     & 10    & -6.21487 \\ \hline
 5     & 50    & -6.13556 \\ \hline
 5     & 100   & -6.11506 \\ \hline
 5     & 200   & -6.10635 \\ \hline
 5     & 500   & -6.09497 \\ \hline
 10    & 10    & -6.18216 \\ \hline
 10    & 50    & -6.14665 \\ \hline
 10    & 100   & -6.10864 \\ \hline
 10    & 200   & -6.10083 \\ \hline
 10    & 500   & -6.13594 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 2, con probabilità 0.6 e times = 10}
	\label{nltcsrii5}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.35208 \\ \hline
 0.5   & 50    & -6.21991 \\ \hline
 0.5   & 100   & -6.14243 \\ \hline
 0.5   & 200   & -6.13501 \\ \hline
 0.5   & 500   & -6.11457 \\ \hline
 1     & 10    & -6.34222 \\ \hline
 1     & 50    & -6.19734 \\ \hline
1     & 100   & -6.14516 \\ \hline
 1     & 200   & -6.101   \\ \hline
 1     & 500   & -6.09123 \\ \hline
 5     & 10    & -6.19478 \\ \hline
 5     & 50    & -6.1117  \\ \hline
 5     & 100   & -6.10484 \\ \hline
 5     & 200   & -6.07638 \\ \hline
 5     & 500   & -6.09773 \\ \hline
 10    & 10    & -6.19689 \\ \hline
 10    & 50    & -6.14672 \\ \hline
 10    & 100   & -6.12733 \\ \hline
 10    & 200   & -6.10693 \\ \hline
 10    & 500   & -6.13927 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement senza rumore, con probabilità 0.8 e times = 10}
	\label{nltcsrii6}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.4633  \\ \hline
 0.5   & 50    & -6.33055 \\ \hline
 0.5   & 100   & -6.21818 \\ \hline
 0.5   & 200   & -6.15824 \\ \hline
 0.5   & 500   & -6.08381 \\ \hline
 1     & 10    & -6.39304 \\ \hline
 1     & 50    & -6.30148 \\ \hline
 1     & 100   & -6.18007 \\ \hline
 1     & 200   & -6.13408 \\ \hline
 1     & 500   & -6.06773 \\ \hline
 5     & 10    & -6.29269 \\ \hline
 5     & 50    & -6.20016 \\ \hline
 5     & 100   & -6.14048 \\ \hline
 5     & 200   & -6.10736 \\ \hline
 5     & 500   & -6.05411 \\ \hline
 10    & 10    & -6.23944 \\ \hline
 10    & 50    & -6.18542 \\ \hline
 10    & 100   & -6.15031 \\ \hline
 10    & 200   & -6.10843 \\ \hline
 10    & 500   & -6.07792 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 0.1, con probabilità 0.8 e times = 10}
	\label{nltcsrii7}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
0.5   & 10    & -6.4291  \\ \hline
0.5   & 50    & -6.27202 \\ \hline
0.5   & 100   & -6.20841 \\ \hline
0.5   & 200   & -6.13534 \\ \hline
 0.5   & 500   & -6.09618 \\ \hline
 1     & 10    & -6.36876 \\ \hline
 1     & 50    & -6.23962 \\ \hline
 1     & 100   & -6.18934 \\ \hline
 1     & 200   & -6.1262  \\ \hline
 1     & 500   & -6.09036 \\ \hline
 5     & 10    & -6.2429  \\ \hline
 5     & 50    & -6.16578 \\ \hline
 5     & 100   & -6.148   \\ \hline
 5     & 200   & -6.06638 \\ \hline
 5     & 500   & -6.07826 \\ \hline
 10    & 10    & -6.22805 \\ \hline
10    & 50    & -6.15302 \\ \hline
 10    & 100   & -6.11755 \\ \hline
 10    & 200   & -6.10926 \\ \hline
 10    & 500   & -6.08803 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 0.5, con probabilità 0.8 e times = 10}
	\label{nltcsrii8}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.37725 \\ \hline
 0.5   & 50    & -6.25252 \\ \hline
0.5   & 100   & -6.162   \\ \hline
0.5   & 200   & -6.11771 \\ \hline
 0.5   & 500   & -6.10755 \\ \hline
 1     & 10    & -6.37504 \\ \hline
1     & 50    & -6.20715 \\ \hline
 1     & 100   & -6.15266 \\ \hline
 1     & 200   & -6.12888 \\ \hline
 1     & 500   & -6.11613 \\ \hline
 5     & 10    & -6.23254 \\ \hline
5     & 50    & -6.15083 \\ \hline
 5     & 100   & -6.12424 \\ \hline
 5     & 200   & -6.09984 \\ \hline
 5     & 500   & -6.11749 \\ \hline
 10    & 10    & -6.18815 \\ \hline
 10    & 50    & -6.14216 \\ \hline
10    & 100   & -6.12766 \\ \hline
 10    & 200   & -6.10517 \\ \hline
 10    & 500   & -6.11693 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 1, con probabilità 0.8 e times = 10}
	\label{nltcsrii9}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
 0.5   & 10    & -6.3698  \\ \hline
 0.5   & 50    & -6.22196 \\ \hline
0.5   & 100   & -6.17539 \\ \hline
 0.5   & 200   & -6.11439 \\ \hline
 0.5   & 500   & -6.09293 \\ \hline
 1     & 10    & -6.32592 \\ \hline
 1     & 50    & -6.20494 \\ \hline
 1     & 100   & -6.12984 \\ \hline
 1     & 200   & -6.12455 \\ \hline
 1     & 500   & -6.09284 \\ \hline
 5     & 10    & -6.21486 \\ \hline
5     & 50    & -6.13556 \\ \hline
 5     & 100   & -6.11506 \\ \hline
 5     & 200   & -6.10635 \\ \hline
 5     & 500   & -6.09497 \\ \hline
 10    & 10    & -6.18216 \\ \hline
 10    & 50    & -6.14665 \\ \hline
 10    & 100   & -6.10864 \\ \hline
 10    & 200   & -6.10083 \\ \hline
10    & 500   & -6.13594 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs, Randomised Iterative Improvement con rumore avente varianza di 2, con probabilità 0.8 e times = 10}
	\label{nltcsrii10}\\
	\hline
	alpha & mnist & test\_ll \\ \hline
0.5   & 10    & -6.35208 \\ \hline
0.5   & 50    & -6.21991 \\ \hline
 0.5   & 100   & -6.14243 \\ \hline
0.5   & 200   & -6.13501 \\ \hline
0.5   & 500   & -6.11457 \\ \hline
1     & 10    & -6.34224 \\ \hline
1     & 50    & -6.19734 \\ \hline
1     & 100   & -6.14516 \\ \hline
 1     & 200   & -6.101   \\ \hline
1     & 500   & -6.09123 \\ \hline
5     & 10    & -6.19478 \\ \hline
 5     & 50    & -6.1117  \\ \hline
5     & 100   & -6.10484 \\ \hline
 5     & 200   & -6.07638 \\ \hline
5     & 500   & -6.09773 \\ \hline
10    & 10    & -6.19689 \\ \hline
10    & 50    & -6.14672 \\ \hline
10    & 100   & -6.12733 \\ \hline
10    & 200   & -6.10693 \\ \hline
10    & 500   & -6.13927 \\ \hline
		\end{longtable}
		\begin{longtable}{|l|l|l|}
			\caption{dataset: nltcs. GRASP senza rumore, k=3 e times=20}
			\label{nltcsgrasp}\\
			\hline
 mnist & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.46470 \\ \hline
50    & 0.50000  & -6.33067 \\ \hline
 100   & 0.50000  & -6.21818 \\ \hline
 200   & 0.50000  & -6.15824 \\ \hline
 500   & 0.50000  & -6.08381 \\ \hline
 10    & 1.00000  & -6.39680 \\ \hline
 50    & 1.00000  & -6.30174 \\ \hline
100   & 1.00000  & -6.18014 \\ \hline
200   & 1.00000  & -6.13409 \\ \hline
500   & 1.00000  & -6.06792 \\ \hline
10    & 5.00000  & -6.28905 \\ \hline
50    & 5.00000  & -6.20052 \\ \hline
100   & 5.00000  & -6.14047 \\ \hline
200   & 5.00000  & -6.10742 \\ \hline
500   & 5.00000  & -6.05411 \\ \hline
10    & 10.00000 & -6.23572 \\ \hline
 50    & 10.00000 & -6.18467 \\ \hline
100   & 10.00000 & -6.15038 \\ \hline
200   & 10.00000 & -6.10842 \\ \hline
500   & 10.00000 & -6.07792 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP con rumore avente varianza di 0.1, k=3 e times=20}
	\label{nltcsgrasp2}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
10    & 0.50000  & -6.43689 \\ \hline
 50    & 0.50000  & -6.29316 \\ \hline
100   & 0.50000  & -6.23490 \\ \hline
200   & 0.50000  & -6.11907 \\ \hline
500   & 0.50000  & -6.08413 \\ \hline
10    & 1.00000  & -6.36846 \\ \hline
50    & 1.00000  & -6.25364 \\ \hline
100   & 1.00000  & -6.19728 \\ \hline
 200   & 1.00000  & -6.14150 \\ \hline
 500   & 1.00000  & -6.09926 \\ \hline
 10    & 5.00000  & -6.25055 \\ \hline
50    & 5.00000  & -6.17462 \\ \hline
100   & 5.00000  & -6.12125 \\ \hline
 200   & 5.00000  & -6.09377 \\ \hline
 500   & 5.00000  & -6.08660 \\ \hline
 10    & 10.00000 & -6.23684 \\ \hline
 50    & 10.00000 & -6.17011 \\ \hline
 100   & 10.00000 & -6.13354 \\ \hline
200   & 10.00000 & -6.12670 \\ \hline
500   & 10.00000 & -6.09542 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
\caption{dataset: nltcs. GRASP con rumore avente varianza di 0.5, k=3 e times=20}
\label{nltcsgrasp3}\\
\hline
mnist & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.37624 \\ \hline
 50    & 0.50000  & -6.19939 \\ \hline
 100   & 0.50000  & -6.16802 \\ \hline
 200   & 0.50000  & -6.13302 \\ \hline
 500   & 0.50000  & -6.11454 \\ \hline
10    & 1.00000  & -6.33618 \\ \hline
 50    & 1.00000  & -6.21600 \\ \hline
 100   & 1.00000  & -6.16475 \\ \hline
 200   & 1.00000  & -6.11245 \\ \hline
 500   & 1.00000  & -6.08045 \\ \hline
 10    & 5.00000  & -6.25346 \\ \hline
 50    & 5.00000  & -6.15292 \\ \hline
 100   & 5.00000  & -6.10763 \\ \hline
200   & 5.00000  & -6.09976 \\ \hline
 500   & 5.00000  & -6.09662 \\ \hline
 10    & 10.00000 & -6.20577 \\ \hline
50    & 10.00000 & -6.16111 \\ \hline
 100   & 10.00000 & -6.13682 \\ \hline
 200   & 10.00000 & -6.10911 \\ \hline
500   & 10.00000 & -6.10118 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
\caption{dataset: nltcs. GRASP con rumore avente varianza di 1, k=3 e times=20}
\label{nltcsgrasp3}\\
\hline
mnist & alpha    & test\_ll \\ \hline
10    & 0.50000  & -6.33551 \\ \hline
 50    & 0.50000  & -6.24323 \\ \hline
 100   & 0.50000  & -6.14590 \\ \hline
 200   & 0.50000  & -6.13673 \\ \hline
 500   & 0.50000  & -6.09849 \\ \hline
 10    & 1.00000  & -6.33970 \\ \hline
 50    & 1.00000  & -6.22954 \\ \hline
 100   & 1.00000  & -6.14952 \\ \hline
 200   & 1.00000  & -6.14029 \\ \hline
 500   & 1.00000  & -6.10862 \\ \hline
10    & 5.00000  & -6.22294 \\ \hline
50    & 5.00000  & -6.15214 \\ \hline
100   & 5.00000  & -6.12833 \\ \hline
 200   & 5.00000  & -6.09818 \\ \hline
 500   & 5.00000  & -6.09551 \\ \hline
10    & 10.00000 & -6.21465 \\ \hline
50    & 10.00000 & -6.14105 \\ \hline
 100   & 10.00000 & -6.11040 \\ \hline
 200   & 10.00000 & -6.10544 \\ \hline
500   & 10.00000 & -6.14092 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP con rumore avente varianza di 2, k=3 e times=20}
	\label{nltcsgrasp4}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.37199 \\ \hline
50    & 0.50000  & -6.27130 \\ \hline
100   & 0.50000  & -6.17216 \\ \hline
 200   & 0.50000  & -6.12675 \\ \hline
 500   & 0.50000  & -6.09528 \\ \hline
 10    & 1.00000  & -6.30394 \\ \hline
50    & 1.00000  & -6.20170 \\ \hline
100   & 1.00000  & -6.15774 \\ \hline
 200   & 1.00000  & -6.09597 \\ \hline
 500   & 1.00000  & -6.08484 \\ \hline
10    & 5.00000  & -6.19903 \\ \hline
50    & 5.00000  & -6.14982 \\ \hline
 100   & 5.00000  & -6.14671 \\ \hline
 200   & 5.00000  & -6.09507 \\ \hline
 500   & 5.00000  & -6.09641 \\ \hline
 10    & 10.00000 & -6.18493 \\ \hline
 50    & 10.00000 & -6.13897 \\ \hline
 100   & 10.00000 & -6.15724 \\ \hline
 200   & 10.00000 & -6.09958 \\ \hline
 500   & 10.00000 & -6.12475 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP senza rumore, k=2 e times=20}
	\label{nltcsgrasp6}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
10    & 0.50000  & -6.46330 \\ \hline
 50    & 0.50000  & -6.33055 \\ \hline
 100   & 0.50000  & -6.21818 \\ \hline
 200   & 0.50000  & -6.15824 \\ \hline
 500   & 0.50000  & -6.08381 \\ \hline
10    & 1.00000  & -6.39304 \\ \hline
 50    & 1.00000  & -6.30148 \\ \hline
 100   & 1.00000  & -6.18007 \\ \hline
 200   & 1.00000  & -6.13408 \\ \hline
 500   & 1.00000  & -6.06773 \\ \hline
 10    & 5.00000  & -6.29269 \\ \hline
 50    & 5.00000  & -6.20016 \\ \hline
100   & 5.00000  & -6.14048 \\ \hline
 200   & 5.00000  & -6.10736 \\ \hline
 500   & 5.00000  & -6.05411 \\ \hline
 10    & 10.00000 & -6.23944 \\ \hline
 50    & 10.00000 & -6.18542 \\ \hline
 100   & 10.00000 & -6.15031 \\ \hline
200   & 10.00000 & -6.10843 \\ \hline
500   & 10.00000 & -6.07792 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP con rumore avente varianza di 0.1, k=2 e times=20}
	\label{nltcsgrasp7}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
 10    & 0.50000  & -6.42253 \\ \hline
50    & 0.50000  & -6.25328 \\ \hline
 100   & 0.50000  & -6.19492 \\ \hline
200   & 0.50000  & -6.13664 \\ \hline
500   & 0.50000  & -6.07789 \\ \hline
10    & 1.00000  & -6.34030 \\ \hline
50    & 1.00000  & -6.26478 \\ \hline
 100   & 1.00000  & -6.16324 \\ \hline
 200   & 1.00000  & -6.11145 \\ \hline
 500   & 1.00000  & -6.10299 \\ \hline
10    & 5.00000  & -6.23029 \\ \hline
50    & 5.00000  & -6.17457 \\ \hline
100   & 5.00000  & -6.14257 \\ \hline
200   & 5.00000  & -6.09716 \\ \hline
500   & 5.00000  & -6.10887 \\ \hline
10    & 10.00000 & -6.22688 \\ \hline
50    & 10.00000 & -6.17211 \\ \hline
100   & 10.00000 & -6.12514 \\ \hline
200   & 10.00000 & -6.10453 \\ \hline
500   & 10.00000 & -6.09702 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP con rumore avente varianza di 0.5, k=2 e times=20}
	\label{nltcsgrasp8}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
	10    & 0.50000  & -6.37711 \\ \hline
		50    & 0.50000  & -6.26173 \\ \hline
	100   & 0.50000  & -6.16558 \\ \hline
		200   & 0.50000  & -6.13839 \\ \hline
	500   & 0.50000  & -6.10185 \\ \hline
	 10    & 1.00000  & -6.33538 \\ \hline
		50    & 1.00000  & -6.23083 \\ \hline
		 100   & 1.00000  & -6.13975 \\ \hline
		200   & 1.00000  & -6.12828 \\ \hline
		500   & 1.00000  & -6.11158 \\ \hline
			10    & 5.00000  & -6.21304 \\ \hline
		50    & 5.00000  & -6.16839 \\ \hline
		100   & 5.00000  & -6.12180 \\ \hline
			200   & 5.00000  & -6.10381 \\ \hline
			500   & 5.00000  & -6.11475 \\ \hline
			 10    & 10.00000 & -6.22862 \\ \hline
			 50    & 10.00000 & -6.15134 \\ \hline
			 100   & 10.00000 & -6.11265 \\ \hline
		200   & 10.00000 & -6.12942 \\ \hline
		500   & 10.00000 & -6.10152 \\ \hline
	\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: nltcs. GRASP con rumore avente varianza di 1, k=2 e times=20}
	\label{nltcsgrasp9}\\
	\hline
	mnist & alpha    & test\_ll \\ \hline
		10    & 0.50000  & -6.31276 \\ \hline
		 50    & 0.50000  & -6.26739 \\ \hline
			 100   & 0.50000  & -6.17068 \\ \hline
			 200   & 0.50000  & -6.12701 \\ \hline
		 500   & 0.50000  & -6.10599 \\ \hline
 10    & 1.00000  & -6.30525 \\ \hline
		 50    & 1.00000  & -6.21461 \\ \hline
			 100   & 1.00000  & -6.15654 \\ \hline
		200   & 1.00000  & -6.11267 \\ \hline
		 500   & 1.00000  & -6.08817 \\ \hline
		 10    & 5.00000  & -6.21272 \\ \hline
			 50    & 5.00000  & -6.13990 \\ \hline
		 100   & 5.00000  & -6.09971 \\ \hline
		200   & 5.00000  & -6.08174 \\ \hline
		 500   & 5.00000  & -6.08593 \\ \hline
 10    & 10.00000 & -6.15607 \\ \hline
		50    & 10.00000 & -6.15051 \\ \hline
100   & 10.00000 & -6.13046 \\ \hline
	 200   & 10.00000 & -6.10758 \\ \hline
		 500   & 10.00000 & -6.13638 \\ \hline
		\end{longtable}
	
	\begin{longtable}{|l|l|l|}
		\caption{dataset: nltcs. GRASP con rumore avente varianza di 2, k=2 e times=20}
		\label{nltcsgrasp10}\\
		\hline
		mnist & alpha    & test\_ll \\ \hline
		10    & 0.50000  & -6.36306 \\ \hline
	50    & 0.50000  & -6.25484 \\ \hline
		 100   & 0.50000  & -6.15950 \\ \hline
		200   & 0.50000  & -6.12439 \\ \hline
		500   & 0.50000  & -6.09978 \\ \hline
	 10    & 1.00000  & -6.32357 \\ \hline
	 50    & 1.00000  & -6.20452 \\ \hline
		100   & 1.00000  & -6.14770 \\ \hline
		200   & 1.00000  & -6.13809 \\ \hline
	500   & 1.00000  & -6.10508 \\ \hline
	 10    & 5.00000  & -6.22282 \\ \hline
		 50    & 5.00000  & -6.16231 \\ \hline
	100   & 5.00000  & -6.12614 \\ \hline
	 200   & 5.00000  & -6.16197 \\ \hline
	 500   & 5.00000  & -6.09882 \\ \hline
	 10    & 10.00000 & -6.17125 \\ \hline
 50    & 10.00000 & -6.11007 \\ \hline
100   & 10.00000 & -6.11713 \\ \hline
		 200   & 10.00000 & -6.11669 \\ \hline
		 500   & 10.00000 & -6.13722 \\ \hline
		\end{longtable}
	\subsubsection{msnbc}
	\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Nessun algoritmo di SLS applicato}
	\label{msnbc}\\
	\hline
	 alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.1018  \\ \hline
 0.5   & 50    & inf      \\ \hline
 0.5   & 100   & -6.07966 \\ \hline
 0.5   & 200   & inf      \\ \hline
 0.5   & 500   & -6.06059 \\ \hline
 1     & 10    & -6.09292 \\ \hline
 1     & 50    & -6.08174 \\ \hline
 1     & 100   & inf      \\ \hline
 1     & 200   & -6.06477 \\ \hline
 1     & 500   & -6.05681 \\ \hline
 5     & 10    & -6.07298 \\ \hline
 5     & 50    & -6.06575 \\ \hline
 5     & 100   & -6.06108 \\ \hline
 5     & 200   & -6.05478 \\ \hline
 5     & 500   & -6.05071 \\ \hline
 10    & 10    & -6.06688 \\ \hline
 10    & 50    & -6.06139 \\ \hline
 10    & 100   & inf      \\ \hline
 10    & 200   & inf      \\ \hline
 10    & 500   & -6.04823 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Nessun algoritmo di SLS applicato,rumore applicato con varianza di 0.1}
	\label{msnbc2}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.09886 \\ \hline
0.5   & 50    & -6.08239 \\ \hline
 0.5   & 100   & -6.07298 \\ \hline
 0.5   & 200   & -6.06176 \\ \hline
 0.5   & 500   & -6.05368 \\ \hline
 1     & 10    & -6.09274 \\ \hline
 1     & 50    & -6.07793 \\ \hline
 1     & 100   & -6.06657 \\ \hline
 1     & 200   & -6.05812 \\ \hline
 1     & 500   & -6.05271 \\ \hline
 5     & 10    & -6.07003 \\ \hline
 5     & 50    & -6.05995 \\ \hline
 5     & 100   & -6.05551 \\ \hline
 5     & 200   & -6.05151 \\ \hline
 5     & 500   & -6.04912 \\ \hline
 10    & 10    & -6.06188 \\ \hline
 10    & 50    & -6.05527 \\ \hline
 10    & 100   & -6.0503  \\ \hline
 10    & 200   & -6.04997 \\ \hline
 10    & 500   & -6.0487  \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Nessun algoritmo di SLS applicato,rumore applicato con varianza di 0.5}
	\label{msnbc3}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.0972  \\ \hline
 0.5   & 50    & -6.08376 \\ \hline
0.5   & 100   & -6.07216 \\ \hline
0.5   & 200   & -6.06044 \\ \hline
0.5   & 500   & -6.05297 \\ \hline
 1     & 10    & -6.08701 \\ \hline
 1     & 50    & -6.07463 \\ \hline
 1     & 100   & -6.06328 \\ \hline
 1     & 200   & -6.05618 \\ \hline
 1     & 500   & -6.05208 \\ \hline
 5     & 10    & -6.06685 \\ \hline
 5     & 50    & -6.05719 \\ \hline
 5     & 100   & -6.05412 \\ \hline
 5     & 200   & -6.05006 \\ \hline
 5     & 500   & -6.04893 \\ \hline
 10    & 10    & -6.06238 \\ \hline
10    & 50    & -6.05576 \\ \hline
10    & 100   & -6.05054 \\ \hline
10    & 200   & -6.04581 \\ \hline
10    & 500   & -6.04681 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Nessun algoritmo di SLS applicato,rumore applicato con varianza di 1}
	\label{msnbc4}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.09274 \\ \hline
 0.5   & 50    & -6.07833 \\ \hline
 0.5   & 100   & -6.06783 \\ \hline
 0.5   & 200   & -6.06107 \\ \hline
 0.5   & 500   & -6.05507 \\ \hline
 1     & 10    & -6.08243 \\ \hline
 1     & 50    & -6.07167 \\ \hline
 1     & 100   & -6.06303 \\ \hline
 1     & 200   & -6.05582 \\ \hline
 1     & 500   & -6.05129 \\ \hline
 5     & 10    & -6.06833 \\ \hline
 5     & 50    & -6.05931 \\ \hline
5     & 100   & -6.05235 \\ \hline
 5     & 200   & -6.04864 \\ \hline
5     & 500   & -6.04735 \\ \hline
10    & 10    & -6.05943 \\ \hline
 10    & 50    & -6.05342 \\ \hline
10    & 100   & -6.0498  \\ \hline
10    & 200   & -6.04698 \\ \hline
10    & 500   & -6.04605 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Nessun algoritmo di SLS applicato,rumore applicato con varianza di 2}
	\label{msnbc5}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.09138 \\ \hline
 0.5   & 50    & -6.07544 \\ \hline
 0.5   & 100   & -6.06719 \\ \hline
 0.5   & 200   & -6.06099 \\ \hline
 0.5   & 500   & -6.05473 \\ \hline
 1     & 10    & -6.0853  \\ \hline
 1     & 50    & -6.07152 \\ \hline
 1     & 100   & -6.06323 \\ \hline
 1     & 200   & -6.05515 \\ \hline
 1     & 500   & -6.05456 \\ \hline
 5     & 10    & -6.06864 \\ \hline
 5     & 50    & -6.0561  \\ \hline
 5     & 100   & -6.05451 \\ \hline
 5     & 200   & -6.04725 \\ \hline
 5     & 500   & -6.04916 \\ \hline
10    & 10    & -6.06131 \\ \hline
 10    & 50    & -6.05342 \\ \hline
 10    & 100   & -6.05042 \\ \hline
 10    & 200   & -6.04709 \\ \hline
 10    & 500   & -6.04758 \\ \hline
\end{longtable}
	\begin{longtable}{|l|l|l|}
		\caption{dataset: msnbc.Iterative Improvement senza rumore}
		\label{msnbcii}\\
		\hline
	 alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.09493 \\ \hline
 0.5   & 50    & -6.08666 \\ \hline
 0.5   & 100   & -6.0794  \\ \hline
 0.5   & 200   & -6.06953 \\ \hline
 0.5   & 500   & -6.06056 \\ \hline
 1     & 10    & -6.08689 \\ \hline
 1     & 50    & -6.08127 \\ \hline
 1     & 100   & -6.07352 \\ \hline
 1     & 200   & -6.0645  \\ \hline
 1     & 500   & -6.05669 \\ \hline
 5     & 10    & -6.06885 \\ \hline
 5     & 50    & -6.0654  \\ \hline
 5     & 100   & -6.06166 \\ \hline
 5     & 200   & -6.05475 \\ \hline
5     & 500   & -6.05071 \\ \hline
 10    & 10    & -6.06344 \\ \hline
 10    & 50    & -6.06084 \\ \hline
 10    & 100   & -6.05697 \\ \hline
 10    & 200   & -6.05163 \\ \hline
 10    & 500   & -6.04827 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc.Iterative Improvement con rumore avente varianza di 0.1}
	\label{msnbcii2}\\
	\hline
	alpha & minst & test\_ll \\ \hline
0.5   & 10    & -6.09395 \\ \hline
 0.5   & 50    & -6.08152 \\ \hline
 0.5   & 100   & -6.07256 \\ \hline
 0.5   & 200   & -6.06152 \\ \hline
 0.5   & 500   & -6.05379 \\ \hline
 1     & 10    & -6.08756 \\ \hline
 1     & 50    & -6.07749 \\ \hline
 1     & 100   & -6.06647 \\ \hline
 1     & 200   & -6.05814 \\ \hline
 1     & 500   & -6.0527  \\ \hline
 5     & 10    & -6.06711 \\ \hline
 5     & 50    & -6.05966 \\ \hline
 5     & 100   & -6.05552 \\ \hline
 5     & 200   & -6.05163 \\ \hline
 5     & 500   & -6.04904 \\ \hline
 10    & 10    & -6.05904 \\ \hline
10    & 50    & -6.05531 \\ \hline
10    & 100   & -6.05032 \\ \hline
 10    & 200   & -6.04996 \\ \hline
 10    & 500   & -6.04871 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc.Iterative Improvement con rumore avente varianza di 0.5}
	\label{msnbcii3}\\
	\hline
	alpha & minst & test\_ll \\ \hline
	 0.5   & 10    & -6.09274 \\ \hline
	 0.5   & 50    & -6.08328 \\ \hline
	 0.5   & 100   & -6.07211 \\ \hline
	0.5   & 200   & -6.06014 \\ \hline
 0.5   & 500   & -6.05293 \\ \hline
	1     & 10    & -6.08388 \\ \hline
	 1     & 50    & -6.07409 \\ \hline
1     & 100   & -6.06299 \\ \hline
	 1     & 200   & -6.05605 \\ \hline
		 1     & 500   & -6.05199 \\ \hline
	 5     & 10    & -6.06495 \\ \hline
	5     & 50    & -6.05685 \\ \hline
		 5     & 100   & -6.05424 \\ \hline
	 5     & 200   & -6.04987 \\ \hline
		 5     & 500   & -6.04889 \\ \hline
	10    & 10    & -6.06049 \\ \hline
	 10    & 50    & -6.05561 \\ \hline
	 10    & 100   & -6.05041 \\ \hline
		10    & 200   & -6.04582 \\ \hline
10    & 500   & -6.04687 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc.Iterative Improvement con rumore avente varianza di 1}
	\label{msnbcii3}\\
	\hline
	alpha & minst & test\_ll \\ \hline
	 0.5   & 10    & -6.08724 \\ \hline
		0.5   & 50    & -6.07759 \\ \hline
	0.5   & 100   & -6.0676  \\ \hline
0.5   & 200   & -6.06088 \\ \hline
	0.5   & 500   & -6.05501 \\ \hline
 1     & 10    & -6.07757 \\ \hline
		 1     & 50    & -6.07125 \\ \hline
	 1     & 100   & -6.06285 \\ \hline
 1     & 200   & -6.05574 \\ \hline
		 1     & 500   & -6.05131 \\ \hline
 5     & 10    & -6.066   \\ \hline
	5     & 50    & -6.05903 \\ \hline
	 5     & 100   & -6.05238 \\ \hline
 5     & 200   & -6.04863 \\ \hline
 5     & 500   & -6.04754 \\ \hline
	 10    & 10    & -6.05782 \\ \hline
	10    & 50    & -6.05321 \\ \hline
	 10    & 100   & -6.04983 \\ \hline
	 10    & 200   & -6.04695 \\ \hline
	10    & 500   & -6.04605 \\ \hline
	\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc.Iterative Improvement con rumore avente varianza di 2}
	\label{msnbcii4}\\
	\hline
	alpha & minst & test\_ll \\ \hline
	 0.5   & 10    & -6.08687 \\ \hline
	 0.5   & 50    & -6.07454 \\ \hline
 0.5   & 100   & -6.06695 \\ \hline
		 0.5   & 200   & -6.06082 \\ \hline
	 0.5   & 500   & -6.05477 \\ \hline
	1     & 10    & -6.08093 \\ \hline
 1     & 50    & -6.07083 \\ \hline
		 1     & 100   & -6.06324 \\ \hline
	 1     & 200   & -6.05518 \\ \hline
	1     & 500   & -6.0546  \\ \hline
	5     & 10    & -6.06582 \\ \hline
	 5     & 50    & -6.05563 \\ \hline
	5     & 100   & -6.05443 \\ \hline
	 5     & 200   & -6.04738 \\ \hline
		 5     & 500   & -6.04913 \\ \hline
 10    & 10    & -6.06044 \\ \hline
	10    & 50    & -6.05318 \\ \hline
10    & 100   & -6.05036 \\ \hline
	 10    & 200   & -6.04705 \\ \hline
		 10    & 500   & -6.04755 \\ \hline
	\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Randomised Iterative Improvement senza rumore, probabilità di 0.6, times=10}
	\label{msnbcrii}\\
	\hline
	 alpha & minst & test\_ll \\ \hline
0.5   & 10    & inf      \\ \hline
0.5   & 50    & inf      \\ \hline
0.5   & 100   & -6.07953 \\ \hline
0.5   & 200   & inf      \\ \hline
0.5   & 500   & -6.06056 \\ \hline
 1     & 10    & inf      \\ \hline
 1     & 50    & inf      \\ \hline
 1     & 100   & -6.07351 \\ \hline
 1     & 200   & -6.06451 \\ \hline
 1     & 500   & -6.05669 \\ \hline
 5     & 10    & -6.06886 \\ \hline
 5     & 50    & -6.0654  \\ \hline
 5     & 100   & -6.06092 \\ \hline
 5     & 200   & -6.05483 \\ \hline
 5     & 500   & -6.05071 \\ \hline
 10    & 10    & -6.06375 \\ \hline
 10    & 50    & -6.06083 \\ \hline
 10    & 100   & -6.05703 \\ \hline
10    & 200   & inf      \\ \hline
 10    & 500   & -6.04823 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Randomised Iterative Improvement con rumore avente varianza di 0.1, probabilità di 0.6, times=10}
	\label{msnbcrii2}\\
	\hline
	alpha & minst & test\_ll \\ \hline
0.5   & 10    & -6.09396 \\ \hline
 0.5   & 50    & -6.08152 \\ \hline
 0.5   & 100   & -6.07256 \\ \hline
 0.5   & 200   & -6.06152 \\ \hline
 0.5   & 500   & -6.05379 \\ \hline
 1     & 10    & -6.08763 \\ \hline
 1     & 50    & -6.07749 \\ \hline
 1     & 100   & -6.06647 \\ \hline
 1     & 200   & -6.05814 \\ \hline
 1     & 500   & -6.0527  \\ \hline
 5     & 10    & -6.06711 \\ \hline
 5     & 50    & -6.05966 \\ \hline
 5     & 100   & -6.05552 \\ \hline
 5     & 200   & -6.05163 \\ \hline
 5     & 500   & -6.04904 \\ \hline
 10    & 10    & -6.05904 \\ \hline
10    & 50    & -6.0553  \\ \hline
10    & 100   & -6.05032 \\ \hline
10    & 200   & -6.04996 \\ \hline
 10    & 500   & -6.04871 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Randomised Iterative Improvement con rumore avente varianza di 0.5, probabilità di 0.6, times=10}
	\label{msnbcrii3}\\
	\hline
	alpha & minst & test\_ll \\ \hline
0.5   & 10    & -6.09274 \\ \hline
0.5   & 50    & -6.08328 \\ \hline
0.5   & 100   & -6.07211 \\ \hline
 0.5   & 200   & -6.06014 \\ \hline
 0.5   & 500   & -6.05293 \\ \hline
 1     & 10    & -6.08388 \\ \hline
 1     & 50    & -6.07409 \\ \hline
 1     & 100   & -6.06299 \\ \hline
 1     & 200   & -6.05605 \\ \hline
 1     & 500   & -6.05199 \\ \hline
 5     & 10    & -6.06495 \\ \hline
 5     & 50    & -6.05685 \\ \hline
 5     & 100   & -6.05424 \\ \hline
 5     & 200   & -6.04987 \\ \hline
 5     & 500   & -6.04889 \\ \hline
 10    & 10    & -6.06049 \\ \hline
 10    & 50    & -6.05561 \\ \hline
 10    & 100   & -6.05041 \\ \hline
 10    & 200   & -6.04582 \\ \hline
 10    & 500   & -6.04687 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Randomised Iterative Improvement con rumore avente varianza di 1, probabilità di 0.6, times=10}
	\label{msnbcrii4}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.08724 \\ \hline
0.5   & 50    & -6.07759 \\ \hline
0.5   & 100   & -6.0676  \\ \hline
0.5   & 200   & -6.06088 \\ \hline
0.5   & 500   & -6.05501 \\ \hline
 1     & 10    & -6.07757 \\ \hline
 1     & 50    & -6.07125 \\ \hline
 1     & 100   & -6.06285 \\ \hline
 1     & 200   & -6.05574 \\ \hline
 1     & 500   & -6.05131 \\ \hline
 5     & 10    & -6.066   \\ \hline
 5     & 50    & -6.05903 \\ \hline
 5     & 100   & -6.05238 \\ \hline
 5     & 200   & -6.04863 \\ \hline
 5     & 500   & -6.04754 \\ \hline
 10    & 10    & -6.05782 \\ \hline
 10    & 50    & -6.05321 \\ \hline
 10    & 100   & -6.04981 \\ \hline
 10    & 200   & -6.04695 \\ \hline
 10    & 500   & -6.04605 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. Randomised Iterative Improvement con rumore avente varianza di 2, probabilità di 0.6, times=10}
	\label{msnbcrii5}\\
	\hline
	alpha & minst & test\_ll \\ \hline
 0.5   & 10    & -6.08687 \\ \hline
 0.5   & 50    & -6.07454 \\ \hline
 0.5   & 100   & -6.06695 \\ \hline
 0.5   & 200   & -6.06082 \\ \hline
 0.5   & 500   & -6.05477 \\ \hline
1     & 10    & -6.08093 \\ \hline
 1     & 50    & -6.07083 \\ \hline
 1     & 100   & -6.06324 \\ \hline
 1     & 200   & -6.05518 \\ \hline
 1     & 500   & -6.0546  \\ \hline
 5     & 10    & -6.06582 \\ \hline
 5     & 50    & -6.05563 \\ \hline
5     & 100   & -6.05443 \\ \hline
5     & 200   & -6.04738 \\ \hline
 5     & 500   & -6.04913 \\ \hline
 10    & 10    & -6.06044 \\ \hline
 10    & 50    & -6.05318 \\ \hline
 10    & 100   & -6.05036 \\ \hline
 10    & 200   & -6.04705 \\ \hline
 10    & 500   & -6.04755 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. GRASP senza rumore, k=2 e times=5}
	\label{msnbcgrasp}\\
	\hline
 alpha    & minst & test\_ll \\ \hline
0.50000  & 10    & -6.09476 \\ \hline
 0.50000  & 50    & -6.08666 \\ \hline
 0.50000  & 100   & -6.07940 \\ \hline
 0.50000  & 200   & -6.06953 \\ \hline
 0.50000  & 500   & -6.06056 \\ \hline
1.00000  & 10    & -6.08690 \\ \hline
 1.00000  & 50    & -6.08078 \\ \hline
1.00000  & 100   & -6.07351 \\ \hline
1.00000  & 200   & -6.06449 \\ \hline
 1.00000  & 500   & -6.05670 \\ \hline
 5.00000  & 10    & -6.06884 \\ \hline
 5.00000  & 50    & -6.06545 \\ \hline
 5.00000  & 100   & -6.06092 \\ \hline
5.00000  & 200   & -6.05525 \\ \hline
5.00000  & 500   & -6.05071 \\ \hline
 10.00000 & 10    & inf      \\ \hline
 10.00000 & 50    & -6.06097 \\ \hline
 10.00000 & 100   & -6.05695 \\ \hline
 10.00000 & 200   & -6.05163 \\ \hline
 10.00000 & 500   & -6.04823 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. GRASP con rumore avente varianza di 0.1, k=2 e times=5}
	\label{msnbcgrasp2}\\
	\hline
	alpha    & minst & test\_ll \\ \hline
 0.50000  & 10    & -6.09238 \\ \hline
0.50000  & 50    & -6.08325 \\ \hline
0.50000  & 100   & -6.07069 \\ \hline
0.50000  & 200   & -6.06160 \\ \hline
0.50000  & 500   & -6.05329 \\ \hline
1.00000  & 10    & -6.08707 \\ \hline
1.00000  & 50    & -6.07506 \\ \hline
1.00000  & 100   & -6.06739 \\ \hline
1.00000  & 200   & -6.05759 \\ \hline
1.00000  & 500   & -6.05438 \\ \hline
5.00000  & 10    & -6.06813 \\ \hline
5.00000  & 50    & -6.05959 \\ \hline
 5.00000  & 100   & -6.05682 \\ \hline
5.00000  & 200   & -6.04910 \\ \hline
5.00000  & 500   & -6.04809 \\ \hline
 10.00000 & 10    & -6.06130 \\ \hline
 10.00000 & 50    & -6.05610 \\ \hline
10.00000 & 100   & -6.05295 \\ \hline
10.00000 & 200   & -6.04846 \\ \hline
10.00000 & 500   & -6.04604 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. GRASP con rumore avente varianza di 0.5, k=2 e times=5}
	\label{msnbcgrasp3}\\
	\hline
	alpha    & minst & test\_ll \\ \hline
 0.50000  & 10    & -6.08897 \\ \hline
 0.50000  & 50    & -6.07542 \\ \hline
 0.50000  & 100   & -6.06913 \\ \hline
 0.50000  & 200   & -6.06118 \\ \hline
 0.50000  & 500   & -6.05478 \\ \hline
 1.00000  & 10    & -6.08613 \\ \hline
 1.00000  & 50    & -6.07488 \\ \hline
1.00000  & 100   & -6.06214 \\ \hline
1.00000  & 200   & -6.05710 \\ \hline
 1.00000  & 500   & -6.05409 \\ \hline
 5.00000  & 10    & -6.06637 \\ \hline
 5.00000  & 50    & -6.05930 \\ \hline
 5.00000  & 100   & -6.05647 \\ \hline
 5.00000  & 200   & -6.04992 \\ \hline
5.00000  & 500   & -6.04978 \\ \hline
10.00000 & 10    & -6.05899 \\ \hline
 10.00000 & 50    & -6.05585 \\ \hline
10.00000 & 100   & -6.04989 \\ \hline
10.00000 & 200   & -6.04542 \\ \hline
10.00000 & 500   & -6.04800 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. GRASP con rumore avente varianza di 1, k=2 e times=5}
	\label{msnbcgrasp4}\\
	\hline
	alpha    & minst & test\_ll \\ \hline
 0.50000  & 10    & -6.08719 \\ \hline
 0.50000  & 50    & -6.07822 \\ \hline
 0.50000  & 100   & -6.06644 \\ \hline
 0.50000  & 200   & -6.05603 \\ \hline
 0.50000  & 500   & -6.05635 \\ \hline
 1.00000  & 10    & -6.08040 \\ \hline
 1.00000  & 50    & -6.07218 \\ \hline
 1.00000  & 100   & -6.06226 \\ \hline
 1.00000  & 200   & -6.05678 \\ \hline
 1.00000  & 500   & -6.05241 \\ \hline
 5.00000  & 10    & -6.06546 \\ \hline
 5.00000  & 50    & -6.05786 \\ \hline
 5.00000  & 100   & -6.05365 \\ \hline
 5.00000  & 200   & -6.04866 \\ \hline
 5.00000  & 500   & -6.05200 \\ \hline
 10.00000 & 10    & -6.06130 \\ \hline
 10.00000 & 50    & -6.05297 \\ \hline
 10.00000 & 100   & -6.04815 \\ \hline
10.00000 & 200   & -6.04767 \\ \hline
10.00000 & 500   & -6.04674 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: msnbc. GRASP con rumore avente varianza di 2, k=2 e times=5}
	\label{msnbcgrasp5}\\
	\hline
	alpha    & minst & test\_ll \\ \hline
 0.50000  & 10    & -6.08746 \\ \hline
 0.50000  & 50    & -6.07373 \\ \hline
 0.50000  & 100   & -6.06740 \\ \hline
 0.50000  & 200   & -6.06083 \\ \hline
 0.50000  & 500   & -6.05353 \\ \hline
 1.00000  & 10    & -6.07751 \\ \hline
 1.00000  & 50    & -6.07044 \\ \hline
 1.00000  & 100   & -6.06386 \\ \hline
 1.00000  & 200   & -6.05509 \\ \hline
 1.00000  & 500   & -6.05438 \\ \hline
 5.00000  & 10    & -6.06289 \\ \hline
 5.00000  & 50    & -6.05700 \\ \hline
 5.00000  & 100   & -6.05271 \\ \hline
5.00000  & 200   & -6.05128 \\ \hline
 5.00000  & 500   & -6.04989 \\ \hline
 10.00000 & 10    & -6.05792 \\ \hline
 10.00000 & 50    & -6.05265 \\ \hline
 10.00000 & 100   & -6.04898 \\ \hline
 10.00000 & 200   & -6.04638 \\ \hline
 10.00000 & 500   & -6.04658 \\ \hline
\end{longtable}

\subsubsection{plants}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Nessun algoritmo di SLS applicato}
	\label{plants}\\
	\hline
	 alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -18.34506 \\ \hline
0.50000  & 50    & -15.54635 \\ \hline
0.50000  & 100   & -14.43864 \\ \hline
 0.50000  & 200   & -13.74225 \\ \hline
 0.50000  & 500   & -13.42109 \\ \hline
1.00000  & 10    & -17.56881 \\ \hline
 1.00000  & 50    & -15.16388 \\ \hline
 1.00000  & 100   & -14.14518 \\ \hline
 1.00000  & 200   & -13.61191 \\ \hline
 1.00000  & 500   & -13.37280 \\ \hline
 5.00000  & 10    & -15.79672 \\ \hline
 5.00000  & 50    & -14.13266 \\ \hline
 5.00000  & 100   & -13.61289 \\ \hline
  5.00000  & 200   & -13.38759 \\ \hline
 5.00000  & 500   & -13.31657 \\ \hline
 10.00000 & 10    & -15.12328 \\ \hline
 10.00000 & 50    & -13.93696 \\ \hline
 10.00000 & 100   & -13.50377 \\ \hline
 10.00000 & 200   & -13.33208 \\ \hline
 10.00000 & 500   & -13.34332 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Nessun algoritmo di SLS applicato, rumore avente varianza di 0.1}
	\label{plants2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -18.22500 \\ \hline
0.50000  & 50    & -15.18123 \\ \hline
0.50000  & 100   & -14.49518 \\ \hline
 0.50000  & 200   & -14.14441 \\ \hline
 0.50000  & 500   & -14.14831 \\ \hline
 1.00000  & 10    & -17.65165 \\ \hline
 1.00000  & 50    & -14.88795 \\ \hline
 1.00000  & 100   & -14.29635 \\ \hline
 1.00000  & 200   & -14.12616 \\ \hline
1.00000  & 500   & -14.18353 \\ \hline
5.00000  & 10    & -16.25273 \\ \hline
 5.00000  & 50    & -14.36497 \\ \hline
 5.00000  & 100   & -14.06806 \\ \hline
5.00000  & 200   & -14.02948 \\ \hline
5.00000  & 500   & -14.26160 \\ \hline
10.00000 & 10    & -15.40876 \\ \hline
10.00000 & 50    & -14.24034 \\ \hline
10.00000 & 100   & -13.91597 \\ \hline
 10.00000 & 200   & -13.95932 \\ \hline
 10.00000 & 500   & -14.37871 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Nessun algoritmo di SLS applicato, rumore avente varianza di 0.5}
	\label{plants3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -16.69839 \\ \hline
 0.50000  & 50    & -14.92640 \\ \hline
 0.50000  & 100   & -14.52405 \\ \hline
 0.50000  & 200   & -14.41954 \\ \hline
 0.50000  & 500   & -15.39397 \\ \hline
1.00000  & 10    & -16.15963 \\ \hline
1.00000  & 50    & -14.55360 \\ \hline
 1.00000  & 100   & -14.27099 \\ \hline
 1.00000  & 200   & -14.49579 \\ \hline
1.00000  & 500   & -15.38226 \\ \hline
5.00000  & 10    & -15.32557 \\ \hline
5.00000  & 50    & -14.38040 \\ \hline
 5.00000  & 100   & -14.19167 \\ \hline
 5.00000  & 200   & -14.38686 \\ \hline
 5.00000  & 500   & -15.28087 \\ \hline
 10.00000 & 10    & -14.79711 \\ \hline
 10.00000 & 50    & -14.17515 \\ \hline
10.00000 & 100   & -14.37276 \\ \hline
 10.00000 & 200   & -14.55493 \\ \hline
 10.00000 & 500   & -15.39746 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Nessun algoritmo di SLS applicato, rumore avente varianza di 1}
	\label{plants4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -16.17996 \\ \hline
 0.50000  & 50    & -14.71248 \\ \hline
 0.50000  & 100   & -14.52424 \\ \hline
 0.50000  & 200   & -14.59403 \\ \hline
 0.50000  & 500   & -15.25462 \\ \hline
 1.00000  & 10    & -15.84489 \\ \hline
 1.00000  & 50    & -14.58301 \\ \hline
 1.00000  & 100   & -14.40907 \\ \hline
 1.00000  & 200   & -14.47674 \\ \hline
 1.00000  & 500   & -15.34071 \\ \hline
 5.00000  & 10    & -14.81431 \\ \hline
 5.00000  & 50    & -14.17908 \\ \hline
 5.00000  & 100   & -14.19804 \\ \hline
 5.00000  & 200   & -14.51679 \\ \hline
5.00000  & 500   & -15.24408 \\ \hline
 10.00000 & 10    & -14.74814 \\ \hline
 10.00000 & 50    & -14.11680 \\ \hline
 10.00000 & 100   & -14.33496 \\ \hline
 10.00000 & 200   & -14.64048 \\ \hline
 10.00000 & 500   & -15.35965 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Nessun algoritmo di SLS applicato, rumore avente varianza di 2}
	\label{plants5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -15.95725 \\ \hline
 0.50000  & 50    & -14.51371 \\ \hline
 0.50000  & 100   & -14.48357 \\ \hline
 0.50000  & 200   & -14.66337 \\ \hline
 0.50000  & 500   & -15.24563 \\ \hline
 1.00000  & 10    & -15.76220 \\ \hline
 1.00000  & 50    & -14.36964 \\ \hline
 1.00000  & 100   & -14.31814 \\ \hline
 1.00000  & 200   & -14.42664 \\ \hline
 1.00000  & 500   & -15.40555 \\ \hline
 5.00000  & 10    & -14.93321 \\ \hline
 5.00000  & 50    & -14.19586 \\ \hline
 5.00000  & 100   & -14.24181 \\ \hline
 5.00000  & 200   & -14.39096 \\ \hline
 5.00000  & 500   & -15.28972 \\ \hline
 10.00000 & 10    & -14.72755 \\ \hline
 10.00000 & 50    & -14.23618 \\ \hline
 10.00000 & 100   & -14.25716 \\ \hline
 10.00000 & 200   & -14.63343 \\ \hline
 10.00000 & 500   & -15.59013 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Iterative improvement senza rumore}
	\label{plantsii}\\
	\hline
 alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -18.07949 \\ \hline
0.50000  & 50    & -15.54599 \\ \hline
 0.50000  & 100   & -14.43883 \\ \hline
 0.50000  & 200   & -13.74234 \\ \hline
 0.50000  & 500   & -13.42111 \\ \hline
 1.00000  & 10    & -17.34970 \\ \hline
 1.00000  & 50    & -15.16379 \\ \hline
 1.00000  & 100   & -14.14551 \\ \hline
 1.00000  & 200   & -13.61206 \\ \hline
1.00000  & 500   & -13.37280 \\ \hline
 5.00000  & 10    & -15.67038 \\ \hline
 5.00000  & 50    & -14.13396 \\ \hline
5.00000  & 100   & -13.61419 \\ \hline
 5.00000  & 200   & -13.38810 \\ \hline
 5.00000  & 500   & -13.31657 \\ \hline
 10.00000 & 10    & -15.04656 \\ \hline
 10.00000 & 50    & -13.93858 \\ \hline
 10.00000 & 100   & -13.50459 \\ \hline
 10.00000 & 200   & -13.33308 \\ \hline
 10.00000 & 500   & -13.34332 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Iterative improvement con rumore avente varianza di 0.1}
	\label{plantsii2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -17.94585 \\ \hline
 0.50000  & 50    & -15.18334 \\ \hline
 0.50000  & 100   & -14.49583 \\ \hline
 0.50000  & 200   & -14.14443 \\ \hline
 0.50000  & 500   & -14.14837 \\ \hline
 1.00000  & 10    & -17.39636 \\ \hline
 1.00000  & 50    & -14.88956 \\ \hline
1.00000  & 100   & -14.29663 \\ \hline
1.00000  & 200   & -14.12585 \\ \hline
 1.00000  & 500   & -14.18353 \\ \hline
 5.00000  & 10    & -16.15021 \\ \hline
 5.00000  & 50    & -14.37039 \\ \hline
 5.00000  & 100   & -14.06819 \\ \hline
 5.00000  & 200   & -14.03037 \\ \hline
 5.00000  & 500   & -14.26164 \\ \hline
 10.00000 & 10    & -15.32935 \\ \hline
 10.00000 & 50    & -14.24338 \\ \hline
 10.00000 & 100   & -13.92070 \\ \hline
 10.00000 & 200   & -13.95949 \\ \hline
 10.00000 & 500   & -14.37927 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Iterative improvement con rumore avente varianza di 0.5}
	\label{plantsii3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -16.59947 \\ \hline
0.50000  & 50    & -14.92737 \\ \hline
 0.50000  & 100   & -14.52380 \\ \hline
 0.50000  & 200   & -14.41984 \\ \hline
 0.50000  & 500   & -15.39397 \\ \hline
 1.00000  & 10    & -16.06660 \\ \hline
 1.00000  & 50    & -14.54638 \\ \hline
 1.00000  & 100   & -14.27031 \\ \hline
 1.00000  & 200   & -14.49488 \\ \hline
 1.00000  & 500   & -15.38227 \\ \hline
 5.00000  & 10    & -15.27942 \\ \hline
 5.00000  & 50    & -14.38140 \\ \hline
 5.00000  & 100   & -14.19264 \\ \hline
 5.00000  & 200   & -14.38723 \\ \hline
 5.00000  & 500   & -15.28089 \\ \hline
 10.00000 & 10    & -14.76145 \\ \hline
 10.00000 & 50    & -14.17603 \\ \hline
 10.00000 & 100   & -14.37370 \\ \hline
 10.00000 & 200   & -14.55570 \\ \hline
 10.00000 & 500   & -15.39762 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Iterative improvement con rumore avente varianza di 1}
	\label{plantsii4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -16.08432 \\ \hline
0.50000  & 50    & -14.71175 \\ \hline
 0.50000  & 100   & -14.52416 \\ \hline
 0.50000  & 200   & -14.59392 \\ \hline
 0.50000  & 500   & -15.25461 \\ \hline
 1.00000  & 10    & -15.77395 \\ \hline
 1.00000  & 50    & -14.58409 \\ \hline
 1.00000  & 100   & -14.40922 \\ \hline
 1.00000  & 200   & -14.47689 \\ \hline
 1.00000  & 500   & -15.34074 \\ \hline
 5.00000  & 10    & -14.75881 \\ \hline
 5.00000  & 50    & -14.18051 \\ \hline
 5.00000  & 100   & -14.19975 \\ \hline
 5.00000  & 200   & -14.51665 \\ \hline
 5.00000  & 500   & -15.24408 \\ \hline
 10.00000 & 10    & -14.68411 \\ \hline
 10.00000 & 50    & -14.11847 \\ \hline
 10.00000 & 100   & -14.33542 \\ \hline
 10.00000 & 200   & -14.64369 \\ \hline
 10.00000 & 500   & -15.35965 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Iterative improvement con rumore avente varianza di 2}
	\label{plantsii5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
	 0.50000  & 10    & -15.87610 \\ \hline
0.50000  & 50    & -14.51356 \\ \hline
0.50000  & 100   & -14.48728 \\ \hline
 0.50000  & 200   & -14.66342 \\ \hline
 0.50000  & 500   & -15.24564 \\ \hline
 1.00000  & 10    & -15.67484 \\ \hline
 1.00000  & 50    & -14.36938 \\ \hline
 1.00000  & 100   & -14.31769 \\ \hline
 1.00000  & 200   & -14.42674 \\ \hline
 1.00000  & 500   & -15.40555 \\ \hline
5.00000  & 10    & -14.89405 \\ \hline
 5.00000  & 50    & -14.19666 \\ \hline
 5.00000  & 100   & -14.24246 \\ \hline
 5.00000  & 200   & -14.39173 \\ \hline
 5.00000  & 500   & -15.28972 \\ \hline
 10.00000 & 10    & -14.70370 \\ \hline
 10.00000 & 50    & -14.23923 \\ \hline
 10.00000 & 100   & -14.26126 \\ \hline
 10.00000 & 200   & -14.63441 \\ \hline
 10.00000 & 500   & -15.59006 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Randomised Iterative Improvement senza rumore, probabilità di 0.6 e times=10}
	\label{plantsrii}\\
	\hline
	 alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -18.12027 \\ \hline
 0.50000  & 50    & -15.54599 \\ \hline
 0.50000  & 100   & -14.43883 \\ \hline
 0.50000  & 200   & -13.74234 \\ \hline
 0.50000  & 500   & -13.42111 \\ \hline
 1.00000  & 10    & -17.40515 \\ \hline
 1.00000  & 50    & -15.16363 \\ \hline
 1.00000  & 100   & -14.14550 \\ \hline
 1.00000  & 200   & -13.61206 \\ \hline
 1.00000  & 500   & -13.37280 \\ \hline
 5.00000  & 10    & -15.69009 \\ \hline
 5.00000  & 50    & -14.13380 \\ \hline
 5.00000  & 100   & -13.61404 \\ \hline
 5.00000  & 200   & -13.38810 \\ \hline
5.00000  & 500   & -13.31657 \\ \hline
10.00000 & 10    & -15.06204 \\ \hline
10.00000 & 50    & -13.93820 \\ \hline
10.00000 & 100   & -13.50433 \\ \hline
 10.00000 & 200   & -13.33308 \\ \hline
 10.00000 & 500   & -13.34332 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Randomised Iterative Improvement con rumore avente varianza di 0.1, probabilità di 0.6 e times=10}
	\label{plantsrii2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -17.99325 \\ \hline
 0.50000  & 50    & -15.18332 \\ \hline
 0.50000  & 100   & -14.49586 \\ \hline
 0.50000  & 200   & -14.14443 \\ \hline
 0.50000  & 500   & -14.14837 \\ \hline
 1.00000  & 10    & -17.43437 \\ \hline
 1.00000  & 50    & -14.88944 \\ \hline
 1.00000  & 100   & -14.29537 \\ \hline
 1.00000  & 200   & -14.12585 \\ \hline
 1.00000  & 500   & -14.18353 \\ \hline
 5.00000  & 10    & -16.15608 \\ \hline
 5.00000  & 50    & -14.36950 \\ \hline
 5.00000  & 100   & -14.06819 \\ \hline
 5.00000  & 200   & -14.03037 \\ \hline
 5.00000  & 500   & -14.26164 \\ \hline
 10.00000 & 10    & -15.33884 \\ \hline
10.00000 & 50    & -14.24338 \\ \hline
10.00000 & 100   & -13.92018 \\ \hline
10.00000 & 200   & -13.95949 \\ \hline
 10.00000 & 500   & -14.37923 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Randomised Iterative Improvement con rumore avente varianza di 0.5, probabilità di 0.6 e times=10}
	\label{plantsrii3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -16.62150 \\ \hline
 0.50000  & 50    & -14.92741 \\ \hline
 0.50000  & 100   & -14.52380 \\ \hline
 0.50000  & 200   & -14.41973 \\ \hline
 0.50000  & 500   & -15.39397 \\ \hline
 1.00000  & 10    & -16.07981 \\ \hline
 1.00000  & 50    & -14.54621 \\ \hline
 1.00000  & 100   & -14.27029 \\ \hline
 1.00000  & 200   & -14.49491 \\ \hline
 1.00000  & 500   & -15.38227 \\ \hline
 5.00000  & 10    & -15.28454 \\ \hline
 5.00000  & 50    & -14.38137 \\ \hline
 5.00000  & 100   & -14.19260 \\ \hline
 5.00000  & 200   & -14.38735 \\ \hline
 5.00000  & 500   & -15.28089 \\ \hline
 10.00000 & 10    & -14.76749 \\ \hline
 10.00000 & 50    & -14.17603 \\ \hline
10.00000 & 100   & -14.37370 \\ \hline
10.00000 & 200   & -14.55570 \\ \hline
10.00000 & 500   & -15.39762 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Randomised Iterative Improvement con rumore avente varianza di 1, probabilità di 0.6 e times=10}
	\label{plantsrii4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -16.10199 \\ \hline
0.50000  & 50    & -14.71175 \\ \hline
 0.50000  & 100   & -14.52415 \\ \hline
 0.50000  & 200   & -14.59409 \\ \hline
 0.50000  & 500   & -15.25460 \\ \hline
 1.00000  & 10    & -15.78869 \\ \hline
 1.00000  & 50    & -14.58302 \\ \hline
 1.00000  & 100   & -14.40922 \\ \hline
 1.00000  & 200   & -14.47689 \\ \hline
 1.00000  & 500   & -15.34074 \\ \hline
 5.00000  & 10    & -14.76605 \\ \hline
5.00000  & 50    & -14.18053 \\ \hline
5.00000  & 100   & -14.19961 \\ \hline
 5.00000  & 200   & -14.51663 \\ \hline
 5.00000  & 500   & -15.24408 \\ \hline
 10.00000 & 10    & -14.69423 \\ \hline
 10.00000 & 50    & -14.11842 \\ \hline
10.00000 & 100   & -14.33542 \\ \hline
 10.00000 & 200   & -14.64336 \\ \hline
10.00000 & 500   & -15.35965 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: plants. Randomised Iterative Improvement con rumore avente varianza di 2, probabilità di 0.6 e times=10}
	\label{plantsrii5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -15.89351 \\ \hline
 0.50000  & 50    & -14.51393 \\ \hline
 0.50000  & 100   & -14.48723 \\ \hline
0.50000  & 200   & -14.66342 \\ \hline
 0.50000  & 500   & -15.24564 \\ \hline
 1.00000  & 10    & -15.68885 \\ \hline
 1.00000  & 50    & -14.36926 \\ \hline
 1.00000  & 100   & -14.31771 \\ \hline
 1.00000  & 200   & -14.42673 \\ \hline
 1.00000  & 500   & -15.40555 \\ \hline
 5.00000  & 10    & -14.89732 \\ \hline
 5.00000  & 50    & -14.19666 \\ \hline
 5.00000  & 100   & -14.24229 \\ \hline
 5.00000  & 200   & -14.39173 \\ \hline
 5.00000  & 500   & -15.28972 \\ \hline
 10.00000 & 10    & -14.70927 \\ \hline
 10.00000 & 50    & -14.23999 \\ \hline
 10.00000 & 100   & -14.25951 \\ \hline
 10.00000 & 200   & -14.63428 \\ \hline
 10.00000 & 500   & -15.59006 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	
	\caption{dataset: plants. GRASP senza rumore, k=2 e times=5}
	\label{plantsgrasp}\\
		\hline
 alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -18.07949 \\ \hline
 0.50000  & 50    & -15.54599 \\ \hline
 0.50000  & 100   & -14.43883 \\ \hline
 0.50000  & 200   & -13.74234 \\ \hline
 0.50000  & 500   & -13.42111 \\ \hline
 1.00000  & 10    & -17.34970 \\ \hline
 1.00000  & 50    & -15.16379 \\ \hline
 1.00000  & 100   & -14.14551 \\ \hline
 1.00000  & 200   & -13.61206 \\ \hline
 1.00000  & 500   & -13.37280 \\ \hline
 5.00000  & 10    & -15.67038 \\ \hline
 5.00000  & 50    & -14.13396 \\ \hline
 5.00000  & 100   & -13.61419 \\ \hline
 5.00000  & 200   & -13.38810 \\ \hline
 5.00000  & 500   & -13.31657 \\ \hline
 10.00000 & 10    & -15.04656 \\ \hline
 10.00000 & 50    & -13.93858 \\ \hline
 10.00000 & 100   & -13.50459 \\ \hline
 10.00000 & 200   & -13.33308 \\ \hline
 10.00000 & 500   & -13.34332 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	
	\caption{dataset: plants. GRASP con rumore avente varianza di 0.1, k=2 e times=5}
	\label{plantsgrasp2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -18.47828 \\ \hline
 0.50000  & 50    & -15.18229 \\ \hline
 0.50000  & 100   & -14.59109 \\ \hline
 0.50000  & 200   & -14.16208 \\ \hline
 0.50000  & 500   & -14.41231 \\ \hline
 1.00000  & 10    & -17.23546 \\ \hline
 1.00000  & 50    & -14.75868 \\ \hline
 1.00000  & 100   & -14.18205 \\ \hline
 1.00000  & 200   & -14.02953 \\ \hline
 1.00000  & 500   & -14.28273 \\ \hline
 5.00000  & 10    & -15.94011 \\ \hline
 5.00000  & 50    & -14.25848 \\ \hline
 5.00000  & 100   & -14.03509 \\ \hline
 5.00000  & 200   & -13.95989 \\ \hline
 5.00000  & 500   & -14.25904 \\ \hline
 10.00000 & 10    & -15.40849 \\ \hline
 10.00000 & 50    & -14.27418 \\ \hline
 10.00000 & 100   & -13.99200 \\ \hline
 10.00000 & 200   & -14.07465 \\ \hline
10.00000 & 500   & -14.36889 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	
	\caption{dataset: plants. GRASP con rumore avente varianza di 0.5, k=2 e times=5}
	\label{plantsgrasp3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -16.41338 \\ \hline
0.50000  & 50    & -14.89816 \\ \hline
 0.50000  & 100   & -14.52314 \\ \hline
 0.50000  & 200   & -14.54890 \\ \hline
 0.50000  & 500   & -15.31216 \\ \hline
 1.00000  & 10    & -16.36803 \\ \hline
 1.00000  & 50    & -14.49050 \\ \hline
 1.00000  & 100   & -14.42055 \\ \hline
 1.00000  & 200   & -14.43960 \\ \hline
 1.00000  & 500   & -15.21043 \\ \hline
5.00000  & 10    & -15.05668 \\ \hline
 5.00000  & 50    & -14.18184 \\ \hline
 5.00000  & 100   & -14.18369 \\ \hline
 5.00000  & 200   & -14.42763 \\ \hline
 5.00000  & 500   & -15.19951 \\ \hline
 10.00000 & 10    & -14.86253 \\ \hline
 10.00000 & 50    & -14.30112 \\ \hline
 10.00000 & 100   & -14.23049 \\ \hline
 10.00000 & 200   & -14.70801 \\ \hline
 10.00000 & 500   & -15.19930 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	
	\caption{dataset: plants. GRASP con rumore avente varianza di 1, k=2 e times=5}
	\label{plantsgrasp4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -16.20078 \\ \hline
 0.50000  & 50    & -14.94015 \\ \hline
 0.50000  & 100   & -14.47259 \\ \hline
 0.50000  & 200   & -14.61070 \\ \hline
 0.50000  & 500   & -15.22049 \\ \hline
 1.00000  & 10    & -15.63829 \\ \hline
 1.00000  & 50    & -14.56272 \\ \hline
 1.00000  & 100   & -14.41656 \\ \hline
 1.00000  & 200   & -14.51440 \\ \hline
 1.00000  & 500   & -15.15965 \\ \hline
 5.00000  & 10    & -14.81668 \\ \hline
 5.00000  & 50    & -14.17960 \\ \hline
 5.00000  & 100   & -14.12578 \\ \hline
 5.00000  & 200   & -14.51885 \\ \hline
 5.00000  & 500   & -15.33101 \\ \hline
 10.00000 & 10    & -14.63288 \\ \hline
 10.00000 & 50    & -14.22588 \\ \hline
10.00000 & 100   & -14.39067 \\ \hline
 10.00000 & 200   & -14.73953 \\ \hline
 10.00000 & 500   & -15.44410 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	
	\caption{dataset: plants. GRASP con rumore avente varianza di 2, k=2 e times=5}
	\label{plantsgrasp5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -16.12965 \\ \hline
 0.50000  & 50    & -14.62373 \\ \hline
 0.50000  & 100   & -14.37235 \\ \hline
 0.50000  & 200   & -14.65704 \\ \hline
 0.50000  & 500   & -15.25362 \\ \hline
 1.00000  & 10    & -15.63459 \\ \hline
 1.00000  & 50    & -14.37064 \\ \hline
 1.00000  & 100   & -14.32341 \\ \hline
 1.00000  & 200   & -14.51511 \\ \hline
 1.00000  & 500   & -15.29370 \\ \hline
 5.00000  & 10    & -14.68516 \\ \hline
 5.00000  & 50    & -14.06141 \\ \hline
5.00000  & 100   & -14.17007 \\ \hline
 5.00000  & 200   & -14.46313 \\ \hline
 5.00000  & 500   & -15.16306 \\ \hline
 10.00000 & 10    & -14.62365 \\ \hline
10.00000 & 50    & -14.14985 \\ \hline
10.00000 & 100   & -14.27446 \\ \hline
10.00000 & 200   & -14.72060 \\ \hline
 10.00000 & 500   & -15.51529 \\ \hline

\end{longtable}
\subsubsection{audio}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Nessun algoritmo di SLS applicato}
	\label{audio}\\
	\hline
	 alpha & minst & test\_ll  \\ \hline
0.5   & 10    & -91.14669 \\ \hline
 0.5   & 50    & -61.08795 \\ \hline
 0.5   & 100   & -50.85409 \\ \hline
 0.5   & 200   & -45.75492 \\ \hline
 0.5   & 500   & -43.02708 \\ \hline
 1     & 10    & -84.28958 \\ \hline
 1     & 50    & -58.36294 \\ \hline
 1     & 100   & -49.65196 \\ \hline
 1     & 200   & -45.32049 \\ \hline
1     & 500   & -42.92807 \\ \hline
5     & 10    & -68.27392 \\ \hline
 5     & 50    & -52.2251  \\ \hline
 5     & 100   & -47.01272 \\ \hline
 5     & 200   & -44.08191 \\ \hline
5     & 500   & -42.47781 \\ \hline
 10    & 10    & -61.89317 \\ \hline
 10    & 50    & -49.65934 \\ \hline
 10    & 100   & -45.71415 \\ \hline
 10    & 200   & -43.50611 \\ \hline
 10    & 500   & -42.3591  \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Nessun algoritmo di SLS applicato, rumore avente varianza di 0.1}
	\label{audio2}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -93.25399 \\ \hline
 0.5   & 50    & -66.79338 \\ \hline
 0.5   & 100   & -58.76185 \\ \hline
 0.5   & 200   & -50.47885 \\ \hline
 0.5   & 500   & -44.00786 \\ \hline
 1     & 10    & -84.91422 \\ \hline
 1     & 50    & -62.4118  \\ \hline
 1     & 100   & -56.09414 \\ \hline
 1     & 200   & -48.44995 \\ \hline
 1     & 500   & -44.09925 \\ \hline
 5     & 10    & -68.59961 \\ \hline
 5     & 50    & -54.88423 \\ \hline
 5     & 100   & -51.09852 \\ \hline
 5     & 200   & -46.0104  \\ \hline
 5     & 500   & -43.20257 \\ \hline
 10    & 10    & -61.19806 \\ \hline
10    & 50    & -52.37637 \\ \hline
10    & 100   & -48.66114 \\ \hline
10    & 200   & -44.36606 \\ \hline
10    & 500   & -43.23819 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Nessun algoritmo di SLS applicato, rumore avente varianza di 0.1}
	\label{audio3}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
0.5   & 10    & -73.62908 \\ \hline
 0.5   & 50    & -55.61807 \\ \hline
 0.5   & 100   & -48.19462 \\ \hline
 0.5   & 200   & -44.53124 \\ \hline
 0.5   & 500   & -43.40571 \\ \hline
 1     & 10    & -68.21539 \\ \hline
 1     & 50    & -52.50207 \\ \hline
 1     & 100   & -46.19494 \\ \hline
 1     & 200   & -43.96729 \\ \hline
1     & 500   & -43.24955 \\ \hline
 5     & 10    & -57.97143 \\ \hline
 5     & 50    & -48.83281 \\ \hline
 5     & 100   & -44.58327 \\ \hline
5     & 200   & -43.40442 \\ \hline
5     & 500   & -43.11888 \\ \hline
 10    & 10    & -53.18342 \\ \hline
 10    & 50    & -46.14378 \\ \hline
 10    & 100   & -44.01881 \\ \hline
 10    & 200   & -43.21577 \\ \hline
10    & 500   & -43.20435 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Nessun algoritmo di SLS applicato, rumore avente varianza di 1}
	\label{audio4}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -68.64829 \\ \hline
 0.5   & 50    & -50.47777 \\ \hline
 0.5   & 100   & -46.03299 \\ \hline
 0.5   & 200   & -44.05689 \\ \hline
0.5   & 500   & -43.38003 \\ \hline
 1     & 10    & -63.26593 \\ \hline
 1     & 50    & -49.21369 \\ \hline
 1     & 100   & -45.58622 \\ \hline
 1     & 200   & -43.60246 \\ \hline
 1     & 500   & -43.34447 \\ \hline
 5     & 10    & -54.78296 \\ \hline
5     & 50    & -46.55168 \\ \hline
5     & 100   & -44.09825 \\ \hline
 5     & 200   & -43.12464 \\ \hline
 5     & 500   & -43.20069 \\ \hline
 10    & 10    & -51.49676 \\ \hline
10    & 50    & -45.44291 \\ \hline
 10    & 100   & -43.82776 \\ \hline
 10    & 200   & -42.99448 \\ \hline
 10    & 500   & -43.06078 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Nessun algoritmo di SLS applicato, rumore avente varianza di 2}
	\label{audio5}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -64.28529 \\ \hline
 0.5   & 50    & -49.24767 \\ \hline
 0.5   & 100   & -45.48895 \\ \hline
 0.5   & 200   & -43.7569  \\ \hline
 0.5   & 500   & -43.4345  \\ \hline
 1     & 10    & -61.80687 \\ \hline
 1     & 50    & -48.73591 \\ \hline
 1     & 100   & -45.33313 \\ \hline
 1     & 200   & -43.72603 \\ \hline
 1     & 500   & -43.41807 \\ \hline
 5     & 10    & -53.5221  \\ \hline
 5     & 50    & -46.17549 \\ \hline
 5     & 100   & -43.938   \\ \hline
 5     & 200   & -43.05493 \\ \hline
 5     & 500   & -43.09917 \\ \hline
10    & 10    & -50.62946 \\ \hline
10    & 50    & -45.01344 \\ \hline
10    & 100   & -43.67545 \\ \hline
10    & 200   & -43.11812 \\ \hline
10    & 500   & -43.22579 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Iterative Improvement senza rumore}
	\label{audioii}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
0.5   & 10    & -86.53359 \\ \hline
0.5   & 50    & -61.07992 \\ \hline
 0.5   & 100   & -50.85409 \\ \hline
 0.5   & 200   & -45.75492 \\ \hline
 0.5   & 500   & -43.02708 \\ \hline
 1     & 10    & -80.3469  \\ \hline
1     & 50    & -58.35761 \\ \hline
1     & 100   & -49.65196 \\ \hline
1     & 200   & -45.32049 \\ \hline
 1     & 500   & -42.92807 \\ \hline
 5     & 10    & -65.87959 \\ \hline
 5     & 50    & -52.22314 \\ \hline
 5     & 100   & -47.01272 \\ \hline
 5     & 200   & -44.08191 \\ \hline
 5     & 500   & -42.47781 \\ \hline
 10    & 10    & -60.17661 \\ \hline
 10    & 50    & -49.65631 \\ \hline
 10    & 100   & -45.71415 \\ \hline
 10    & 200   & -43.50611 \\ \hline
10    & 500   & -42.3591  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Iterative Improvement con rumore avente varianza di 0.1}
	\label{audioii2}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -88.93752 \\ \hline
 0.5   & 50    & -66.79422 \\ \hline
 0.5   & 100   & -58.76185 \\ \hline
 0.5   & 200   & -50.47885 \\ \hline
 0.5   & 500   & -44.00786 \\ \hline
 1     & 10    & -81.29596 \\ \hline
 1     & 50    & -62.40237 \\ \hline
 1     & 100   & -56.09414 \\ \hline
 1     & 200   & -48.44995 \\ \hline
 1     & 500   & -44.09925 \\ \hline
 5     & 10    & -66.51819 \\ \hline
 5     & 50    & -54.88106 \\ \hline
5     & 100   & -51.09852 \\ \hline
5     & 200   & -46.0104  \\ \hline
5     & 500   & -43.20257 \\ \hline
10    & 10    & -59.71859 \\ \hline
10    & 50    & -52.3738  \\ \hline
10    & 100   & -48.66114 \\ \hline
10    & 200   & -44.36606 \\ \hline
10    & 500   & -43.23819 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Iterative Improvement con rumore avente varianza di 0.5}
	\label{audioii3}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
0.5   & 10    & -72.40827 \\ \hline
0.5   & 50    & -55.5884  \\ \hline
 0.5   & 100   & -48.19462 \\ \hline
0.5   & 200   & -44.53124 \\ \hline
0.5   & 500   & -43.40571 \\ \hline
 1     & 10    & -67.02668 \\ \hline
 1     & 50    & -52.49503 \\ \hline
 1     & 100   & -46.19494 \\ \hline
 1     & 200   & -43.96729 \\ \hline
 1     & 500   & -43.24955 \\ \hline
 5     & 10    & -57.41615 \\ \hline
 5     & 50    & -48.82937 \\ \hline
 5     & 100   & -44.58327 \\ \hline
 5     & 200   & -43.40442 \\ \hline
 5     & 500   & -43.11888 \\ \hline
 10    & 10    & -52.69957 \\ \hline
 10    & 50    & -46.14339 \\ \hline
 10    & 100   & -44.01881 \\ \hline
 10    & 200   & -43.21577 \\ \hline
 10    & 500   & -43.20435 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Iterative Improvement con rumore avente varianza di 1}
	\label{audioii4}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -67.21437 \\ \hline
 0.5   & 50    & -50.47272 \\ \hline
 0.5   & 100   & -46.03299 \\ \hline
 0.5   & 200   & -44.05689 \\ \hline
 0.5   & 500   & -43.38003 \\ \hline
 1     & 10    & -61.93876 \\ \hline
1     & 50    & -49.20654 \\ \hline
1     & 100   & -45.58622 \\ \hline
 1     & 200   & -43.60246 \\ \hline
 1     & 500   & -43.34447 \\ \hline
 5     & 10    & -54.09819 \\ \hline
 5     & 50    & -46.54914 \\ \hline
 5     & 100   & -44.09825 \\ \hline
 5     & 200   & -43.12464 \\ \hline
 5     & 500   & -43.20069 \\ \hline
 10    & 10    & -50.94421 \\ \hline
 10    & 50    & -45.43967 \\ \hline
10    & 100   & -43.82776 \\ \hline
10    & 200   & -42.99448 \\ \hline
 10    & 500   & -43.06078 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Iterative Improvement con rumore avente varianza di 2}
	\label{audioii5}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -62.95375 \\ \hline
 0.5   & 50    & -49.23908 \\ \hline
 0.5   & 100   & -45.48895 \\ \hline
 0.5   & 200   & -43.7569  \\ \hline
 0.5   & 500   & -43.4345  \\ \hline
 1     & 10    & -60.6537  \\ \hline
 1     & 50    & -48.7289  \\ \hline
 1     & 100   & -45.33313 \\ \hline
 1     & 200   & -43.72603 \\ \hline
 1     & 500   & -43.41807 \\ \hline
 5     & 10    & -52.84854 \\ \hline
 5     & 50    & -46.17535 \\ \hline
 5     & 100   & -43.938   \\ \hline
 5     & 200   & -43.05493 \\ \hline
 5     & 500   & -43.09917 \\ \hline
 10    & 10    & -50.07419 \\ \hline
 10    & 50    & -45.01224 \\ \hline
10    & 100   & -43.67545 \\ \hline
 10    & 200   & -43.11812 \\ \hline
 10    & 500   & -43.22579 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Randomised Iterative Improvement senza rumore, con probabilità di 0.6, times=10}
	\label{audiorii}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -89.49295 \\ \hline
 0.50000  & 50    & -61.08580 \\ \hline
 0.50000  & 100   & -50.85409 \\ \hline
 0.50000  & 200   & -45.75492 \\ \hline
 0.50000  & 500   & -43.02708 \\ \hline
 1.00000  & 10    & -82.90760 \\ \hline
 1.00000  & 50    & -58.36284 \\ \hline
 1.00000  & 100   & -49.65196 \\ \hline
 1.00000  & 200   & -45.32049 \\ \hline
 1.00000  & 500   & -42.92807 \\ \hline
 5.00000  & 10    & -67.43674 \\ \hline
 5.00000  & 50    & -52.22478 \\ \hline
 5.00000  & 100   & -47.01272 \\ \hline
 5.00000  & 200   & -44.08191 \\ \hline
 5.00000  & 500   & -42.47781 \\ \hline
 10.00000 & 10    & -61.27329 \\ \hline
 10.00000 & 50    & -49.65832 \\ \hline
 10.00000 & 100   & -45.71415 \\ \hline
 10.00000 & 200   & -43.50611 \\ \hline
 10.00000 & 500   & -42.35910 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Randomised Iterative Improvement con rumore avente varianza di 0.1, con probabilità di 0.6, times=10}
	\label{audiorii2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -91.63904 \\ \hline
 0.50000  & 50    & -66.79385 \\ \hline
 0.50000  & 100   & -58.76185 \\ \hline
 0.50000  & 200   & -50.47885 \\ \hline
 0.50000  & 500   & -44.00786 \\ \hline
1.00000  & 10    & -83.53287 \\ \hline
1.00000  & 50    & -62.41378 \\ \hline
1.00000  & 100   & -56.09414 \\ \hline
1.00000  & 200   & -48.44995 \\ \hline
 1.00000  & 500   & -44.09925 \\ \hline
 5.00000  & 10    & -67.77280 \\ \hline
 5.00000  & 50    & -54.88209 \\ \hline
 5.00000  & 100   & -51.09852 \\ \hline
5.00000  & 200   & -46.01040 \\ \hline
 5.00000  & 500   & -43.20257 \\ \hline
 10.00000 & 10    & -60.63103 \\ \hline
 10.00000 & 50    & -52.37320 \\ \hline
 10.00000 & 100   & -48.66114 \\ \hline
 10.00000 & 200   & -44.36606 \\ \hline
 10.00000 & 500   & -43.23819 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Randomised Iterative Improvement con rumore avente varianza di 0.5, con probabilità di 0.6, times=10}
	\label{audiorii3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -73.17010 \\ \hline
 0.50000  & 50    & -55.60261 \\ \hline
 0.50000  & 100   & -48.19462 \\ \hline
 0.50000  & 200   & -44.53124 \\ \hline
 0.50000  & 500   & -43.40571 \\ \hline
 1.00000  & 10    & -67.72980 \\ \hline
 1.00000  & 50    & -52.49980 \\ \hline
 1.00000  & 100   & -46.19494 \\ \hline
 1.00000  & 200   & -43.96729 \\ \hline
 1.00000  & 500   & -43.24955 \\ \hline
 5.00000  & 10    & -57.74201 \\ \hline
 5.00000  & 50    & -48.82967 \\ \hline
 5.00000  & 100   & -44.58327 \\ \hline
5.00000  & 200   & -43.40442 \\ \hline
 5.00000  & 500   & -43.11888 \\ \hline
 10.00000 & 10    & -52.95476 \\ \hline
 10.00000 & 50    & -46.14351 \\ \hline
10.00000 & 100   & -44.01881 \\ \hline
 10.00000 & 200   & -43.21577 \\ \hline
10.00000 & 500   & -43.20435 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Randomised Iterative Improvement con rumore avente varianza di 1, con probabilità di 0.6, times=10}
	\label{audiorii4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -68.08467 \\ \hline
 0.50000  & 50    & -50.47476 \\ \hline
 0.50000  & 100   & -46.03299 \\ \hline
 0.50000  & 200   & -44.05689 \\ \hline
 0.50000  & 500   & -43.38003 \\ \hline
 1.00000  & 10    & -62.73354 \\ \hline
1.00000  & 50    & -49.21161 \\ \hline
1.00000  & 100   & -45.58622 \\ \hline
1.00000  & 200   & -43.60246 \\ \hline
 1.00000  & 500   & -43.34447 \\ \hline
 5.00000  & 10    & -54.50358 \\ \hline
5.00000  & 50    & -46.54870 \\ \hline
 5.00000  & 100   & -44.09825 \\ \hline
 5.00000  & 200   & -43.12464 \\ \hline
5.00000  & 500   & -43.20069 \\ \hline
10.00000 & 10    & -51.24390 \\ \hline
10.00000 & 50    & -45.44091 \\ \hline
10.00000 & 100   & -43.82776 \\ \hline
 10.00000 & 200   & -42.99448 \\ \hline
 10.00000 & 500   & -43.06078 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. Randomised Iterative Improvement con rumore avente varianza di 2, con probabilità di 0.6, times=10}
	\label{audiorii4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -63.81357 \\ \hline
0.50000  & 50    & -49.24532 \\ \hline
 0.50000  & 100   & -45.48895 \\ \hline
 0.50000  & 200   & -43.75690 \\ \hline
 0.50000  & 500   & -43.43450 \\ \hline
 1.00000  & 10    & -61.38592 \\ \hline
1.00000  & 50    & -48.73387 \\ \hline
1.00000  & 100   & -45.33313 \\ \hline
1.00000  & 200   & -43.72603 \\ \hline
1.00000  & 500   & -43.41807 \\ \hline
 5.00000  & 10    & -53.25954 \\ \hline
 5.00000  & 50    & -46.17514 \\ \hline
 5.00000  & 100   & -43.93800 \\ \hline
 5.00000  & 200   & -43.05493 \\ \hline
5.00000  & 500   & -43.09917 \\ \hline
10.00000 & 10    & -50.37972 \\ \hline
10.00000 & 50    & -45.01229 \\ \hline
10.00000 & 100   & -43.67545 \\ \hline
10.00000 & 200   & -43.11812 \\ \hline
10.00000 & 500   & -43.22579 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. GRASP senza rumore, k=2 e times=5}
	\label{audiograsp}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -86.53359 \\ \hline
0.50000  & 50    & -61.07992 \\ \hline
 0.50000  & 100   & -50.85409 \\ \hline
 0.50000  & 200   & -45.75492 \\ \hline
0.50000  & 500   & -43.02708 \\ \hline
1.00000  & 10    & -80.34690 \\ \hline
1.00000  & 50    & -58.35761 \\ \hline
 1.00000  & 100   & -49.65196 \\ \hline
 1.00000  & 200   & -45.32049 \\ \hline
 1.00000  & 500   & -42.92807 \\ \hline
 5.00000  & 10    & -65.87959 \\ \hline
5.00000  & 50    & -52.22314 \\ \hline
5.00000  & 100   & -47.01272 \\ \hline
5.00000  & 200   & -44.08191 \\ \hline
 5.00000  & 500   & -42.47781 \\ \hline
 10.00000 & 10    & -60.17661 \\ \hline
10.00000 & 50    & -49.65631 \\ \hline
10.00000 & 100   & -45.71415 \\ \hline
 10.00000 & 200   & -43.50611 \\ \hline
 10.00000 & 500   & -42.35910 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. GRASP con rumore avente varianza di 0.1, k=2 e times=5}
	\label{audiograsp2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -89.24376 \\ \hline
 0.50000  & 50    & -65.86100 \\ \hline
 0.50000  & 100   & -59.61444 \\ \hline
 0.50000  & 200   & -49.98266 \\ \hline
 0.50000  & 500   & -43.54181 \\ \hline
1.00000  & 10    & -81.61063 \\ \hline
 1.00000  & 50    & -63.14966 \\ \hline
 1.00000  & 100   & -56.47197 \\ \hline
 1.00000  & 200   & -48.11819 \\ \hline
 1.00000  & 500   & -43.76387 \\ \hline
 5.00000  & 10    & -66.54191 \\ \hline
 5.00000  & 50    & -54.79697 \\ \hline
5.00000  & 100   & -50.14578 \\ \hline
5.00000  & 200   & -45.66688 \\ \hline
5.00000  & 500   & -43.27520 \\ \hline
 10.00000 & 10    & -60.79241 \\ \hline
 10.00000 & 50    & -52.07687 \\ \hline
 10.00000 & 100   & -48.53165 \\ \hline
10.00000 & 200   & -45.19603 \\ \hline
10.00000 & 500   & -43.26450 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. GRASP con rumore avente varianza di 0.5, k=2 e times=5}
	\label{audiograsp3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -71.71523 \\ \hline
 0.50000  & 50    & -55.95491 \\ \hline
 0.50000  & 100   & -47.02566 \\ \hline
 0.50000  & 200   & -44.37196 \\ \hline
 0.50000  & 500   & -43.49786 \\ \hline
 1.00000  & 10    & -68.11237 \\ \hline
 1.00000  & 50    & -53.76134 \\ \hline
 1.00000  & 100   & -46.53680 \\ \hline
 1.00000  & 200   & -44.11199 \\ \hline
 1.00000  & 500   & -43.36671 \\ \hline
 5.00000  & 10    & -57.14880 \\ \hline
 5.00000  & 50    & -48.26350 \\ \hline
 5.00000  & 100   & -44.64247 \\ \hline
 5.00000  & 200   & -43.44105 \\ \hline
5.00000  & 500   & -43.13550 \\ \hline
10.00000 & 10    & -53.01980 \\ \hline
10.00000 & 50    & -46.68603 \\ \hline
10.00000 & 100   & -44.00530 \\ \hline
10.00000 & 200   & -43.13181 \\ \hline
10.00000 & 500   & -43.14111 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. GRASP con rumore avente varianza di 1, k=2 e times=5}
	\label{audiograsp4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -65.83947 \\ \hline
0.50000  & 50    & -50.91896 \\ \hline
0.50000  & 100   & -46.05856 \\ \hline
 0.50000  & 200   & -43.82429 \\ \hline
 0.50000  & 500   & -43.43185 \\ \hline
 1.00000  & 10    & -62.08980 \\ \hline
 1.00000  & 50    & -49.40018 \\ \hline
 1.00000  & 100   & -45.65623 \\ \hline
1.00000  & 200   & -43.96511 \\ \hline
 1.00000  & 500   & -43.25371 \\ \hline
 5.00000  & 10    & -54.13160 \\ \hline
 5.00000  & 50    & -46.51167 \\ \hline
 5.00000  & 100   & -44.02634 \\ \hline
 5.00000  & 200   & -43.26453 \\ \hline
 5.00000  & 500   & -43.16043 \\ \hline
10.00000 & 10    & -51.08469 \\ \hline
10.00000 & 50    & -45.22951 \\ \hline
10.00000 & 100   & -43.82062 \\ \hline
10.00000 & 200   & -43.10559 \\ \hline
10.00000 & 500   & -43.09334 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: audio. GRASP con rumore avente varianza di 2, k=2 e times=5}
	\label{audiograsp5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -63.87992 \\ \hline
 0.50000  & 50    & -49.57349 \\ \hline
 0.50000  & 100   & -45.50533 \\ \hline
 0.50000  & 200   & -43.93398 \\ \hline
 0.50000  & 500   & -43.28222 \\ \hline
 1.00000  & 10    & -60.32249 \\ \hline
 1.00000  & 50    & -48.70213 \\ \hline
 1.00000  & 100   & -45.39570 \\ \hline
 1.00000  & 200   & -43.84778 \\ \hline
 1.00000  & 500   & -43.31314 \\ \hline
5.00000  & 10    & -53.38256 \\ \hline
5.00000  & 50    & -46.01039 \\ \hline
5.00000  & 100   & -44.19040 \\ \hline
 5.00000  & 200   & -43.17796 \\ \hline
 5.00000  & 500   & -43.24012 \\ \hline
 10.00000 & 10    & -50.47683 \\ \hline
 10.00000 & 50    & -44.92644 \\ \hline
 10.00000 & 100   & -43.74475 \\ \hline
 10.00000 & 200   & -43.24427 \\ \hline
 10.00000 & 500   & -43.22367 \\ \hline
\end{longtable}
\subsubsection{jester}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Nessun algoritmo di SLS applicato}
	\label{jester}\\
	\hline
alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -125.47147 \\ \hline
0.50000  & 50    & -77.60039  \\ \hline
0.50000  & 100   & -64.07435  \\ \hline
 0.50000  & 200   & -58.14350  \\ \hline
 0.50000  & 500   & -55.66913  \\ \hline
 1.00000  & 10    & -115.37348 \\ \hline
 1.00000  & 50    & -74.78563  \\ \hline
 1.00000  & 100   & -63.12024  \\ \hline
 1.00000  & 200   & -57.88698  \\ \hline
 1.00000  & 500   & -55.63026  \\ \hline
 5.00000  & 10    & -92.69368  \\ \hline
 5.00000  & 50    & -67.95320  \\ \hline
5.00000  & 100   & -60.18760  \\ \hline
5.00000  & 200   & -57.07277  \\ \hline
 5.00000  & 500   & -55.51808  \\ \hline
 10.00000 & 10    & -82.25151  \\ \hline
 10.00000 & 50    & -64.53820  \\ \hline
 10.00000 & 100   & -59.55639  \\ \hline
10.00000 & 200   & -56.68866  \\ \hline
10.00000 & 500   & -55.36343  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Nessun algoritmo di SLS applicato, con rumore avente varianza di 0.1}
	\label{jester2}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -123.96596 \\ \hline
 0.50000  & 50    & -84.74470  \\ \hline
 0.50000  & 100   & -74.23206  \\ \hline
 0.50000  & 200   & -60.87688  \\ \hline
 0.50000  & 500   & -56.48170  \\ \hline
 1.00000  & 10    & -111.48674 \\ \hline
 1.00000  & 50    & -81.91541  \\ \hline
 1.00000  & 100   & -70.38324  \\ \hline
 1.00000  & 200   & -60.52336  \\ \hline
 1.00000  & 500   & -56.53947  \\ \hline
 5.00000  & 10    & -90.29219  \\ \hline
 5.00000  & 50    & -72.25335  \\ \hline
 5.00000  & 100   & -65.10541  \\ \hline
 5.00000  & 200   & -58.63544  \\ \hline
 5.00000  & 500   & -56.55605  \\ \hline
 10.00000 & 10    & -80.16234  \\ \hline
10.00000 & 50    & -68.64548  \\ \hline
10.00000 & 100   & -63.17137  \\ \hline
 10.00000 & 200   & -57.18403  \\ \hline
 10.00000 & 500   & -56.30876  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Nessun algoritmo di SLS applicato, con rumore avente varianza di 0.5}
	\label{jester3}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -95.45710  \\ \hline
 0.50000  & 50    & -70.89194  \\ \hline
 0.50000  & 100   & -59.63240  \\ \hline
 0.50000  & 200   & -56.55942  \\ \hline
 0.50000  & 500   & -56.47966  \\ \hline
 1.00000  & 10    & -88.33916  \\ \hline
 1.00000  & 50    & -68.73705  \\ \hline
 1.00000  & 100   & -58.84856  \\ \hline
 1.00000  & 200   & -56.44747  \\ \hline
 1.00000  & 500   & -56.42325  \\ \hline
 5.00000  & 10    & -74.82425  \\ \hline
 5.00000  & 50    & -63.30852  \\ \hline
5.00000  & 100   & -57.67745  \\ \hline
 5.00000  & 200   & -56.27756  \\ \hline
 5.00000  & 500   & -56.31981  \\ \hline
 10.00000 & 10    & -68.64321  \\ \hline
 10.00000 & 50    & -60.80596  \\ \hline
 10.00000 & 100   & -57.04721  \\ \hline
 10.00000 & 200   & -56.18231  \\ \hline
 10.00000 & 500   & -56.31387  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Nessun algoritmo di SLS applicato, con rumore avente varianza di 1}
	\label{jester4}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -86.95881  \\ \hline
 0.50000  & 50    & -64.29676  \\ \hline
 0.50000  & 100   & -58.11172  \\ \hline
 0.50000  & 200   & -56.38537  \\ \hline
 0.50000  & 500   & -56.31009  \\ \hline
 1.00000  & 10    & -82.26095  \\ \hline
 1.00000  & 50    & -63.08243  \\ \hline
 1.00000  & 100   & -57.49093  \\ \hline
 1.00000  & 200   & -56.43339  \\ \hline
 1.00000  & 500   & -56.46230  \\ \hline
 5.00000  & 10    & -71.09183  \\ \hline
5.00000  & 50    & -59.39682  \\ \hline
 5.00000  & 100   & -57.01868  \\ \hline
 5.00000  & 200   & -56.25458  \\ \hline
 5.00000  & 500   & -56.32458  \\ \hline
 10.00000 & 10    & -67.01140  \\ \hline
 10.00000 & 50    & -58.71985  \\ \hline
 10.00000 & 100   & -56.75743  \\ \hline
 10.00000 & 200   & -56.17670  \\ \hline
10.00000 & 500   & -56.35394  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Nessun algoritmo di SLS applicato, con rumore avente varianza di 2}
	\label{jester6}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -84.54932  \\ \hline
0.50000  & 50    & -62.71570  \\ \hline
 0.50000  & 100   & -57.72094  \\ \hline
 0.50000  & 200   & -56.43870  \\ \hline
 0.50000  & 500   & -56.38441  \\ \hline
1.00000  & 10    & -79.28506  \\ \hline
 1.00000  & 50    & -61.56495  \\ \hline
 1.00000  & 100   & -57.42628  \\ \hline
 1.00000  & 200   & -56.36929  \\ \hline
 1.00000  & 500   & -56.37674  \\ \hline
 5.00000  & 10    & -69.47289  \\ \hline
5.00000  & 50    & -59.71290  \\ \hline
5.00000  & 100   & -56.95557  \\ \hline
 5.00000  & 200   & -56.18646  \\ \hline
 5.00000  & 500   & -56.49626  \\ \hline
 10.00000 & 10    & -65.84738  \\ \hline
 10.00000 & 50    & -58.50574  \\ \hline
 10.00000 & 100   & -56.68976  \\ \hline
 10.00000 & 200   & -56.24749  \\ \hline
 10.00000 & 500   & -56.44242  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Iterative Improvement senza rumore}
	\label{jesterii}\\
	\hline
 alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -116.32133 \\ \hline
0.50000  & 50    & -77.56757  \\ \hline
 0.50000  & 100   & -64.07435  \\ \hline
 0.50000  & 200   & -58.14350  \\ \hline
 0.50000  & 500   & -55.66913  \\ \hline
 1.00000  & 10    & -107.46736 \\ \hline
 1.00000  & 50    & -74.76065  \\ \hline
 1.00000  & 100   & -63.12024  \\ \hline
 1.00000  & 200   & -57.88698  \\ \hline
 1.00000  & 500   & -55.63026  \\ \hline
 5.00000  & 10    & -87.68040  \\ \hline
5.00000  & 50    & -67.93596  \\ \hline
 5.00000  & 100   & -60.18760  \\ \hline
 5.00000  & 200   & -57.07277  \\ \hline
 5.00000  & 500   & -55.51808  \\ \hline
 10.00000 & 10    & -78.75370  \\ \hline
 10.00000 & 50    & -64.50606  \\ \hline
 10.00000 & 100   & -59.55639  \\ \hline
 10.00000 & 200   & -56.68866  \\ \hline
 10.00000 & 500   & -55.36343  \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Iterative Improvement con rumore avente varianza di 0.1}
	\label{jesterii2}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -116.67330 \\ \hline
 0.50000  & 50    & -84.69322  \\ \hline
 0.50000  & 100   & -74.23084  \\ \hline
 0.50000  & 200   & -60.87688  \\ \hline
0.50000  & 500   & -56.48170  \\ \hline
 1.00000  & 10    & -104.89657 \\ \hline
1.00000  & 50    & -81.89254  \\ \hline
 1.00000  & 100   & -70.38287  \\ \hline
 1.00000  & 200   & -60.52336  \\ \hline
 1.00000  & 500   & -56.53947  \\ \hline
5.00000  & 10    & -86.64506  \\ \hline
 5.00000  & 50    & -72.24410  \\ \hline
 5.00000  & 100   & -65.10541  \\ \hline
 5.00000  & 200   & -58.63544  \\ \hline
 5.00000  & 500   & -56.55605  \\ \hline
 10.00000 & 10    & -77.73502  \\ \hline
 10.00000 & 50    & -68.63968  \\ \hline
 10.00000 & 100   & -63.17137  \\ \hline
10.00000 & 200   & -57.18403  \\ \hline
 10.00000 & 500   & -56.30876  \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Iterative Improvement con rumore avente varianza di 0.5}
	\label{jesterii3}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -93.11098  \\ \hline
0.50000  & 50    & -70.87476  \\ \hline
0.50000  & 100   & -59.63240  \\ \hline
 0.50000  & 200   & -56.55942  \\ \hline
 0.50000  & 500   & -56.47966  \\ \hline
1.00000  & 10    & -86.60999  \\ \hline
1.00000  & 50    & -68.72542  \\ \hline
1.00000  & 100   & -58.84856  \\ \hline
 1.00000  & 200   & -56.44747  \\ \hline
 1.00000  & 500   & -56.42325  \\ \hline
 5.00000  & 10    & -73.82940  \\ \hline
 5.00000  & 50    & -63.30679  \\ \hline
 5.00000  & 100   & -57.67745  \\ \hline
 5.00000  & 200   & -56.27756  \\ \hline
 5.00000  & 500   & -56.31981  \\ \hline
 10.00000 & 10    & -67.80168  \\ \hline
 10.00000 & 50    & -60.80318  \\ \hline
10.00000 & 100   & -57.04721  \\ \hline
10.00000 & 200   & -56.18231  \\ \hline
10.00000 & 500   & -56.31387  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Iterative Improvement con rumore avente varianza di 1}
	\label{jesterii4}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -84.48067  \\ \hline
 0.50000  & 50    & -64.27972  \\ \hline
 0.50000  & 100   & -58.11172  \\ \hline
0.50000  & 200   & -56.38537  \\ \hline
0.50000  & 500   & -56.31009  \\ \hline
1.00000  & 10    & -80.36819  \\ \hline
1.00000  & 50    & -63.07313  \\ \hline
1.00000  & 100   & -57.49093  \\ \hline
1.00000  & 200   & -56.43339  \\ \hline
1.00000  & 500   & -56.46230  \\ \hline
 5.00000  & 10    & -70.09661  \\ \hline
 5.00000  & 50    & -59.39682  \\ \hline
 5.00000  & 100   & -57.01868  \\ \hline
 5.00000  & 200   & -56.25458  \\ \hline
 5.00000  & 500   & -56.32458  \\ \hline
 10.00000 & 10    & -66.19346  \\ \hline
 10.00000 & 50    & -58.71940  \\ \hline
 10.00000 & 100   & -56.75743  \\ \hline
10.00000 & 200   & -56.17670  \\ \hline
10.00000 & 500   & -56.35394  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Iterative Improvement con rumore avente varianza di 2}
	\label{jesterii5}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -82.26377  \\ \hline
0.50000  & 50    & -62.71403  \\ \hline
 0.50000  & 100   & -57.72094  \\ \hline
 0.50000  & 200   & -56.43870  \\ \hline
 0.50000  & 500   & -56.38441  \\ \hline
 1.00000  & 10    & -77.16463  \\ \hline
 1.00000  & 50    & -61.55057  \\ \hline
 1.00000  & 100   & -57.42628  \\ \hline
 1.00000  & 200   & -56.36929  \\ \hline
 1.00000  & 500   & -56.37674  \\ \hline
 5.00000  & 10    & -68.37132  \\ \hline
 5.00000  & 50    & -59.70823  \\ \hline
 5.00000  & 100   & -56.95557  \\ \hline
 5.00000  & 200   & -56.18646  \\ \hline
 5.00000  & 500   & -56.49626  \\ \hline
 10.00000 & 10    & -65.03417  \\ \hline
 10.00000 & 50    & -58.50098  \\ \hline
 10.00000 & 100   & -56.68976  \\ \hline
 10.00000 & 200   & -56.24749  \\ \hline
 10.00000 & 500   & -56.44242  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Randomised Iterative Improvement senza rumore, probabilità di 0.6 e times=10}
	\label{jesterrii}\\
	\hline
alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -123.20239 \\ \hline
 0.50000  & 50    & -77.58687  \\ \hline
 0.50000  & 100   & -64.07435  \\ \hline
 0.50000  & 200   & -58.14350  \\ \hline
 0.50000  & 500   & -55.66913  \\ \hline
 1.00000  & 10    & -113.48656 \\ \hline
 1.00000  & 50    & -74.77720  \\ \hline
 1.00000  & 100   & -63.12024  \\ \hline
 1.00000  & 200   & -57.88698  \\ \hline
1.00000  & 500   & -55.63026  \\ \hline
5.00000  & 10    & -91.41547  \\ \hline
 5.00000  & 50    & -67.95082  \\ \hline
 5.00000  & 100   & -60.18760  \\ \hline
 5.00000  & 200   & -57.07277  \\ \hline
 5.00000  & 500   & -55.51808  \\ \hline
 10.00000 & 10    & -81.39364  \\ \hline
 10.00000 & 50    & -64.53512  \\ \hline
 10.00000 & 100   & -59.55639  \\ \hline
 10.00000 & 200   & -56.68866  \\ \hline
 10.00000 & 500   & -55.36343  \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Randomised Iterative Improvement con rumore avente varianza di 0.1, probabilità di 0.6 e times=10}
	\label{jesterrii2}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -121.74964 \\ \hline
0.50000  & 50    & -84.73066  \\ \hline
 0.50000  & 100   & -74.23188  \\ \hline
 0.50000  & 200   & -60.87688  \\ \hline
 0.50000  & 500   & -56.48170  \\ \hline
 1.00000  & 10    & -109.55391 \\ \hline
1.00000  & 50    & -81.91289  \\ \hline
 1.00000  & 100   & -70.38241  \\ \hline
 1.00000  & 200   & -60.52336  \\ \hline
 1.00000  & 500   & -56.53947  \\ \hline
 5.00000  & 10    & -89.16471  \\ \hline
 5.00000  & 50    & -72.25139  \\ \hline
 5.00000  & 100   & -65.10541  \\ \hline
 5.00000  & 200   & -58.63544  \\ \hline
5.00000  & 500   & -56.55605  \\ \hline
10.00000 & 10    & -79.34948  \\ \hline
10.00000 & 50    & -68.64256  \\ \hline
 10.00000 & 100   & -63.17137  \\ \hline
 10.00000 & 200   & -57.18403  \\ \hline
 10.00000 & 500   & -56.30876  \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Randomised Iterative Improvement con rumore avente varianza di 0.5, probabilità di 0.6 e times=10}
	\label{jesterrii3}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -94.61148  \\ \hline
 0.50000  & 50    & -70.88654  \\ \hline
 0.50000  & 100   & -59.63240  \\ \hline
 0.50000  & 200   & -56.55942  \\ \hline
 0.50000  & 500   & -56.47966  \\ \hline
 1.00000  & 10    & -87.64976  \\ \hline
 1.00000  & 50    & -68.73406  \\ \hline
 1.00000  & 100   & -58.84856  \\ \hline
 1.00000  & 200   & -56.44747  \\ \hline
 1.00000  & 500   & -56.42325  \\ \hline
 5.00000  & 10    & -74.39909  \\ \hline
 5.00000  & 50    & -63.30869  \\ \hline
 5.00000  & 100   & -57.67745  \\ \hline
 5.00000  & 200   & -56.27756  \\ \hline
 5.00000  & 500   & -56.31981  \\ \hline
 10.00000 & 10    & -68.27235  \\ \hline
 10.00000 & 50    & -60.80655  \\ \hline
 10.00000 & 100   & -57.04721  \\ \hline
 10.00000 & 200   & -56.18231  \\ \hline
10.00000 & 500   & -56.31387  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Randomised Iterative Improvement con rumore avente varianza di 1, probabilità di 0.6 e times=10}
	\label{jesterrii4}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -86.05713  \\ \hline
 0.50000  & 50    & -64.28896  \\ \hline
 0.50000  & 100   & -58.11172  \\ \hline
 0.50000  & 200   & -56.38537  \\ \hline
0.50000  & 500   & -56.31009  \\ \hline
1.00000  & 10    & -81.53795  \\ \hline
 1.00000  & 50    & -63.08068  \\ \hline
 1.00000  & 100   & -57.49093  \\ \hline
 1.00000  & 200   & -56.43339  \\ \hline
 1.00000  & 500   & -56.46230  \\ \hline
 5.00000  & 10    & -70.68207  \\ \hline
 5.00000  & 50    & -59.39682  \\ \hline
 5.00000  & 100   & -57.01868  \\ \hline
 5.00000  & 200   & -56.25458  \\ \hline
 5.00000  & 500   & -56.32458  \\ \hline
10.00000 & 10    & -66.66028  \\ \hline
10.00000 & 50    & -58.71972  \\ \hline
 10.00000 & 100   & -56.75743  \\ \hline
 10.00000 & 200   & -56.17670  \\ \hline
10.00000 & 500   & -56.35394  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. Randomised Iterative Improvement con rumore avente varianza di 2, probabilità di 0.6 e times=10}
	\label{jesterrii5}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -83.75635  \\ \hline
 0.50000  & 50    & -62.71507  \\ \hline
 0.50000  & 100   & -57.72094  \\ \hline
 0.50000  & 200   & -56.43870  \\ \hline
 0.50000  & 500   & -56.38441  \\ \hline
1.00000  & 10    & -78.46292  \\ \hline
1.00000  & 50    & -61.56375  \\ \hline
1.00000  & 100   & -57.42628  \\ \hline
1.00000  & 200   & -56.36929  \\ \hline
1.00000  & 500   & -56.37674  \\ \hline
5.00000  & 10    & -69.01856  \\ \hline
5.00000  & 50    & -59.71177  \\ \hline
5.00000  & 100   & -56.95557  \\ \hline
5.00000  & 200   & -56.18646  \\ \hline
5.00000  & 500   & -56.49626  \\ \hline
10.00000 & 10    & -65.50718  \\ \hline
10.00000 & 50    & -58.50331  \\ \hline
10.00000 & 100   & -56.68976  \\ \hline
10.00000 & 200   & -56.24749  \\ \hline
 10.00000 & 500   & -56.44242  \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. GRASP senza rumore, k=2 e times=3} 
	\label{jestergrasp}\\
	\hline
 alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -116.32133 \\ \hline
0.50000  & 50    & -77.56757  \\ \hline
0.50000  & 100   & -64.07435  \\ \hline
0.50000  & 200   & -58.14350  \\ \hline
 0.50000  & 500   & -55.66913  \\ \hline
 1.00000  & 10    & -107.46736 \\ \hline
 1.00000  & 50    & -74.76065  \\ \hline
 1.00000  & 100   & -63.12024  \\ \hline
1.00000  & 200   & -57.88698  \\ \hline
 1.00000  & 500   & -55.63026  \\ \hline
 5.00000  & 10    & -87.68040  \\ \hline
 5.00000  & 50    & -67.93596  \\ \hline
 5.00000  & 100   & -60.18760  \\ \hline
 5.00000  & 200   & -57.07277  \\ \hline
 5.00000  & 500   & -55.51808  \\ \hline
 10.00000 & 10    & -78.75370  \\ \hline
10.00000 & 50    & -64.50606  \\ \hline
 10.00000 & 100   & -59.55639  \\ \hline
 10.00000 & 200   & -56.68866  \\ \hline
10.00000 & 500   & -55.36343  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. GRASP con rumore avente varianza di 0.1, k=2 e times=3} 
	\label{jestergrasp2}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -117.10172 \\ \hline
0.50000  & 50    & -87.07865  \\ \hline
 0.50000  & 100   & -73.89528  \\ \hline
 0.50000  & 200   & -61.22163  \\ \hline
 0.50000  & 500   & -56.36799  \\ \hline
 1.00000  & 10    & -108.89930 \\ \hline
 1.00000  & 50    & -81.59752  \\ \hline
 1.00000  & 100   & -70.55376  \\ \hline
 1.00000  & 200   & -60.03375  \\ \hline
 1.00000  & 500   & -56.46136  \\ \hline
 5.00000  & 10    & -87.65345  \\ \hline
 5.00000  & 50    & -72.31113  \\ \hline
 5.00000  & 100   & -64.67077  \\ \hline
 5.00000  & 200   & -58.25021  \\ \hline
 5.00000  & 500   & -56.49393  \\ \hline
 10.00000 & 10    & -77.79056  \\ \hline
10.00000 & 50    & -68.45883  \\ \hline
10.00000 & 100   & -62.33121  \\ \hline
10.00000 & 200   & -57.46528  \\ \hline
 10.00000 & 500   & -56.28735  \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. GRASP con rumore avente varianza di 0.5, k=2 e times=3} 
	\label{jestergrasp3}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -92.81448  \\ \hline
 0.50000  & 50    & -71.13757  \\ \hline
 0.50000  & 100   & -59.64419  \\ \hline
 0.50000  & 200   & -56.54086  \\ \hline
 0.50000  & 500   & -56.42753  \\ \hline
1.00000  & 10    & -87.12938  \\ \hline
 1.00000  & 50    & -70.16060  \\ \hline
1.00000  & 100   & -59.12976  \\ \hline
1.00000  & 200   & -56.51973  \\ \hline
1.00000  & 500   & -56.38371  \\ \hline
5.00000  & 10    & -73.11728  \\ \hline
 5.00000  & 50    & -63.04958  \\ \hline
5.00000  & 100   & -57.92319  \\ \hline
5.00000  & 200   & -56.22606  \\ \hline
 5.00000  & 500   & -56.28537  \\ \hline
10.00000 & 10    & -68.55255  \\ \hline
10.00000 & 50    & -59.86038  \\ \hline
10.00000 & 100   & -57.24748  \\ \hline
10.00000 & 200   & -56.14986  \\ \hline
10.00000 & 500   & -56.35530  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. GRASP con rumore avente varianza di 1, k=2 e times=3} 
	\label{jestergrasp4}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
 0.50000  & 10    & -82.70918  \\ \hline
0.50000  & 50    & -63.58495  \\ \hline
 0.50000  & 100   & -58.36002  \\ \hline
 0.50000  & 200   & -56.35401  \\ \hline
 0.50000  & 500   & -56.50172  \\ \hline
 1.00000  & 10    & -80.25192  \\ \hline
 1.00000  & 50    & -63.41697  \\ \hline
 1.00000  & 100   & -57.79841  \\ \hline
1.00000  & 200   & -56.50708  \\ \hline
1.00000  & 500   & -56.46392  \\ \hline
5.00000  & 10    & -69.65531  \\ \hline
5.00000  & 50    & -59.85005  \\ \hline
5.00000  & 100   & -57.24598  \\ \hline
5.00000  & 200   & -56.22508  \\ \hline
5.00000  & 500   & -56.39747  \\ \hline
10.00000 & 10    & -65.73489  \\ \hline
10.00000 & 50    & -58.74664  \\ \hline
10.00000 & 100   & -56.72399  \\ \hline
10.00000 & 200   & -56.13675  \\ \hline
10.00000 & 500   & -56.22782  \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: jester. GRASP con rumore avente varianza di 2, k=2 e times=3} 
	\label{jestergrasp5}\\
	\hline
	alpha    & minst & test\_ll   \\ \hline
0.50000  & 10    & -80.63601  \\ \hline
 0.50000  & 50    & -62.54936  \\ \hline
 0.50000  & 100   & -57.83377  \\ \hline
0.50000  & 200   & -56.54435  \\ \hline
0.50000  & 500   & -56.40681  \\ \hline
1.00000  & 10    & -77.47129  \\ \hline
 1.00000  & 50    & -61.98214  \\ \hline
 1.00000  & 100   & -57.53426  \\ \hline
1.00000  & 200   & -56.36805  \\ \hline
1.00000  & 500   & -56.46247  \\ \hline
5.00000  & 10    & -67.76990  \\ \hline
5.00000  & 50    & -59.16688  \\ \hline
5.00000  & 100   & -57.01176  \\ \hline
5.00000  & 200   & -56.23598  \\ \hline
5.00000  & 500   & -56.38628  \\ \hline
 10.00000 & 10    & -64.95266  \\ \hline
 10.00000 & 50    & -58.34480  \\ \hline
 10.00000 & 100   & -56.56797  \\ \hline
10.00000 & 200   & -56.15862  \\ \hline
 10.00000 & 500   & -56.25609  \\ \hline
\end{longtable}
\subsubsection{accidents}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Nessun algoritmo di SLS applicato}
	\label{accidentsa}\\
	\hline
 alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -62.15722 \\ \hline
 0.5   & 50    & -41.48072 \\ \hline
 0.5   & 100   & -34.8115  \\ \hline
 0.5   & 200   & -31.66529 \\ \hline
 0.5   & 500   & -30.07461 \\ \hline
 1     & 10    & -57.77949 \\ \hline
 1     & 50    & -40.02941 \\ \hline
 1     & 100   & -34.13259 \\ \hline
 1     & 200   & -31.37831 \\ \hline
 1     & 500   & -29.99737 \\ \hline
 5     & 10    & -47.56462 \\ \hline
 5     & 50    & -36.11655 \\ \hline
 5     & 100   & -32.39425 \\ \hline
5     & 200   & -30.58739 \\ \hline
 5     & 500   & -29.86769 \\ \hline
10    & 10    & -44.0421  \\ \hline
10    & 50    & -34.65678 \\ \hline
 10    & 100   & -31.74568 \\ \hline
 10    & 200   & -30.40374 \\ \hline
 10    & 500   & -29.87294 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Nessun algoritmo di SLS applicato, rumore con varianza di 0.1}
	\label{accidentsa2}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -63.24715 \\ \hline
 0.5   & 50    & -42.83256 \\ \hline
0.5   & 100   & -36.76239 \\ \hline
 0.5   & 200   & -32.63634 \\ \hline
 0.5   & 500   & -32.08839 \\ \hline
 1     & 10    & -57.64453 \\ \hline
1     & 50    & -40.30993 \\ \hline
1     & 100   & -35.31977 \\ \hline
 1     & 200   & -32.99985 \\ \hline
 1     & 500   & -32.22128 \\ \hline
 5     & 10    & -47.17704 \\ \hline
 5     & 50    & -36.70782 \\ \hline
 5     & 100   & -34.22309 \\ \hline
 5     & 200   & -32.39974 \\ \hline
 5     & 500   & -32.16439 \\ \hline
 10    & 10    & -40.61994 \\ \hline
 10    & 50    & -35.6842  \\ \hline
 10    & 100   & -33.35076 \\ \hline
 10    & 200   & -32.34451 \\ \hline
 10    & 500   & -32.32546 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Nessun algoritmo di SLS applicato, rumore con varianza di 0.5}
	\label{accidentsa3}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -48.92708 \\ \hline
 0.5   & 50    & -38.82337 \\ \hline
 0.5   & 100   & -37.0477  \\ \hline
 0.5   & 200   & -36.81411 \\ \hline
 0.5   & 500   & -36.57223 \\ \hline
 1     & 10    & -43.76265 \\ \hline
1     & 50    & -37.88438 \\ \hline
 1     & 100   & -36.70673 \\ \hline
1     & 200   & -37.22731 \\ \hline
1     & 500   & -37.03811 \\ \hline
 5     & 10    & -40.7394  \\ \hline
 5     & 50    & -37.74227 \\ \hline
 5     & 100   & -38.57633 \\ \hline
 5     & 200   & -37.23902 \\ \hline
 5     & 500   & -36.61287 \\ \hline
 10    & 10    & -38.85897 \\ \hline
 10    & 50    & -41.63151 \\ \hline
 10    & 100   & -38.23747 \\ \hline
10    & 200   & -35.87211 \\ \hline
10    & 500   & -38.46156 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Nessun algoritmo di SLS applicato, rumore con varianza di 1}
	\label{accidentsa4}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -45.89425 \\ \hline
 0.5   & 50    & -40.87472 \\ \hline
 0.5   & 100   & -36.99941 \\ \hline
 0.5   & 200   & -36.80616 \\ \hline
 0.5   & 500   & -37.64417 \\ \hline
 1     & 10    & -45.50123 \\ \hline
 1     & 50    & -38.30319 \\ \hline
 1     & 100   & -36.81792 \\ \hline
 1     & 200   & -36.14623 \\ \hline
 1     & 500   & -36.94757 \\ \hline
 5     & 10    & -39.49768 \\ \hline
5     & 50    & -36.83786 \\ \hline
 5     & 100   & -38.45275 \\ \hline
 5     & 200   & -36.63342 \\ \hline
 5     & 500   & -36.98186 \\ \hline
 10    & 10    & -39.79604 \\ \hline
 10    & 50    & -36.35389 \\ \hline
 10    & 100   & -36.68787 \\ \hline
10    & 200   & -36.51541 \\ \hline
 10    & 500   & -39.22734 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Nessun algoritmo di SLS applicato, rumore con varianza di 2}
	\label{accidentsa5}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -46.95516 \\ \hline
 0.5   & 50    & -38.15991 \\ \hline
 0.5   & 100   & -36.80423 \\ \hline
 0.5   & 200   & -36.24913 \\ \hline
 0.5   & 500   & -36.8532  \\ \hline
 1     & 10    & -42.19005 \\ \hline
 1     & 50    & -37.86626 \\ \hline
 1     & 100   & -36.5232  \\ \hline
 1     & 200   & -36.38709 \\ \hline
 1     & 500   & -36.81791 \\ \hline
 5     & 10    & -40.3583  \\ \hline
 5     & 50    & -36.60563 \\ \hline
 5     & 100   & -39.77466 \\ \hline
 5     & 200   & -36.11985 \\ \hline
 5     & 500   & -37.39663 \\ \hline
 10    & 10    & -38.93062 \\ \hline
 10    & 50    & -36.13608 \\ \hline
 10    & 100   & -36.02575 \\ \hline
 10    & 200   & -36.78881 \\ \hline
 10    & 500   & -37.05795 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Iterative Improvement senza rumore.}
	\label{accidentsii}\\
	\hline
alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -59.55557 \\ \hline
 0.5   & 50    & -41.43517 \\ \hline
 0.5   & 100   & -34.8115  \\ \hline
 0.5   & 200   & -31.66529 \\ \hline
 0.5   & 500   & -30.07461 \\ \hline
 1     & 10    & -55.57526 \\ \hline
 1     & 50    & -39.99336 \\ \hline
 1     & 100   & -34.13259 \\ \hline
1     & 200   & -31.37831 \\ \hline
 1     & 500   & -29.99737 \\ \hline
 5     & 10    & -46.25857 \\ \hline
5     & 50    & -36.11375 \\ \hline
5     & 100   & -32.39425 \\ \hline
 5     & 200   & -30.58739 \\ \hline
 5     & 500   & -29.86769 \\ \hline
 10    & 10    & -43.00602 \\ \hline
 10    & 50    & -34.65808 \\ \hline
 10    & 100   & -31.74568 \\ \hline
 10    & 200   & -30.40374 \\ \hline
 10    & 500   & -29.87294 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Iterative Improvement con rumore avente varianza di 0.1.}
	\label{accidentsii2}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -61.42059 \\ \hline
 0.5   & 50    & -42.82839 \\ \hline
 0.5   & 100   & -36.76239 \\ \hline
 0.5   & 200   & -32.6298  \\ \hline
 0.5   & 500   & -32.08839 \\ \hline
 1     & 10    & -55.92635 \\ \hline
 1     & 50    & -40.31144 \\ \hline
 1     & 100   & -35.31977 \\ \hline
 1     & 200   & -32.99985 \\ \hline
 1     & 500   & -32.22128 \\ \hline
5     & 10    & -46.30341 \\ \hline
 5     & 50    & -36.69938 \\ \hline
5     & 100   & -34.22222 \\ \hline
 5     & 200   & -32.39974 \\ \hline
 5     & 500   & -32.16439 \\ \hline
 10    & 10    & -40.17745 \\ \hline
 10    & 50    & -35.68809 \\ \hline
10    & 100   & -33.35076 \\ \hline
 10    & 200   & -32.34451 \\ \hline
 10    & 500   & -32.33111 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Iterative Improvement con rumore avente varianza di 0.5.}
	\label{accidentsii3}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -48.23628 \\ \hline
 0.5   & 50    & -38.82527 \\ \hline
 0.5   & 100   & -37.04328 \\ \hline
0.5   & 200   & -36.80303 \\ \hline
0.5   & 500   & -36.56383 \\ \hline
 1     & 10    & -43.28092 \\ \hline
1     & 50    & -37.88003 \\ \hline
1     & 100   & -36.69973 \\ \hline
1     & 200   & -37.2429  \\ \hline
 1     & 500   & -37.03816 \\ \hline
 5     & 10    & -40.52503 \\ \hline
 5     & 50    & -37.74671 \\ \hline
5     & 100   & -38.57674 \\ \hline
5     & 200   & -37.24174 \\ \hline
5     & 500   & -36.61234 \\ \hline
 10    & 10    & -38.81693 \\ \hline
 10    & 50    & -41.6321  \\ \hline
 10    & 100   & -38.23583 \\ \hline
 10    & 200   & -35.87213 \\ \hline
10    & 500   & -38.47142 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Iterative Improvement con rumore avente varianza di 1.}
	\label{accidentsii5}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -45.28576 \\ \hline
0.5   & 50    & -40.88067 \\ \hline
 0.5   & 100   & -36.99741 \\ \hline
 0.5   & 200   & -36.80462 \\ \hline
 0.5   & 500   & -37.64433 \\ \hline
 1     & 10    & -44.90638 \\ \hline
 1     & 50    & -38.30141 \\ \hline
 1     & 100   & -36.82177 \\ \hline
 1     & 200   & -36.1385  \\ \hline
 1     & 500   & -36.94913 \\ \hline
 5     & 10    & -39.30069 \\ \hline
 5     & 50    & -36.83263 \\ \hline
 5     & 100   & -38.45021 \\ \hline
 5     & 200   & -36.63443 \\ \hline
5     & 500   & -36.98315 \\ \hline
10    & 10    & -39.61138 \\ \hline
 10    & 50    & -36.34962 \\ \hline
10    & 100   & -36.68793 \\ \hline
 10    & 200   & -36.51427 \\ \hline
 10    & 500   & -39.23051 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Iterative Improvement con rumore avente varianza di 2.}
	\label{accidentsii4}\\
	\hline
	alpha & minst & test\_ll  \\ \hline
 0.5   & 10    & -46.23398 \\ \hline
 0.5   & 50    & -38.15193 \\ \hline
0.5   & 100   & -36.80296 \\ \hline
0.5   & 200   & -36.24463 \\ \hline
 0.5   & 500   & -36.85057 \\ \hline
 1     & 10    & -41.84649 \\ \hline
 1     & 50    & -37.86544 \\ \hline
 1     & 100   & -36.5232  \\ \hline
 1     & 200   & -36.38577 \\ \hline
 1     & 500   & -36.81791 \\ \hline
 5     & 10    & -40.10899 \\ \hline
5     & 50    & -36.60217 \\ \hline
5     & 100   & -39.77575 \\ \hline
 5     & 200   & -36.12266 \\ \hline
 5     & 500   & -37.39628 \\ \hline
 10    & 10    & -38.68157 \\ \hline
10    & 50    & -36.13497 \\ \hline
 10    & 100   & -36.02559 \\ \hline
10    & 200   & -36.79904 \\ \hline
10    & 500   & -37.05795 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Randomised Iterative Improvement senza rumore, probabilità di 0.6 e times=10}
	\label{accidentsrii}\\
	\hline
 alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -60.83934 \\ \hline
 0.50000  & 50    & -41.46811 \\ \hline
 0.50000  & 100   & -34.81150 \\ \hline
 0.50000  & 200   & -31.66529 \\ \hline
 0.50000  & 500   & -30.07461 \\ \hline
 1.00000  & 10    & -56.61832 \\ \hline
1.00000  & 50    & -40.01901 \\ \hline
1.00000  & 100   & -34.13259 \\ \hline
1.00000  & 200   & -31.37831 \\ \hline
 1.00000  & 500   & -29.99737 \\ \hline
5.00000  & 10    & -46.90489 \\ \hline
5.00000  & 50    & -36.11715 \\ \hline
5.00000  & 100   & -32.39425 \\ \hline
5.00000  & 200   & -30.58739 \\ \hline
 5.00000  & 500   & -29.86769 \\ \hline
 10.00000 & 10    & -43.51441 \\ \hline
 10.00000 & 50    & -34.65816 \\ \hline
 10.00000 & 100   & -31.74568 \\ \hline
 10.00000 & 200   & -30.40374 \\ \hline
 10.00000 & 500   & -29.87294 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Randomised Iterative Improvement con rumore avente 0.1 di varianza, probabilità di 0.6 e times=10}
	\label{accidentsrii2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -62.24658 \\ \hline
0.50000  & 50    & -42.83205 \\ \hline
 0.50000  & 100   & -36.76239 \\ \hline
 0.50000  & 200   & -32.62977 \\ \hline
 0.50000  & 500   & -32.08839 \\ \hline
 1.00000  & 10    & -56.69594 \\ \hline
 1.00000  & 50    & -40.31181 \\ \hline
 1.00000  & 100   & -35.31977 \\ \hline
1.00000  & 200   & -32.99985 \\ \hline
 1.00000  & 500   & -32.22128 \\ \hline
5.00000  & 10    & -46.72341 \\ \hline
5.00000  & 50    & -36.71141 \\ \hline
5.00000  & 100   & -34.22311 \\ \hline
5.00000  & 200   & -32.39974 \\ \hline
5.00000  & 500   & -32.16439 \\ \hline
10.00000 & 10    & -40.36175 \\ \hline
10.00000 & 50    & -35.69038 \\ \hline
10.00000 & 100   & -33.35076 \\ \hline
10.00000 & 200   & -32.34451 \\ \hline
10.00000 & 500   & -32.32418 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Randomised Iterative Improvement con rumore avente 0.5 di varianza, probabilità di 0.6 e times=10}
	\label{accidentsrii3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -48.50305 \\ \hline
0.50000  & 50    & -38.82681 \\ \hline
0.50000  & 100   & -37.04483 \\ \hline
 0.50000  & 200   & -36.80748 \\ \hline
 0.50000  & 500   & -36.56031 \\ \hline
 1.00000  & 10    & -43.49345 \\ \hline
 1.00000  & 50    & -37.88454 \\ \hline
 1.00000  & 100   & -36.70702 \\ \hline
1.00000  & 200   & -37.23132 \\ \hline
1.00000  & 500   & -37.03785 \\ \hline
5.00000  & 10    & -40.62816 \\ \hline
5.00000  & 50    & -37.74194 \\ \hline
5.00000  & 100   & -38.57267 \\ \hline
5.00000  & 200   & -37.23877 \\ \hline
 5.00000  & 500   & -36.61554 \\ \hline
 10.00000 & 10    & -38.83921 \\ \hline
10.00000 & 50    & -41.63274 \\ \hline
10.00000 & 100   & -38.23947 \\ \hline
10.00000 & 200   & -35.87381 \\ \hline
10.00000 & 500   & -38.46426 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Randomised Iterative Improvement con rumore avente 1 di varianza, probabilità di 0.6 e times=10}
	\label{accidentsrii4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -45.53200 \\ \hline
 0.50000  & 50    & -40.87799 \\ \hline
 0.50000  & 100   & -36.99684 \\ \hline
 0.50000  & 200   & -36.80544 \\ \hline
0.50000  & 500   & -37.64447 \\ \hline
1.00000  & 10    & -45.09297 \\ \hline
1.00000  & 50    & -38.30121 \\ \hline
1.00000  & 100   & -36.81693 \\ \hline
 1.00000  & 200   & -36.14076 \\ \hline
 1.00000  & 500   & -36.94905 \\ \hline
 5.00000  & 10    & -39.40048 \\ \hline
 5.00000  & 50    & -36.83562 \\ \hline
 5.00000  & 100   & -38.45222 \\ \hline
 5.00000  & 200   & -36.63333 \\ \hline
 5.00000  & 500   & -36.98179 \\ \hline
 10.00000 & 10    & -39.69391 \\ \hline
10.00000 & 50    & -36.35079 \\ \hline
10.00000 & 100   & -36.68908 \\ \hline
 10.00000 & 200   & -36.51637 \\ \hline
 10.00000 & 500   & -39.23095 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. Randomised Iterative Improvement con rumore avente 2 di varianza, probabilità di 0.6 e times=10}
	\label{accidentsrii5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -46.53237 \\ \hline
0.50000  & 50    & -38.15916 \\ \hline
 0.50000  & 100   & -36.80448 \\ \hline
0.50000  & 200   & -36.24695 \\ \hline
 0.50000  & 500   & -36.85022 \\ \hline
1.00000  & 10    & -41.98584 \\ \hline
1.00000  & 50    & -37.86619 \\ \hline
 1.00000  & 100   & -36.52320 \\ \hline
 1.00000  & 200   & -36.38285 \\ \hline
1.00000  & 500   & -36.81791 \\ \hline
 5.00000  & 10    & -40.20916 \\ \hline
 5.00000  & 50    & -36.60366 \\ \hline
 5.00000  & 100   & -39.77520 \\ \hline
 5.00000  & 200   & -36.12183 \\ \hline
 5.00000  & 500   & -37.39627 \\ \hline
10.00000 & 10    & -38.78225 \\ \hline
10.00000 & 50    & -36.13419 \\ \hline
 10.00000 & 100   & -36.02577 \\ \hline
 10.00000 & 200   & -36.78771 \\ \hline
 10.00000 & 500   & -37.05795 \\ \hline
\end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. GRASP senza rumore, k=2 e times=5}
	\label{accidentsgrasp}\\
	\hline
 alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -59.55557 \\ \hline
0.50000  & 50    & -41.43517 \\ \hline
0.50000  & 100   & -34.81150 \\ \hline
0.50000  & 200   & -31.66529 \\ \hline
0.50000  & 500   & -30.07461 \\ \hline
1.00000  & 10    & -55.57526 \\ \hline
 1.00000  & 50    & -39.99336 \\ \hline
 1.00000  & 100   & -34.13259 \\ \hline
 1.00000  & 200   & -31.37831 \\ \hline
 1.00000  & 500   & -29.99737 \\ \hline
 5.00000  & 10    & -46.25857 \\ \hline
 5.00000  & 50    & -36.11375 \\ \hline
 5.00000  & 100   & -32.39425 \\ \hline
5.00000  & 200   & -30.58739 \\ \hline
 5.00000  & 500   & -29.86769 \\ \hline
 10.00000 & 10    & -43.00602 \\ \hline
 10.00000 & 50    & -34.65808 \\ \hline
 10.00000 & 100   & -31.74568 \\ \hline
 10.00000 & 200   & -30.40374 \\ \hline
 10.00000 & 500   & -29.87294 \\ \hline
 \end{longtable}
\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. GRASP con rumore avente varianza di 0.1, k=2 e times=5}
	\label{accidentsgrasp2}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -60.19237 \\ \hline
 0.50000  & 50    & -42.19728 \\ \hline
 0.50000  & 100   & -36.02921 \\ \hline
 0.50000  & 200   & -33.44082 \\ \hline
 0.50000  & 500   & -32.03720 \\ \hline
 1.00000  & 10    & -53.05862 \\ \hline
 1.00000  & 50    & -40.69411 \\ \hline
 1.00000  & 100   & -35.71858 \\ \hline
1.00000  & 200   & -32.73453 \\ \hline
1.00000  & 500   & -31.96624 \\ \hline
 5.00000  & 10    & -46.54484 \\ \hline
 5.00000  & 50    & -37.49317 \\ \hline
 5.00000  & 100   & -33.90308 \\ \hline
5.00000  & 200   & -32.28769 \\ \hline
5.00000  & 500   & -32.45413 \\ \hline
10.00000 & 10    & -42.34078 \\ \hline
10.00000 & 50    & -35.20659 \\ \hline
10.00000 & 100   & -33.12773 \\ \hline
10.00000 & 200   & -32.34099 \\ \hline
10.00000 & 500   & -32.17970 \\ \hline
\end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. GRASP con rumore avente varianza di 0.5, k=2 e times=5}
	\label{accidentsgrasp3}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -43.23682 \\ \hline
 0.50000  & 50    & -39.00616 \\ \hline
 0.50000  & 100   & -37.04375 \\ \hline
 0.50000  & 200   & -36.29167 \\ \hline
 0.50000  & 500   & -36.39501 \\ \hline
1.00000  & 10    & -40.89193 \\ \hline
 1.00000  & 50    & -38.27758 \\ \hline
 1.00000  & 100   & -37.71308 \\ \hline
 1.00000  & 200   & -36.17050 \\ \hline
1.00000  & 500   & -37.53471 \\ \hline
 5.00000  & 10    & -40.01585 \\ \hline
 5.00000  & 50    & -37.36379 \\ \hline
 5.00000  & 100   & -36.05226 \\ \hline
 5.00000  & 200   & -37.22279 \\ \hline
5.00000  & 500   & -36.68760 \\ \hline
10.00000 & 10    & -38.35159 \\ \hline
 10.00000 & 50    & -36.65296 \\ \hline
10.00000 & 100   & -36.50834 \\ \hline
 10.00000 & 200   & -36.38754 \\ \hline
 10.00000 & 500   & -37.63893 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. GRASP con rumore avente varianza di 1, k=2 e times=5}
	\label{accidentsgrasp4}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
 0.50000  & 10    & -46.69148 \\ \hline
 0.50000  & 50    & -39.09587 \\ \hline
 0.50000  & 100   & -36.91035 \\ \hline
 0.50000  & 200   & -36.47041 \\ \hline
 0.50000  & 500   & -37.17706 \\ \hline
 1.00000  & 10    & -45.45079 \\ \hline
 1.00000  & 50    & -38.04120 \\ \hline
 1.00000  & 100   & -36.47830 \\ \hline
 1.00000  & 200   & -36.34505 \\ \hline
1.00000  & 500   & -37.14244 \\ \hline
5.00000  & 10    & -41.04134 \\ \hline
5.00000  & 50    & -40.24065 \\ \hline
5.00000  & 100   & -37.59176 \\ \hline
 5.00000  & 200   & -35.70818 \\ \hline
 5.00000  & 500   & -37.15097 \\ \hline
 10.00000 & 10    & -39.38945 \\ \hline
 10.00000 & 50    & -38.38552 \\ \hline
10.00000 & 100   & -37.20100 \\ \hline
10.00000 & 200   & -36.84360 \\ \hline
 10.00000 & 500   & -37.17689 \\ \hline
 \end{longtable}

\begin{longtable}{|l|l|l|}
	\caption{dataset: accidents. GRASP con rumore avente varianza di 2, k=2 e times=5}
	\label{accidentsgrasp5}\\
	\hline
	alpha    & minst & test\_ll  \\ \hline
0.50000  & 10    & -46.38060 \\ \hline
 0.50000  & 50    & -38.60307 \\ \hline
 0.50000  & 100   & -38.18711 \\ \hline
 0.50000  & 200   & -36.35354 \\ \hline
0.50000  & 500   & -36.87568 \\ \hline
1.00000  & 10    & -44.39325 \\ \hline
1.00000  & 50    & -37.54959 \\ \hline
1.00000  & 100   & -36.45150 \\ \hline
 1.00000  & 200   & -36.02602 \\ \hline
1.00000  & 500   & -36.84540 \\ \hline
5.00000  & 10    & -40.62676 \\ \hline
 5.00000  & 50    & -36.63745 \\ \hline
 5.00000  & 100   & -35.80353 \\ \hline
5.00000  & 200   & -36.54389 \\ \hline
5.00000  & 500   & -36.56929 \\ \hline
 10.00000 & 10    & -39.07415 \\ \hline
 10.00000 & 50    & -36.28810 \\ \hline
 10.00000 & 100   & -40.96060 \\ \hline
 10.00000 & 200   & -36.19343 \\ \hline
 10.00000 & 500   & -37.88981 \\ \hline
\end{longtable}
	\subsection{Discussione}\label{disc}
	Questo capitolo racchiude tutte le considerazioni e le discussioni riguardo i risultati precedentemente inseriti. Ogni sotto-capitolo contiene la discussione dei risultati per ogni algoritmo.
	\subsubsection{Versione originale di base}
	Prima di applicare gli algoritmi di ricerca al problema sono state eseguite delle grid searches con l'algoritmo originale.\\
	I risultati ottenuti sono stati utilizzati come base con cui comparare eventuali miglioramenti prodotti dagli algoritmi di ricerca e dalle modifiche presentate in \ref{for} e \ref{noise}.
	
	Si è subito notato come l'aggiunta di rumore alla matrice delle mutue informazioni abbia avuto il medesimo impatto per ogni dataset testato. In particolare si è notato come l'aggiunta di rumore causi un miglioramento della log-likelihood solamente quando si utilizza un valore di $\delta$ piccolo; ovvero quando si termina la ricerca di un nodo, su cui splittare, a causa di un numero piccolo di istanze rimanenti.
	
	L'aggiunta di rumore peggiora la likelihood rispetto al modello senza aggiunta di rumore quando si utilizza un $\delta$ grande. Ciò è probabilmente dovuto al fatto che avere un $\delta$ grande porta il modello a ''vedere'' meno istanze e quindi ad adattarsi in maniera peggiore alla distribuzione campione rappresentata dal training set. Avere un $\delta$ minore porta il modello a fare un pò di overfitting e quindi aggiungere del rumore lo aiuta ad evadere da un minimo locale.
	\begin{figure}
		\centering
		\includegraphics[height=\textheight]{grafici/nltcs_base.pdf}
		\caption{}
		\label{fig:nltcsfig}
	\end{figure}
	
	La figura \ref{fig:nltcsfig} rappresenta l'andamento del log-likelihood in funzione del parametro alpha.
	Ogni grafico rappresenta l'andamento del likelihood per $\delta$ uguale a 10,200,500 rispettivamente.
	Ogni grafico visualizza le serie di likelihood in base alla varianza scelta per il rumore avente diversi valori (0.1,0.5,1,2,3).\\Si può notare come per mnist piccolo l'aggiunta del rumore abbia un impatto positivo nel likelihood. Mentre nel caso di mnist grande (500) l'aggiunta del rumore provochi una diminuzione del likelihood.\\
	In tutti i casi l'aumentare della varianza contribuisce all'aumento del likelihood nel caso di mnist piccolo e alla diminuzione del likelihood nel caso di mnist grande.
	
	In generale si può notare come un alpha grande permetta di avere un likelihood migliore rispetto ad alpha piccoli.
	
	Si è notato inoltre come in caso di assenza di rumore il parametro $\delta$ influisca direttamente sull'apprendimento del modello. In particolare confrontando i vari log-likelihood misurati sul training set, validation set e test set si è potuto notare come l'utilizzo di $\delta$ grandi permetta al modello di non incorrere in overfitting.
	
	
	La figura \ref{fig:trainvalidtestfig} rappresenta l'andamento del likelihood sulle varie partizioni del dataset nltcs, non aggiungendo rumore e non utilizzando nessun algoritmo di ricerca.\\
	Le linee tratteggiate indicano il likelihood misurato sul training set, quelle di tipo tratto-punto quello misurato sul validation set e quelle continue quello misurato sul test set.
	\\	\vbox{Ogni colore rappresenta un valore di $\delta$:
		\begin{itemize}  
			\item arancio: delta=10 
			\item celeste: delta=50 
			\item rosso: delta=100
			\item verde: delta=200
			\item nero: delta=500 
	\end{itemize}}
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{grafici/trainvalidtest-cropped.pdf}
		\caption{}
		\label{fig:trainvalidtestfig}
	\end{figure}
	
	Dal grafico emerge come l'aumentare di $\delta$ permetta al likelihood sul training set di diminuire in favore dell'aumento dei likelihood su validation e test set. Quindi utilizzando un $\delta$ piccolo si tende a fare overfitting performando bene sul training set ma male su validation e test set.
	
	\`{E} emerso che utilizzando un $\delta$ piccolo porta il modello a fare overfitting. Si può ovviare a questo problema aggiungendo del rumore.\\
	Se si utilizza un $\delta$ grande allora conviene non aggiungere rumore.\\
	Se si utilizza un delta medio allora conviene aggiungere rumore utilizzando una varianza né troppo alta e né troppo bassa.
	
	Un altro esempio è rappresentato nella figura \ref{fig:plants}, dove il dataset plants ha prodotto il medesimo comportamento.
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{grafici/senza_al_plants.pdf}
		\caption{}
		\label{fig:plants}
	\end{figure}
	\newpage
	\subsubsection{Iterative improvement}
	L'applicazione di Iterative Improvement al processo di apprendimento ha avuto risultati discreti. In particolare si è notato che iterative improvement riesce a migliorare la likelihood rispetto alla versione di base solamente quando $\delta$ è basso.\\
	In generale per ogni dataset iterative improvement riesce a migliorare l'apprendimento quando $\delta$ è uguale a 10. Per valori di $\delta$ crescenti la versione base e la versione con iterative improvement raggiungono gli stessi valori di log-likelihood.
	
	\begin{figure}
		\centerline{
			\includegraphics[width=\linewidth]{grafici/audio_base_vs_ii.pdf}}
		\caption{}
		\label{fig:audiofig}
	\end{figure}
	\begin{figure}
		\centerline{
			\includegraphics[height=\textheight]{grafici/accidents_base_vs_ii.pdf}}
		\caption{}
		\label{fig:accfig}
	\end{figure}
	La figura \ref{fig:audiofig} mostra l'andamento della likelihood, sul dataset ''audio'',  comparando la versione di base con la versione a cui è stato applicato iterative improvement.
	Si può notare come utilizzando un $\delta$ piccolo, iterative improvement raggiunga risultati migliori per ogni valore di alpha. Utilizzando un $\delta$ maggiore o uguale a 100 fa si che le due serie diventino identiche, evidenziando il fatto che iterative improvement non riesce ad evadere dal minimo locale raggiunto in principio dalla versione base.
	
	Lo stesso accade per gli altri datasets. Nella figura \ref{fig:accfig} sono riportati anche i grafici del dataset ''accidents''.
	\newpage
	\subsubsection{Randomised iterative improvement}
	Randomised Iterative improvement è stato applicato a tutti i datasets. Come parametro $p$, rappresentante la probabilità di scegliere una componente random dalla Restricted Candidate List, sono stati utilizzati i valori 0.6 e 0.8. Come parametro $t$ rappresentante il numero di iterazioni massime da fare è stato utilizzato il valore 10. Tutti gli altri parametri sono stati soggetto a grid search come anticipatamente descritto.
	
	Questa variante ha presentato gli stessi risultati dell'applicazione di Iterative Improvement. RII migliora l'adattamento del modello all'insieme di dati quando $\delta$ è basso (10-100) . Utilizziando valori di $\delta > 100$ RII raggiunge gli stessi risultati di Iterative Improvement e della versione base.
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{grafici/vsvs.pdf}
		\caption{Grafico derivato dal dataset ''msnbc''}
		\label{fig:riifig}
	\end{figure}
	
	Nella figura \ref{fig:riifig} è riportato l'andamento del likelihood con mnist piccolo e rumore avente varianza 0.1. Con mnist piccolo RII aiuta il modello ad adattarsi meglio, ma al crescere di mnist la likelihood è esattamente uguale a quella raggiunta dalla versione di base.
	
	\subsubsection{GRASP}
	Anche GRASP ha avuto un comportamento molto simile a quello degli altri algoritmi. Per quanto riguarda GRASP come valore di $t$ sono stati utilizzati i seguenti valori $\{ 3,5,20\}$; essi sono stati decisi in base alla dimensione del dataset. Ad esempio sul dataset ''nltcs'' è stato potuto utilizzare $t=20$, mentre per datasets più grandi sono stati utilizzati 3 e 5.
	
	Il parametro $k$, rappresentante la dimensione della RCL contenente le migliori $k$ componenti da cui scegliere, è stato scelto dall'insieme $\{2,3\}$. Per datasets piccoli sono stati utilizzati tutti e due. Per datasets grandi come ''plants'' è stato utilizzato solamente il valore 3.
	
	Anche con GRASP utilizzare un valore di $\delta$ grande ha implicato avere valori di log-likelihood non migliori della versione originale. Tuttavia l'utilizzo di valori di $\delta$ tra 10 e 100 ha evidenziato che GRASP riesce comunque ad avere risultati migliori della versione base con gli stessi parametri. Questi risultati migliori si allineano, bene o male, con i valori ottenuti dalle altre varianti algoritmiche.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=\linewidth]{grafici/graspp.pdf}
		\caption{Grafico derivato dal dataset ''jester''}
		\label{fig:graspfig}
	\end{figure}
	La figura \ref{fig:graspfig} mostra l'andamento del likelihood in funzione del parametro alpha sul dataset jester utilizzando un $\delta = 10$ e applicando rumore avente varianza 0.5.\\
	In quasi tutti i punti GRASP ottiene dei valori leggermente migliori rispetto alla versione base. Codesti miglioramenti sono nell'ordine di 0.5 - 1.0.
	
	\newpage
	\section{Conclusioni}\label{conc}
	Il compito degli algoritmi di apprendimento descritti in questa tesi è stato quello di apprendere modelli grafici probabilistici per fare density estimation sfruttando risultati consolidati negli anni come l'apprendimento di alberi di Chow-Liu.\\
	Questa tesi ha avuto il compito di esplorare l'applicazione di algoritmi stocastici di ricerca locale all'apprendimento di Cutset Network. In particolare gli algoritmi sono stati applicati all'interno dell'algoritmo dCSN presentato in \cite{dimauro}.
	
	Il primo step della tesi è stato quello di presentare alcuni modelli grafici probabilistici come le reti bayesiane, gli alberi di Chow-Liu, le Cutset Networks e la tecnica delle misture.
	
	Successivamente sono stati introdotti gli algoritmi di ricerca locale stocastica ed in particolare Iterative Improvement, Randomised Iterative Improvement e GRASP.
	
	Poi sono state presentate alcune modifiche all'algoritmo originale di dCSN come l'aggiunta di rumore alla matrice delle mutue informazioni e l'utilizzo di foreste al posto degli alberi di Chow-Liu nelle Cutset Networks.
	
	Dopo aver presentato i datasets su cui è stata fatta la sperimentazione, sono stati riportati i risultati in forma tabellare così come prodotti dagli script scritti in python.
	
	Dai risultati è emerso come l'applicazione degli algoritmi di ricerca locale migliori le prestazioni dell'algoritmo originale solamente per particolari combinazioni di parametri. Per altre combinazioni di parametri è stato riscontrato che l'applicazione di algoritmi di ricerca non migliori né peggiori il log-likelihood misurato sulla partizione dedicata al test set di ogni dataset provato.\\
	L'aggiunta di rumore alla matrice delle mutue informazioni ha avuto un riscontro positivo nel caso in cui vengano utilizzate molte osservazioni del training set per l'apprendimento della struttura. Il valore della varianza del rumore è stato direttamente proporzionale al miglioramento o peggioramento riscontrato nei risultati numerici.
	
	Guardando in maniera generale i risultati numerici, gli algoritmi di ricerca locale non sono riusciti ad avere log-likelihood complessivamente migliori rispetto ai log-likelihood raggiunti dalla versione base di dCSN.
	\clearpage
	\section*{Ringraziamenti}
	\addcontentsline{toc}{section}{Ringraziamenti}
	Ringrazio il mio relatore, il Dr. Nicola Di Mauro. Ringrazio anche il Dr. Antonio Vergari per il continuo supporto fornitomi. Ringrazio i miei genitori e il mio cane per la compagnia fornitami durante la stesura della tesi. Ringrazio i Carbon based lifeforms, Boards of Canada, Solar Fields, Connect.Ohm e Lamb of God per aver fatto da sottofondo durante i miei studi.
	\newpage
	\begin{thebibliography}{2}
		
		\bibitem{chowliu}C. Chow and C. Liu. Approximating discrete probability distributions
		with dependence trees. IEEE Transactions on Information
		Theory, 14$-$462 467, 1968
		\bibitem{rahman}Rahman, T., Kothalkar, P., Gogate, V.: Cutset networks: A simple, tractable,and scalable approach for improving the accuracy of Chow-Liu trees. In: Machine Learning and Knowledge Discovery in Databases, LNCS, vol. 8725, pp. 630$-$645. Springer (2014)
		\bibitem{dimauro}Di Mauro, N.; Vergari, A.; and Esposito, F. 2015. Learning accurate
		cutset networks by exploiting decomposability. In AI*IA 2015
		Advances in Artificial Intelligence, 221$-$232
		\bibitem{onisko}A. Onisko, M. Druzdzel, and H. Wasyluk. A Bayesian network
		model for diagnosis of liver disorders. In Proceedings of the 11th Conference on Biocybernetics and Biomedical Engineering,
		pages 842$-$846, 1999
		\bibitem{slsbook}
		Hoos, H.H. and St{\"u}tzle. Stochastic Local Search: Foundations and Applications, isbn 9780080498249, The Morgan Kaufmann Series in Artificial Intelligence. Elsevier Science (2004)
		\bibitem{koller}Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Techniques. MIT Press (2009)
		\bibitem{bre96}L. Breiman. Arcing classifiers. Technical report, Dept. of
		Statistics, University of California, 1996.
		\bibitem{manuscript}François Schnitzler. Mixtures of Tree-Structured Probabilistic
		Graphical Models for Density Estimation in
		High Dimensional Spaces
		\bibitem{friedman}Friedman, N., Geiger, D., Goldszmidt, M.: Bayesian network classifiers. Machine learning 29(2-3), 131$-$163 (1997)
		\bibitem{heckerman}Heckerman, D., Geiger, D., Chickering, D.: Learning bayesian networks: the combination of knowledge and statistical data. Machine  learning 20, 197$-$243 (1995)
		\bibitem{lowd}Lowd, D., Davis, J.: Learning Markov network structure with decision trees. In: Proceedings of the 10th IEEE International Conference on Data Mining. pp. 334$-$343. IEEE Computer Society Press (2010)
		\bibitem{haaren}Haaren, J.V., Davis, J.: Markov network structure learning: A randomized feature generation approach. In: Proceedings of the 26th Conference on Articial Intelligence. AAAI Press (2012)
		\bibitem{meila}Meila, M., Jordan, M.: Learning with mixtures of trees. Journal of Machine Learning Research 1, 1$-$48 (2000)
		\bibitem{dechter}Dechter, R., Mateescu, R.: AND/OR search spaces for graphical models. Artificial
		Intelligence 171(2), 73$-$106 (2007)
		\bibitem{krusk}J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical Society, 7(1):48$-$50, 1956.
		\bibitem{kullback}S. Kullback and R. Leibler. On information and sufficiency.
		The Annals of Mathematical Statistics, 22(1):79$-$86, 1951.
		\bibitem{bishop} Christopher Bishop. Neural networks for pattern recognition. Clarendon Press-Oxford, 1995.
		\bibitem{bishop2} Christopher Bishop. Pattern Recognition and Machine Learning. Springer Science-Business Media, LLC, 2006
	\end{thebibliography}
\end{document}